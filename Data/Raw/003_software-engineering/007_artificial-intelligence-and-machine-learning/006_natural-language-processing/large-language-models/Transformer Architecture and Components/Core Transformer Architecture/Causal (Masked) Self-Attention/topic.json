{
  "topic_title": "Causal (Masked) Self-Attention",
  "entry_domain": "Transformer Architecture and Components",
  "entry_subdomain": "Core Transformer Architecture",
  "subdomain_title": "Large Language Models",
  "subdomain_folder": "006_natural-language-processing",
  "domain_title": "Artificial Intelligence And Machine Learning",
  "domain_folder": "007_artificial-intelligence-and-machine-learning",
  "category_title": "Software Engineering",
  "category_folder": "003_software-engineering",
  "curriculum_type": "software-engineering",
  "hierarchy": {
    "level_1_category": "Software Engineering",
    "level_2_domain": "Artificial Intelligence And Machine Learning",
    "level_3_subdomain": "Large Language Models",
    "level_4_entry_domain": "Transformer Architecture and Components",
    "level_5_entry_subdomain": "Core Transformer Architecture",
    "level_6_topic": "Causal (Masked) Self-Attention"
  },
  "metadata": {
    "created_at": "2025-12-31T10:01:47.096645",
    "source_file": "subdomain.json"
  }
}