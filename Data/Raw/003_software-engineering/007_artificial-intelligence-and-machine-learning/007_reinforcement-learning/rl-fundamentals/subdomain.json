{
  "subdomain_title": "RL Fundamentals",
  "domain_title": "Reinforcement Learning",
  "category_title": "Software Engineering",
  "curriculum_type": "software-engineering",
  "cleaned_at": "2025-12-30T09:12:18.797206",
  "model_used": "google/gemini-2.5-flash",
  "entries": [
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "n-Armed Bandit Problem Formulation"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Action-Value Methods"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Incremental Implementation"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Nonstationary Problem Tracking"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Optimistic Initial Values"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Upper-Confidence-Bound (UCB) Action Selection"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Gradient Bandit Algorithms"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Associative Search and Contextual Bandits"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Epsilon-Greedy Methods"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Greedy Action Selection"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Sample-Average Methods"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Exploration Strategies"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Exploitation Strategies"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Balancing Exploration and Exploitation"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Regret Minimization"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Exploration-Exploitation Trade-off",
      "topic": "Performance Metrics for Bandit Algorithms"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "State Representation"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Action Space Definition"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Reward Signal Structure"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Transition Dynamics"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Observation vs. State"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Feedback Mechanisms"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Sequential Interaction Protocol"
    },
    {
      "domain": "Sequential Decision-Making Foundations",
      "subdomain": "Agent-Environment Interface",
      "topic": "Time Step Formalization"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "State Space Definition"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "Action Space Definition"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "Transition Probability Functions"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "Reward Function Specification"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "Discount Factor and Its Role"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "Markov Property"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "MDP Tuple Notation"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "MDP Formalism",
      "topic": "Episodic vs. Continuing Tasks"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Expected Return Definition"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Cumulative Return Calculation"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Discounted Return"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Undiscounted Return"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Reward Hypothesis"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Finite Horizon Returns"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Infinite Horizon Returns"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Returns and Goals",
      "topic": "Unified Notation for Episodic and Continuing Tasks"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Deterministic Policies"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Stochastic Policies"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Policy Representation"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Policy Notation and Formalism"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Stationary Policies"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Non-Stationary Policies"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Policy Evaluation Criteria"
    },
    {
      "domain": "Markov Decision Processes (MDPs)",
      "subdomain": "Policies",
      "topic": "Optimal Policy Definition"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "State-Value Function Definition (V-function)"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "Expected Return from State"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "State-Value under Policy π"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "Recursive Decomposition"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "Value Function Properties"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "State-Value Estimation Methods"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "Optimal State-Value Function (V*)"
    },
    {
      "domain": "Value Functions",
      "subdomain": "State-Value Functions",
      "topic": "Value Function Representation"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Action-Value Function Definition (Q-function)"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Expected Return from State-Action Pair"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Action-Value under Policy π"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Q-function Properties"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Relationship between V and Q"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Action Selection from Q-values"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Optimal Action-Value Function (Q*)"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Action-Value Functions",
      "topic": "Q-function Estimation Methods"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Bellman Expectation Equation for V"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Bellman Expectation Equation for Q"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Bellman Optimality Equation for V*"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Bellman Optimality Equation for Q*"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Bellman Backup Operations"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Bellman Equation Derivation"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Recursive Value Function Relations"
    },
    {
      "domain": "Value Functions",
      "subdomain": "Bellman Equations",
      "topic": "Solving Bellman Equations"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "Iterative Policy Evaluation Algorithm"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "State-Value Prediction"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "Synchronous Backups"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "Convergence Guarantees"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "Truncation and Stopping Criteria"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "In-Place Updates"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "Computational Complexity"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Evaluation",
      "topic": "Full Model Requirement"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Policy Improvement Theorem"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Greedy Policy Improvement"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Action Selection from Value Functions"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Policy Improvement Guarantees"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Deterministic Policy Improvement"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Policy Comparison Methods"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Improvement Step Formalization"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Policy Improvement",
      "topic": "Convergence to Optimal Policy"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Policy Iteration Algorithm"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Value Iteration Algorithm"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Generalized Policy Iteration (GPI)"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Asynchronous Dynamic Programming"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Modified Policy Iteration"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Prioritized Sweeping"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Real-Time Dynamic Programming"
    },
    {
      "domain": "Dynamic Programming Methods",
      "subdomain": "Control Algorithms",
      "topic": "Computational Efficiency Considerations"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "First-Visit MC Prediction"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Every-Visit MC Prediction"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Sample Episode Generation"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Return Averaging"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Incremental Mean Computation"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Model-Free Prediction"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Convergence Properties"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Prediction",
      "topic": "Bias-Variance Trade-offs"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "Monte Carlo ES (Exploring Starts)"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "On-Policy MC Control"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "Epsilon-Soft Policies"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "Action-Value Estimation for Control"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "GPI with Monte Carlo Methods"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "Maintaining Exploration"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "Convergence to Optimal Policy"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Monte Carlo Control",
      "topic": "Episode-Based Updates"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Importance Sampling Basics"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Ordinary Importance Sampling"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Weighted Importance Sampling"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Off-Policy Prediction"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Off-Policy Control"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Target Policy vs. Behavior Policy"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Importance Sampling Ratio"
    },
    {
      "domain": "Monte Carlo Methods",
      "subdomain": "Off-Policy Monte Carlo",
      "topic": "Variance Reduction Techniques"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "TD(0) Algorithm"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "Bootstrapping Concept"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "TD Update Rule"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "TD Error Calculation"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "Learning Rate (Step Size) Selection"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "Advantages of TD Methods"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "TD vs. MC Comparison"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "TD Prediction",
      "topic": "Convergence Properties of TD(0)"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "Sarsa Algorithm"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "State-Action-Reward-State-Action Transitions"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "On-Policy Learning"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "Epsilon-Greedy Action Selection"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "Sarsa Convergence"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "Expected Sarsa"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "Sarsa for Episodic Tasks"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "On-Policy TD Control",
      "topic": "Sarsa for Continuing Tasks"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Q-Learning Algorithm"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Off-Policy Learning Mechanism"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Max Q-Value Operator"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Q-Learning Update Rule"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Target Policy vs. Behavior Policy in Q-Learning"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Q-Learning Convergence Guarantees"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Double Q-Learning"
    },
    {
      "domain": "Temporal-Difference Learning",
      "subdomain": "Off-Policy TD Control",
      "topic": "Optimality of Q-Learning"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "n-Step TD Prediction"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "n-Step Return Definition"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "n-Step Sarsa"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "n-Step Q-Learning"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "Unified View of n-Step Methods"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "Choosing n Value"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "n-Step Off-Policy Learning"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "n-Step Methods",
      "topic": "Truncated Returns"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Forward View of TD(λ)"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Backward View of TD(λ)"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "λ-Return Definition"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Trace Decay Parameter"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Accumulating Traces"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Replacing Traces"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Eligibility Trace Updates"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Eligibility Traces",
      "topic": "Equivalence of Forward and Backward Views"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Sarsa(λ) Algorithm"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Watkins's Q(λ)"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Peng's Q(λ)"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Off-Policy Eligibility Traces"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Importance Sampling with Traces"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Variable λ Methods"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "Implementation Efficiency"
    },
    {
      "domain": "n-Step and Eligibility Trace Methods",
      "subdomain": "Advanced Trace Methods",
      "topic": "True Online TD(λ)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Need for Function Approximation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Generalization in RL"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Feature Engineering"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Linear Function Approximation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Nonlinear Function Approximation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Approximation Error"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "State Aggregation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Value Function Approximation Foundations",
      "topic": "Parametric Value Functions"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Gradient Descent for Value Functions"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Stochastic Gradient Descent (SGD)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Mean Squared Value Error"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "TD Learning with Function Approximation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Semi-Gradient Methods"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Deadly Triad of RL"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Convergence with Function Approximation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Gradient-Based Methods",
      "topic": "Update Rule Derivation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "Neural Network Function Approximators"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "Deep Q-Network Architecture"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "Experience Replay Buffer"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "Target Network Stabilization"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "DQN Training Algorithm"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "Atari Game Benchmarks"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "Overestimation Bias in DQN"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Deep Q-Networks (DQN)",
      "topic": "DQN Variants and Extensions"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Parametric Policy Representation"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Softmax Policy Parameterization"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Gaussian Policy Parameterization"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Neural Network Policies"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Policy Gradient Theorem"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Score Function Estimator"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Log Derivative Trick"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Policy Parameterization",
      "topic": "Direct Policy Optimization"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "REINFORCE Algorithm"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "Monte Carlo Policy Gradient"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "REINFORCE with Baseline"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "Variance Reduction Techniques"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "Baseline Function Selection"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "Policy Gradient Update Rule"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "Unbiased Gradient Estimates"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "REINFORCE and Variants",
      "topic": "REINFORCE Convergence"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Actor-Critic Architecture"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Value Function as Critic"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Policy as Actor"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Advantage Function"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "A3C (Asynchronous Advantage Actor-Critic)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "A2C (Advantage Actor-Critic)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "TD Error for Policy Gradients"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Bootstrapped Policy Gradients"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "Trust Region Policy Optimization (TRPO)"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "KL Divergence Constraint"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "Natural Policy Gradient"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "Conjugate Gradient Method"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "Fisher Information Matrix"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "Line Search Procedures"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "Conservative Policy Updates"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Trust Region Methods",
      "topic": "TRPO Performance Guarantees"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "PPO Algorithm"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "Clipped Surrogate Objective"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "Importance Sampling Ratio Clipping"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "Adaptive KL Penalty"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "PPO vs. TRPO Comparison"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "Multiple Epochs of Minibatch Updates"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "PPO Implementation Simplicity"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Proximal Policy Optimization",
      "topic": "PPO Empirical Performance"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Deterministic Policy Gradient Theorem"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "DDPG (Deep Deterministic Policy Gradient)"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Continuous Action Spaces"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Ornstein-Uhlenbeck Noise"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "TD3 (Twin Delayed DDPG)"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Clipped Double-Q Learning"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Target Policy Smoothing"
    },
    {
      "domain": "Advanced Policy Optimization",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Delayed Policy Updates"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Transition Model Learning"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Reward Model Learning"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Supervised Learning for Models"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Model Accuracy and Errors"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Sample Efficiency of Model-Based RL"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Table-Based Models"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Parametric Model Approximation"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Model Learning",
      "topic": "Neural Network Dynamics Models"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Dyna Architecture"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Dyna-Q Algorithm"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Integrated Planning and Learning"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Simulated Experience Generation"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Real vs. Simulated Experience"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Model Errors and Their Impact"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Trajectory Sampling"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Planning with Models",
      "topic": "Background Planning"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Heuristic Search Methods"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Monte Carlo Tree Search (MCTS)"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Upper Confidence Bounds for Trees (UCT)"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Rollout Policies"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "AlphaGo and MCTS"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Simulation-Based Planning"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Tree Policy vs. Default Policy"
    },
    {
      "domain": "Model-Based Reinforcement Learning",
      "subdomain": "Search and Planning",
      "topic": "Planning Depth Considerations"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Epsilon-Greedy Exploration"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Boltzmann Exploration"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Random Noise Addition"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Action Space Dithering"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Decaying Exploration Rates"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Entropy Regularization"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Uniform Random Exploration"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Undirected Exploration",
      "topic": "Parameter Space Noise"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Upper Confidence Bounds (UCB)"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Thompson Sampling"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Optimism in Face of Uncertainty"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Count-Based Exploration Bonuses"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Intrinsic Motivation"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Curiosity-Driven Exploration"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Information Gain Methods"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Directed Exploration",
      "topic": "Posterior Sampling"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Exploration via Disagreement"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Prediction Error as Exploration Signal"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Random Network Distillation (RND)"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Novelty Detection"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "State Visitation Counts"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Hash-Based State Counting"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Empowerment and Information Theory"
    },
    {
      "domain": "Exploration Strategies",
      "subdomain": "Advanced Exploration Methods",
      "topic": "Goal-Conditioned Exploration"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Importance Sampling for Off-Policy"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Per-Decision Importance Sampling"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Doubly Robust Estimation"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Off-Policy Policy Evaluation"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Behavior Policy vs. Target Policy"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "High Variance in Off-Policy Methods"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Off-Policy Correction Techniques"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Off-Policy Evaluation",
      "topic": "Evaluation Metrics"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Offline RL Problem Formulation"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Batch-Constrained RL"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Conservative Q-Learning (CQL)"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Distribution Shift Challenges"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Extrapolation Error"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Policy Constraints for Offline RL"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Behavior Cloning from Fixed Dataset"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Batch and Offline RL",
      "topic": "Offline Policy Optimization"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Behavioral Cloning"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Supervised Learning from Demonstrations"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Dataset Aggregation (DAgger)"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Inverse Reinforcement Learning (IRL)"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Apprenticeship Learning"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Generative Adversarial Imitation Learning (GAIL)"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Learning from Human Preferences"
    },
    {
      "domain": "Off-Policy and Offline Reinforcement Learning",
      "subdomain": "Imitation Learning",
      "topic": "Expert Demonstration Requirements"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Continuous State Spaces"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Continuous Action Spaces"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Action Space Discretization"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Policy for Continuous Actions"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Soft Actor-Critic (SAC)"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Maximum Entropy RL"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Stochastic Policies for Continuous Actions"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Continuous Control",
      "topic": "Torque and Force Control"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "Partially Observable MDPs (POMDPs)"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "Observation vs. State Distinction"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "Belief States"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "Recurrent Neural Networks for POMDPs"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "LSTM and GRU for Memory"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "History-Based Policies"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "State Estimation Methods"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Partial Observability",
      "topic": "Observation Models"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Multi-Agent RL Fundamentals"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Cooperative vs. Competitive Settings"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Nash Equilibrium in Multi-Agent RL"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Communication in Multi-Agent Systems"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Hierarchical Reinforcement Learning"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Options Framework"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Temporal Abstraction"
    },
    {
      "domain": "Special Topics and Extensions",
      "subdomain": "Multi-Agent and Hierarchical RL",
      "topic": "Skill Discovery and Reuse"
    }
  ],
  "stats": {
    "entry_count": 336,
    "input_tokens": 5003,
    "output_tokens": 9703,
    "cost_usd": 0.0257584,
    "latency_ms": 46686.27643585205,
    "duration_seconds": 49.102031
  }
}