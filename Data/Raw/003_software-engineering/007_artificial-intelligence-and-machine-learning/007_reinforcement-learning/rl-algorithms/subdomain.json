{
  "subdomain_title": "RL Algorithms",
  "domain_title": "Reinforcement Learning",
  "category_title": "Software Engineering",
  "curriculum_type": "software-engineering",
  "cleaned_at": "2025-12-30T09:12:18.784358",
  "model_used": "google/gemini-2.5-flash",
  "entries": [
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Agent-Environment Interface"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Goals and Rewards"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Returns and Episodes"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Markov Property"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Value Functions"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Optimal Value Functions"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Bellman Equations"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Markov Decision Processes",
      "topic": "Policy Definitions"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Action-Value Methods"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Incremental Implementation"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Epsilon-Greedy Selection"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Optimistic Initial Values"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Upper-Confidence-Bound (UCB)"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Gradient Bandits"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Multi-Armed Bandits",
      "topic": "Contextual Bandits"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Exploration-Exploitation",
      "topic": "Epsilon-Greedy Strategies"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Exploration-Exploitation",
      "topic": "Softmax Action Selection"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Exploration-Exploitation",
      "topic": "Thompson Sampling"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Exploration-Exploitation",
      "topic": "Optimism Under Uncertainty"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Exploration-Exploitation",
      "topic": "Curiosity-Driven Methods"
    },
    {
      "domain": "Foundational Concepts",
      "subdomain": "Exploration-Exploitation",
      "topic": "Count-Based Exploration"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Policy Evaluation"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Policy Improvement"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Policy Iteration"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Value Iteration"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Asynchronous Dynamic Programming"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Generalized Policy Iteration"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Dynamic Programming",
      "topic": "Principle of Optimality"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "First-Visit MC Prediction"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "Every-Visit MC Prediction"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "MC Estimation of Action Values"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "MC Control with Exploring Starts"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "On-Policy MC Control"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "Off-Policy MC Prediction"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "Importance Sampling"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Monte Carlo Methods",
      "topic": "Incremental MC Implementation"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "TD(0) Prediction"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "TD Error"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "SARSA (On-Policy TD Control)"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "Q-Learning (Off-Policy TD Control)"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "Expected SARSA"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "Double Q-Learning"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "n-Step TD Methods"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Temporal-Difference Learning",
      "topic": "TD(λ) with Eligibility Traces"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "Dyna-Q"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "Dyna-Q+"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "Prioritized Sweeping"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "Trajectory Sampling"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "Real-Time Dynamic Programming"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "Monte Carlo Tree Search (MCTS)"
    },
    {
      "domain": "Tabular Solution Methods",
      "subdomain": "Planning with Tabular Methods",
      "topic": "UCT (Upper Confidence Trees)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "DQN (Deep Q-Network)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Experience Replay"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Target Networks"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Double DQN (DDQN)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Dueling DQN"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Prioritized Experience Replay (PER)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Noisy DQN"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Deep Q-Networks",
      "topic": "Rainbow DQN"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Distributional RL",
      "topic": "C51 (Categorical DQN)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Distributional RL",
      "topic": "Quantile Regression DQN (QR-DQN)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Distributional RL",
      "topic": "Implicit Quantile Networks (IQN)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Distributional RL",
      "topic": "Fully Parameterized Quantile Function (FQF)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Distributional RL",
      "topic": "Distributional Policy Gradients"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Distributional RL",
      "topic": "Risk-Sensitive RL"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Recurrent Q-Networks",
      "topic": "Deep Recurrent Q-Learning (DRQN)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Recurrent Q-Networks",
      "topic": "R2D2 (Recurrent Replay Distributed DQN)"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Recurrent Q-Networks",
      "topic": "LSTM-based Q-Networks"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Recurrent Q-Networks",
      "topic": "Recurrent Experience Replay"
    },
    {
      "domain": "Value-Based Deep RL",
      "subdomain": "Recurrent Q-Networks",
      "topic": "Stored State vs. Zero State"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Basic Policy Gradients",
      "topic": "Policy Gradient Theorem"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Basic Policy Gradients",
      "topic": "REINFORCE"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Basic Policy Gradients",
      "topic": "REINFORCE with Baseline"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Basic Policy Gradients",
      "topic": "Advantage Function"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Basic Policy Gradients",
      "topic": "Natural Policy Gradient"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Basic Policy Gradients",
      "topic": "Vanilla Policy Gradient (VPG)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Advantage Actor-Critic (A2C)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Asynchronous Advantage Actor-Critic (A3C)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Generalized Advantage Estimation (GAE)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "TD(λ) Actor-Critic"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Eligibility Traces for Actor-Critic"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Actor-Critic Methods",
      "topic": "Soft Actor-Critic (SAC)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Trust Region Methods",
      "topic": "Trust Region Policy Optimization (TRPO)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Trust Region Methods",
      "topic": "Proximal Policy Optimization (PPO)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Trust Region Methods",
      "topic": "PPO-Clip"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Trust Region Methods",
      "topic": "PPO-Penalty"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Trust Region Methods",
      "topic": "Kronecker-Factored Trust Region (ACKTR)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Trust Region Methods",
      "topic": "Actor-Critic with Experience Replay (ACER)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Deterministic Policy Gradient (DPG)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Deep Deterministic Policy Gradient (DDPG)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Twin Delayed DDPG (TD3)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Soft Actor-Critic (SAC)"
    },
    {
      "domain": "Policy Gradient Methods",
      "subdomain": "Deterministic Policy Gradients",
      "topic": "Stochastic Value Gradients (SVG)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Forward Dynamics Models"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Inverse Dynamics Models"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Gaussian Process Models (PILCO)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Neural Network Dynamics Models"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Ensemble Models (PETS)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Probabilistic Dynamics"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Learned Dynamics Models",
      "topic": "Uncertainty Quantification"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "Model Predictive Control (MPC)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "Shooting Methods (Random Shooting, CEM)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "Dyna Architecture"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "Model-Based Value Expansion (MBVE)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "MBMF (Model-Based Model-Free)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "Model-Ensemble Trust-Region (ME-TRPO)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Planning with Learned Models",
      "topic": "Trajectory Optimization"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "World Models"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "Dreamer (V1, V2, V3)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "Recurrent State Space Models (RSSM)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "PlaNet"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "TD-MPC (Temporal Difference MPC)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "Latent Imagination"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Latent Space Models",
      "topic": "Variational Autoencoders for Dynamics"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Model-Based Policy Optimization",
      "topic": "MBPO (Model-Based Policy Optimization)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Model-Based Policy Optimization",
      "topic": "STEVE (Stochastic Ensemble Value Expansion)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Model-Based Policy Optimization",
      "topic": "MB-MPO (Model-Based MPO)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Model-Based Policy Optimization",
      "topic": "Guided Policy Search (GPS)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Model-Based Policy Optimization",
      "topic": "Probabilistic Inference for Learning Control (PILCO)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Model-Based Policy Optimization",
      "topic": "SLBO (Sample-Efficient Learning by Backpropagation)"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "MuZero"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "Stochastic MuZero"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "Gumbel MuZero"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "EfficientZero"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "Sampled MuZero"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "AlphaZero"
    },
    {
      "domain": "Model-Based RL",
      "subdomain": "Value-Equivalent Models",
      "topic": "Expert Iteration (ExIt)"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Count-Based Exploration"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Pseudocount Methods"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Intrinsic Curiosity Module (ICM)"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Random Network Distillation (RND)"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Prediction Error as Curiosity"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Empowerment"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Intrinsic Motivation",
      "topic": "Information Gain"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Uncertainty-Based Exploration",
      "topic": "Bayesian Exploration"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Uncertainty-Based Exploration",
      "topic": "Thompson Sampling"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Uncertainty-Based Exploration",
      "topic": "Posterior Sampling for RL (PSRL)"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Uncertainty-Based Exploration",
      "topic": "UCB-Based Methods"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Uncertainty-Based Exploration",
      "topic": "Optimism in Face of Uncertainty"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Uncertainty-Based Exploration",
      "topic": "Bootstrap DQN"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Unsupervised RL",
      "topic": "DIAYN (Diversity is All You Need)"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Unsupervised RL",
      "topic": "Variational Intrinsic Control (VIC)"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Unsupervised RL",
      "topic": "VALOR"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Unsupervised RL",
      "topic": "Skill Discovery"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Unsupervised RL",
      "topic": "Goal-Conditioned Exploration"
    },
    {
      "domain": "Exploration Methods",
      "subdomain": "Unsupervised RL",
      "topic": "Maximum Entropy Exploration"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Temporal Abstraction",
      "topic": "Options Framework"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Temporal Abstraction",
      "topic": "Semi-Markov Decision Processes (SMDPs)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Temporal Abstraction",
      "topic": "Intra-Option Learning"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Temporal Abstraction",
      "topic": "Option-Critic Architecture"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Temporal Abstraction",
      "topic": "Termination Functions"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Temporal Abstraction",
      "topic": "Option Discovery"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Goal-Conditioned RL",
      "topic": "Universal Value Function Approximators (UVFA)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Goal-Conditioned RL",
      "topic": "Hindsight Experience Replay (HER)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Goal-Conditioned RL",
      "topic": "Goal-Conditioned Policies"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Goal-Conditioned RL",
      "topic": "Automatic Goal Generation"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Goal-Conditioned RL",
      "topic": "Subgoal Discovery"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Goal-Conditioned RL",
      "topic": "Hierarchical Actor-Critic (HAC)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Feudal Architectures",
      "topic": "FeUdal Networks (FuN)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Feudal Architectures",
      "topic": "Manager-Worker Hierarchy"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Feudal Architectures",
      "topic": "HIRO (Data Efficient HRL)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Feudal Architectures",
      "topic": "Director-Actor-Critic"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Feudal Architectures",
      "topic": "Strategic Attentive Writer (STRAW)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Feudal Architectures",
      "topic": "Hierarchical DQN (h-DQN)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Task Decomposition",
      "topic": "MAXQ Value Function Decomposition"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Task Decomposition",
      "topic": "HAM (Hierarchical Abstract Machines)"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Task Decomposition",
      "topic": "Hierarchical Policy Gradients"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Task Decomposition",
      "topic": "Recursive Reasoning"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Task Decomposition",
      "topic": "Bottleneck Discovery"
    },
    {
      "domain": "Hierarchical RL",
      "subdomain": "Task Decomposition",
      "topic": "Graph-Based Abstraction"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Independent Learning",
      "topic": "Independent Q-Learning (IQL)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Independent Learning",
      "topic": "Independent Actor-Critic (IA2C)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Independent Learning",
      "topic": "Independent PPO (IPPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Independent Learning",
      "topic": "Independent DDPG (IDDPG)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Independent Learning",
      "topic": "Independent TRPO (ITRPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Independent Learning",
      "topic": "Hysteretic Q-Learning"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Centralized Training Decentralized Execution",
      "topic": "MADDPG (Multi-Agent DDPG)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Centralized Training Decentralized Execution",
      "topic": "MAPPO (Multi-Agent PPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Centralized Training Decentralized Execution",
      "topic": "MAA2C (Multi-Agent A2C)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Centralized Training Decentralized Execution",
      "topic": "MATRPO (Multi-Agent TRPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Centralized Training Decentralized Execution",
      "topic": "Centralized Critic"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Centralized Training Decentralized Execution",
      "topic": "Communication Protocols"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Value Decomposition",
      "topic": "VDN (Value Decomposition Network)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Value Decomposition",
      "topic": "QMIX (Monotonic Value Factorization)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Value Decomposition",
      "topic": "QTRAN (Q-Transformation)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Value Decomposition",
      "topic": "QPLEX"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Value Decomposition",
      "topic": "Weighted QMIX"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Value Decomposition",
      "topic": "VDPPO (Value Decomposition PPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Cooperative MARL",
      "topic": "COMA (Counterfactual Multi-Agent)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Cooperative MARL",
      "topic": "FACMAC (Factored Multi-Agent Critic)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Cooperative MARL",
      "topic": "HAPPO (Heterogeneous-Agent PPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Cooperative MARL",
      "topic": "HATRPO (Heterogeneous-Agent TRPO)"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Cooperative MARL",
      "topic": "Joint Action Learning"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Cooperative MARL",
      "topic": "Credit Assignment"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Competitive and Mixed MARL",
      "topic": "Nash Q-Learning"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Competitive and Mixed MARL",
      "topic": "Minimax Q-Learning"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Competitive and Mixed MARL",
      "topic": "Self-Play"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Competitive and Mixed MARL",
      "topic": "Population-Based Training"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Competitive and Mixed MARL",
      "topic": "Opponent Modeling"
    },
    {
      "domain": "Multi-Agent RL",
      "subdomain": "Competitive and Mixed MARL",
      "topic": "Game-Theoretic RL"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Behavioral Cloning",
      "topic": "Supervised Behavioral Cloning"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Behavioral Cloning",
      "topic": "DAgger (Dataset Aggregation)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Behavioral Cloning",
      "topic": "DART (Disturbance-Based Adaptation)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Behavioral Cloning",
      "topic": "SMILe (Sequential Model Imitation)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Behavioral Cloning",
      "topic": "Covariate Shift Problem"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Behavioral Cloning",
      "topic": "Query-Efficient Imitation"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Inverse Reinforcement Learning",
      "topic": "Maximum Entropy IRL"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Inverse Reinforcement Learning",
      "topic": "Maximum Causal Entropy IRL"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Inverse Reinforcement Learning",
      "topic": "Apprenticeship Learning"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Inverse Reinforcement Learning",
      "topic": "Guided Cost Learning (GCL)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Inverse Reinforcement Learning",
      "topic": "Relative Entropy IRL"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Inverse Reinforcement Learning",
      "topic": "Bayesian IRL"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Adversarial Imitation Learning",
      "topic": "GAIL (Generative Adversarial Imitation Learning)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Adversarial Imitation Learning",
      "topic": "AIRL (Adversarial Inverse RL)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Adversarial Imitation Learning",
      "topic": "VAIL (Variational Adversarial Imitation)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Adversarial Imitation Learning",
      "topic": "InfoGAIL"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Adversarial Imitation Learning",
      "topic": "Discriminator Design"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Adversarial Imitation Learning",
      "topic": "Reward Recovery"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Learning from Observation",
      "topic": "Third-Person Imitation"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Learning from Observation",
      "topic": "Domain Confusion"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Learning from Observation",
      "topic": "Time-Contrastive Networks (TCN)"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Learning from Observation",
      "topic": "Video Imitation"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Learning from Observation",
      "topic": "Cross-Embodiment Imitation"
    },
    {
      "domain": "Imitation Learning and Inverse RL",
      "subdomain": "Learning from Observation",
      "topic": "State-Only Imitation"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Conservative Value Estimation",
      "topic": "Conservative Q-Learning (CQL)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Conservative Value Estimation",
      "topic": "Batch-Constrained Q-Learning (BCQ)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Conservative Value Estimation",
      "topic": "BEAR (Bootstrapping Error Accumulation Reduction)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Conservative Value Estimation",
      "topic": "BRAC (Behavior Regularized Actor-Critic)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Conservative Value Estimation",
      "topic": "TD3+BC"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Conservative Value Estimation",
      "topic": "IQL (Implicit Q-Learning)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Model-Based Offline RL",
      "topic": "MOPO (Model-Based Offline Policy Optimization)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Model-Based Offline RL",
      "topic": "COMBO (Conservative Offline Model-Based)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Model-Based Offline RL",
      "topic": "RAMBO (Robust Adversarial Model-Based)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Model-Based Offline RL",
      "topic": "MOReL (Model-Based Offline RL)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Model-Based Offline RL",
      "topic": "Pessimism Principle"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Model-Based Offline RL",
      "topic": "Uncertainty Penalties"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Offline-to-Online RL",
      "topic": "Offline Pretraining with Online Finetuning"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Offline-to-Online RL",
      "topic": "Advantage Weighted Regression (AWR)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Offline-to-Online RL",
      "topic": "Cal-QL (Calibrated Q-Learning)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Offline-to-Online RL",
      "topic": "Hybrid RL"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Offline-to-Online RL",
      "topic": "Balanced Replay"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Offline-to-Online RL",
      "topic": "Safe Policy Improvement"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Dataset Quality and Coverage",
      "topic": "Data Collection Strategies"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Dataset Quality and Coverage",
      "topic": "Dataset Diagnostics"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Dataset Quality and Coverage",
      "topic": "Behavior Policy Evaluation"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Dataset Quality and Coverage",
      "topic": "Off-Policy Evaluation (OPE)"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Dataset Quality and Coverage",
      "topic": "Importance Sampling Corrections"
    },
    {
      "domain": "Offline RL and Batch RL",
      "subdomain": "Dataset Quality and Coverage",
      "topic": "Marginal Importance Weights"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Meta-Learning for RL",
      "topic": "MAML (Model-Agnostic Meta-Learning)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Meta-Learning for RL",
      "topic": "RL² (Fast RL via Slow RL)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Meta-Learning for RL",
      "topic": "SNAIL (Simple Neural Attentive Meta-Learner)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Meta-Learning for RL",
      "topic": "ProMP (Probabilistic Meta-Policy)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Meta-Learning for RL",
      "topic": "Meta-Q-Learning"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Meta-Learning for RL",
      "topic": "Gradient-Based Meta-RL"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Contextual and Task-Conditioned RL",
      "topic": "Context Encoders"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Contextual and Task-Conditioned RL",
      "topic": "Task Embeddings"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Contextual and Task-Conditioned RL",
      "topic": "Multi-Task RL"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Contextual and Task-Conditioned RL",
      "topic": "Task Inference Networks"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Contextual and Task-Conditioned RL",
      "topic": "PEARL (Probabilistic Embeddings)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Contextual and Task-Conditioned RL",
      "topic": "VariBAD (Variational Bayes Adaptive Deep RL)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Transfer and Lifelong Learning",
      "topic": "Progressive Neural Networks"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Transfer and Lifelong Learning",
      "topic": "PathNet"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Transfer and Lifelong Learning",
      "topic": "PackNet"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Transfer and Lifelong Learning",
      "topic": "Policy Distillation"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Transfer and Lifelong Learning",
      "topic": "Successor Features"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Transfer and Lifelong Learning",
      "topic": "Generalized Policy Improvement (GPI)"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Few-Shot RL",
      "topic": "Rapid Adaptation"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Few-Shot RL",
      "topic": "Experience Replay for Meta-RL"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Few-Shot RL",
      "topic": "Context-Based Adaptation"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Few-Shot RL",
      "topic": "Model-Based Meta-RL"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Few-Shot RL",
      "topic": "Zero-Shot Transfer"
    },
    {
      "domain": "Meta-RL and Transfer Learning",
      "subdomain": "Few-Shot RL",
      "topic": "Domain Randomization"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Linear Function Approximation",
      "topic": "Linear Value Function Approximation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Linear Function Approximation",
      "topic": "Feature Construction"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Linear Function Approximation",
      "topic": "Tile Coding"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Linear Function Approximation",
      "topic": "Radial Basis Functions (RBF)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Linear Function Approximation",
      "topic": "Fourier Basis"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Linear Function Approximation",
      "topic": "Coarse Coding"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Neural Network Architectures",
      "topic": "Multilayer Perceptrons (MLP)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Neural Network Architectures",
      "topic": "Convolutional Neural Networks (CNN)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Neural Network Architectures",
      "topic": "Recurrent Neural Networks (RNN/LSTM/GRU)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Neural Network Architectures",
      "topic": "Transformers for RL"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Neural Network Architectures",
      "topic": "Attention Mechanisms"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Neural Network Architectures",
      "topic": "Graph Neural Networks (GNN)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Stability and Convergence",
      "topic": "Deadly Triad Problem"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Stability and Convergence",
      "topic": "Gradient Temporal-Difference (GTD)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Stability and Convergence",
      "topic": "GTD2 and TDC"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Stability and Convergence",
      "topic": "Emphatic TD"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Stability and Convergence",
      "topic": "Residual Gradients"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Stability and Convergence",
      "topic": "Target Networks"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Representation Learning",
      "topic": "Autoencoders for State Representation"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Representation Learning",
      "topic": "Contrastive Learning"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Representation Learning",
      "topic": "CURL (Contrastive Unsupervised RL)"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Representation Learning",
      "topic": "Self-Supervised Learning"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Representation Learning",
      "topic": "Successor Representations"
    },
    {
      "domain": "Function Approximation",
      "subdomain": "Representation Learning",
      "topic": "Auxiliary Tasks"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Partially Observable Environments",
      "topic": "POMDP (Partially Observable MDP)"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Partially Observable Environments",
      "topic": "Belief State Planning"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Partially Observable Environments",
      "topic": "Memory-Augmented Networks"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Partially Observable Environments",
      "topic": "DRQN (Deep Recurrent Q-Network)"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Partially Observable Environments",
      "topic": "Neural Episodic Control (NEC)"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Partially Observable Environments",
      "topic": "MERLIN"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Continuous Action Spaces",
      "topic": "Action Space Discretization"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Continuous Action Spaces",
      "topic": "Normalized Advantage Functions (NAF)"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Continuous Action Spaces",
      "topic": "Parameterized Actions"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Continuous Action Spaces",
      "topic": "Hybrid Action Spaces"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Continuous Action Spaces",
      "topic": "Stochastic Policy Gradients"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Continuous Action Spaces",
      "topic": "Deterministic Policy Gradients"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Sparse and Delayed Rewards",
      "topic": "Reward Shaping"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Sparse and Delayed Rewards",
      "topic": "Hindsight Learning"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Sparse and Delayed Rewards",
      "topic": "Curriculum Learning"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Sparse and Delayed Rewards",
      "topic": "Episodic Memory"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Sparse and Delayed Rewards",
      "topic": "Go-Explore"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Sparse and Delayed Rewards",
      "topic": "Never Give Up (NGU)"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Average Reward Setting",
      "topic": "R-Learning"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Average Reward Setting",
      "topic": "Average Reward TD"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Average Reward Setting",
      "topic": "Differential Q-Learning"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Average Reward Setting",
      "topic": "Relative Value Iteration"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Average Reward Setting",
      "topic": "Gain-Bias Functions"
    },
    {
      "domain": "Specialized RL Algorithms",
      "subdomain": "Average Reward Setting",
      "topic": "Continuing Tasks"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Constrained RL",
      "topic": "Constrained MDPs (CMDPs)"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Constrained RL",
      "topic": "Constrained Policy Optimization (CPO)"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Constrained RL",
      "topic": "Lagrangian Methods"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Constrained RL",
      "topic": "Safe Policy Gradients"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Constrained RL",
      "topic": "Reward-Cost Trade-offs"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Constrained RL",
      "topic": "Barrier Functions"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Risk-Sensitive RL",
      "topic": "Conditional Value at Risk (CVaR)"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Risk-Sensitive RL",
      "topic": "Mean-Variance Optimization"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Risk-Sensitive RL",
      "topic": "Worst-Case Optimization"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Risk-Sensitive RL",
      "topic": "Robust MDPs"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Risk-Sensitive RL",
      "topic": "Distributional Perspectives"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Risk-Sensitive RL",
      "topic": "Risk-Averse Policies"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Safe Exploration",
      "topic": "Safety Layers"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Safe Exploration",
      "topic": "Shield Functions"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Safe Exploration",
      "topic": "Human-in-the-Loop RL"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Safe Exploration",
      "topic": "Learning from Demonstrations for Safety"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Safe Exploration",
      "topic": "Safe Policy Improvement"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Safe Exploration",
      "topic": "Constrained Exploration"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Robustness to Distribution Shift",
      "topic": "Domain Randomization"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Robustness to Distribution Shift",
      "topic": "Adversarial Training"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Robustness to Distribution Shift",
      "topic": "Robust Value Functions"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Robustness to Distribution Shift",
      "topic": "Uncertainty Quantification"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Robustness to Distribution Shift",
      "topic": "Sim-to-Real Transfer"
    },
    {
      "domain": "Safe and Robust RL",
      "subdomain": "Robustness to Distribution Shift",
      "topic": "Model Ensembles"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Preference-Based RL",
      "topic": "Learning from Human Preferences"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Preference-Based RL",
      "topic": "Dueling Bandits"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Preference-Based RL",
      "topic": "Preference Elicitation"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Preference-Based RL",
      "topic": "Ranking-Based Learning"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Preference-Based RL",
      "topic": "Comparison Queries"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Preference-Based RL",
      "topic": "Active Preference Learning"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Reward Shaping",
      "topic": "Potential-Based Reward Shaping"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Reward Shaping",
      "topic": "Intrinsic Rewards"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Reward Shaping",
      "topic": "Auxiliary Rewards"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Reward Shaping",
      "topic": "Reward Engineering"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Reward Shaping",
      "topic": "Optimal Reward Design"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Reward Shaping",
      "topic": "Shaping Convergence Guarantees"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Multi-Objective RL",
      "topic": "Pareto Optimality"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Multi-Objective RL",
      "topic": "Scalarization Methods"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Multi-Objective RL",
      "topic": "Pareto Front Approximation"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Multi-Objective RL",
      "topic": "Weight Adaptation"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Multi-Objective RL",
      "topic": "Multi-Objective Policy Gradients"
    },
    {
      "domain": "Reward Learning and Specification",
      "subdomain": "Multi-Objective RL",
      "topic": "Trade-off Discovery"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Parallel and Distributed Training",
      "topic": "A3C (Asynchronous Advantage Actor-Critic)"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Parallel and Distributed Training",
      "topic": "IMPALA (Importance Weighted Actor-Learner)"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Parallel and Distributed Training",
      "topic": "Ape-X (Distributed Prioritized Experience Replay)"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Parallel and Distributed Training",
      "topic": "R2D2 (Recurrent Replay Distributed DQN)"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Parallel and Distributed Training",
      "topic": "Parameter Server Architectures"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Parallel and Distributed Training",
      "topic": "Gradient Aggregation"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Sample Efficiency",
      "topic": "Experience Replay Strategies"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Sample Efficiency",
      "topic": "Data Augmentation"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Sample Efficiency",
      "topic": "Model-Based Acceleration"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Sample Efficiency",
      "topic": "Auxiliary Tasks"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Sample Efficiency",
      "topic": "Self-Supervised Pretraining"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Sample Efficiency",
      "topic": "Transfer from Simulators"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Computational Efficiency",
      "topic": "Network Compression"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Computational Efficiency",
      "topic": "Knowledge Distillation"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Computational Efficiency",
      "topic": "Pruning"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Computational Efficiency",
      "topic": "Quantization"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Computational Efficiency",
      "topic": "Early Stopping Strategies"
    },
    {
      "domain": "Scalability and Distributed RL",
      "subdomain": "Computational Efficiency",
      "topic": "Efficient Architecture Search"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Convergence and Optimality",
      "topic": "Policy Improvement Theorem"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Convergence and Optimality",
      "topic": "Contraction Mappings"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Convergence and Optimality",
      "topic": "Fixed Point Theorems"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Convergence and Optimality",
      "topic": "Convergence of TD Learning"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Convergence and Optimality",
      "topic": "Convergence of Q-Learning"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Convergence and Optimality",
      "topic": "Optimality Conditions"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Sample Complexity",
      "topic": "PAC Learning Bounds"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Sample Complexity",
      "topic": "Regret Bounds"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Sample Complexity",
      "topic": "Sample Complexity of Exploration"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Sample Complexity",
      "topic": "Information-Theoretic Bounds"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Sample Complexity",
      "topic": "Minimax Lower Bounds"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Sample Complexity",
      "topic": "Instance-Dependent Bounds"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Function Approximation Theory",
      "topic": "Approximation Error"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Function Approximation Theory",
      "topic": "Generalization Error"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Function Approximation Theory",
      "topic": "Bias-Variance Tradeoff"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Function Approximation Theory",
      "topic": "Universal Approximation"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Function Approximation Theory",
      "topic": "Feature Selection Theory"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Function Approximation Theory",
      "topic": "Representational Capacity"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Policy Gradient Theory",
      "topic": "Compatible Function Approximation"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Policy Gradient Theory",
      "topic": "Natural Gradient Justification"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Policy Gradient Theory",
      "topic": "Trust Region Analysis"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Policy Gradient Theory",
      "topic": "Monotonic Improvement Guarantees"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Policy Gradient Theory",
      "topic": "Variance Reduction Theory"
    },
    {
      "domain": "Theoretical Foundations",
      "subdomain": "Policy Gradient Theory",
      "topic": "Actor-Critic Convergence"
    }
  ],
  "stats": {
    "entry_count": 404,
    "input_tokens": 5989,
    "output_tokens": 11033,
    "cost_usd": 0.0293792,
    "latency_ms": 56391.63327217102,
    "duration_seconds": 58.794275
  }
}