<?xml version="1.0" encoding="UTF-8"?>
<topic_prompt version="2.0">
  <metadata>
    <topic_title>002_Incident Response Simulations</topic_title>
    <hierarchy>
      <category>Cybersecurity</category>
      <domain>002_Incident Response And Forensics</domain>
      <subdomain>002_Incident Response Lifecycle</subdomain>
      <entry_domain>001_Preparation</entry_domain>
      <entry_subdomain>014_Training and Awareness</entry_subdomain>
    </hierarchy>
    <voting_summary>
      <consensus>True</consensus>
      <approval>70.0%</approval>
      <voters>7</voters>
    </voting_summary>
    <generation_timestamp>2026-01-18T12:53:44.913689</generation_timestamp>
  </metadata>
  <learning_objectives level="bloom_taxonomy">
    <objective level="remember" measurable="true" verbs="define">Define key terminology</objective>
    <objective level="understand" measurable="true" verbs="explain">Explain core concepts</objective>
    <objective level="apply" measurable="true" verbs="apply">Apply knowledge to scenarios</objective>
    <objective level="analyze" measurable="true" verbs="analyze">Analyze relationships</objective>
  </learning_objectives>
  <active_learning>
    <discussion_prompt>In a group discussion, debate the following: 'To what extent do tabletop simulations adequately prepare teams for full-scale cyber incidents compared to technical drills? Use examples from NIST SP 800-61 Rev. 2 phases to support your arguments and identify when each type is most appropriate.' Encourage critical thinking by requiring participants to reference voter-suggested metrics like response time and gap identification.</discussion_prompt>
    <peer_teaching>Explain the key concepts to a partner without using technical jargon.</peer_teaching>
    <problem_solving>Given a scenario, apply the framework to solve the problem.</problem_solving>
  </active_learning>
  <scaffolding>
    <layer level="1" name="Foundation">
      <focus>Basic terminology and definitions</focus>
      <content/>
    </layer>
    <layer level="2" name="Components">
      <focus>Framework components and structure</focus>
      <content/>
    </layer>
    <layer level="3" name="Implementation">
      <focus>Practical implementation steps</focus>
      <content/>
    </layer>
    <layer level="4" name="Integration">
      <focus>Advanced integration and optimization</focus>
      <content/>
    </layer>
  </scaffolding>
  <flashcard_generation>
    <output_schema>
      <field name="question" type="string"/>
      <field name="correct_answer" type="string"/>
      <field name="distractors" type="[{'text': 'string', 'explanation': 'string'}]"/>
      <field name="explanation" type="string"/>
      <field name="bloom_level" type="enum"/>
      <field name="topic_hierarchy" type="object"/>
    </output_schema>
    <distractor_protocol>
      <step number="1">rules</step>
      <step number="2">quantity</step>
    </distractor_protocol>
    <system_prompt>You are an expert flashcard generator for cybersecurity education, specializing in '002_Incident Response Simulations' (Topic Hierarchy: Cybersecurity &gt; 002_Incident Response And Forensics &gt; 002_Incident Response Lifecycle &gt; 001_Preparation &gt; 014_Training and Awareness &gt; 002_Incident Response Simulations). Use NIST SP 800-61 Rev. 2 (2012, phases: Preparation, Detection &amp; Analysis, Containment/Eradication/Recovery, Post-Incident Activity) as primary sourceâ€”note Rev. 3 is not yet published. Incorporate NIST CSF 2.0 integration, simulation types (tabletop, functional, full-scale), voter priorities (completeness, pedagogy, accuracy), and research context on IR simulations for testing capabilities.
Generate 40-50 high-quality flashcards following this EXACT schema: JSON array of objects with 'front', 'back' (nested: answer, explanation, hints array, bloom_level, layer), 'type'.
**Pedagogical Integration:**
- Align to Bloom's objectives: [insert learning_objectives array here].
- Scaffold across 4 layers: [insert scaffolding array here].
- Ensure active learning tie-ins (e.g., questions prompting analysis like voter MCQs).
- Distractors: Strictly follow protocol [insert distractor_protocol here].
**Content Guidelines:**
- Cover completeness: Full NIST phases, simulation planning/execution, metrics/debriefs, concept map elements.
- Accuracy: No future Rev. 3 details; emphasize preparation/training.
- Voter Assessments: Include MCQ examples (e.g., purpose: test IR vs. tools; NIST doc: 800-61 vs. 800-53).
- Variety: 40% REMEMBER/UNDERSTAND (Layers 1-2), 60% APPLY/ANALYZE/EVALUATE/CREATE (Layers 3-4).
Output ONLY the JSON array. No additional text.</system_prompt>
  </flashcard_generation>
</topic_prompt>