<?xml version="1.0" encoding="UTF-8"?>
<topic_prompt version="2.0">
  <metadata>
    <topic_title>Configuration Compliance Checking</topic_title>
    <hierarchy>
      <category>Cybersecurity</category>
      <domain>002_Incident Response And Forensics</domain>
      <subdomain>002_Incident Response Lifecycle</subdomain>
      <entry_domain>008_Eradication</entry_domain>
      <entry_subdomain>006_Validation and Verification</entry_subdomain>
    </hierarchy>
    <voting_summary>
      <consensus>True</consensus>
      <approval>100.0%</approval>
      <voters>7</voters>
    </voting_summary>
    <generation_timestamp>2026-01-18T13:03:24.426499</generation_timestamp>
  </metadata>
  <learning_objectives level="bloom_taxonomy">
    <objective level="remember" measurable="true" verbs="define">Define key terminology</objective>
    <objective level="understand" measurable="true" verbs="explain">Explain core concepts</objective>
    <objective level="apply" measurable="true" verbs="apply">Apply knowledge to scenarios</objective>
    <objective level="analyze" measurable="true" verbs="analyze">Analyze relationships</objective>
  </learning_objectives>
  <active_learning>
    <discussion_prompt>In the eradication phase of incident response, debate whether automated compliance checking tools (e.g., OpenSCAP) should fully override manual verifications or complement them. Use NIST SP 800-128 examples to argue for risks of over-reliance on automation (e.g., false positives missing nuanced threats) vs. human oversight (e.g., contextual awareness in complex environments).</discussion_prompt>
    <peer_teaching>Explain the key concepts to a partner without using technical jargon.</peer_teaching>
    <problem_solving>Given a scenario, apply the framework to solve the problem.</problem_solving>
  </active_learning>
  <scaffolding>
    <layer level="1" name="Foundation">
      <focus>Basic terminology and definitions</focus>
      <content/>
    </layer>
    <layer level="2" name="Components">
      <focus>Framework components and structure</focus>
      <content/>
    </layer>
    <layer level="3" name="Implementation">
      <focus>Practical implementation steps</focus>
      <content/>
    </layer>
    <layer level="4" name="Integration">
      <focus>Advanced integration and optimization</focus>
      <content/>
    </layer>
  </scaffolding>
  <flashcard_generation>
    <output_schema>
      <field name="question" type="string"/>
      <field name="correct_answer" type="string"/>
      <field name="distractors" type="[{'text': 'string', 'explanation': 'string'}]"/>
      <field name="explanation" type="string"/>
      <field name="bloom_level" type="enum"/>
      <field name="topic_hierarchy" type="object"/>
    </output_schema>
    <distractor_protocol>
      <step number="1">rules</step>
    </distractor_protocol>
    <system_prompt>You are an expert flashcard generator for cybersecurity education, specializing in Configuration Compliance Checking (Topic Hierarchy: Level 1: Cybersecurity → Level 2: 002_Incident Response And Forensics → Level 3: 002_Incident Response Lifecycle → Level 4: 008_Eradication → Level 5: 006_Validation and Verification → Level 6: Configuration Compliance Checking). Generate high-quality, Anki-optimized flashcards supporting university pedagogy: Bloom's Taxonomy objectives (Remember: define CM/SecCM; Understand: role in IR eradication; Apply: verify configs; Analyze: auto vs manual; Evaluate: automation risks; Create: checklists), 4 scaffolding layers (Layer1 Foundation: basics/prereqs; Layer2 Components: NIST SP 800-128/800-53, CIS, SCAP/OpenSCAP; Layer3 Implementation: scan/assess/remediate; Layer4 Integration: full IR lifecycle), and active learning tie-ins (e.g., debate automation, peer-teach SecCM, scenario checklists).
Content must be grounded in research: NIST SP 800-128 (Security-Focused CM guide), SP 800-53 controls, CIS benchmarks; big picture: post-eradication validation prevents re-compromise.
Output exactly 50 flashcards as a JSON array of objects. Strictly follow schema: each card has 'front', 'back', 'type' (definition_recall/mcq/application/analysis_evaluation), 'bloom_level' (from objectives), 'layer' (layer1_foundation/etc.), 'explanation' (2-4 sentences + NIST ref + active learning link), and 'distractors' (array of 3 for MCQ only, with misconception notes). Balance: 12-13 cards per layer/Bloom level; 25% MCQ with distractor protocol (plausible misconceptions: SecCM=CM, one-time compliance, full automation override). Ensure active recall, no hints on front, explanations reference voter activities (e.g., 'Supports problem-solving exercise on RDP misconfigs').
Example card: {"front": "What is SecCM per NIST SP 800-128? A) General CM B) Security-focused CM C) Forensics D) Recovery", "back": "B) Security-focused CM", "type": "mcq", "bloom_level": "Remember", "layer": "layer1_foundation", "explanation": "SecCM emphasizes security configs (NIST SP 800-128). Builds foundation for peer-teaching. Distractors: A (confuses with CM), C (wrong phase), D (next lifecycle).", "distractors": ["A: Near-miss general CM", "C: Unrelated forensics", "D: Opposite lifecycle phase"]}.</system_prompt>
  </flashcard_generation>
</topic_prompt>