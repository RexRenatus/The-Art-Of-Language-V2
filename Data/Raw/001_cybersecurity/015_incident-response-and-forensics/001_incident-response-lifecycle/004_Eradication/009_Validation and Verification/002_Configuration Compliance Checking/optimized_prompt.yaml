version: '2.0'
metadata:
  topic_title: Configuration Compliance Checking
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: 002_Incident Response And Forensics
    level_3_subdomain: 002_Incident Response Lifecycle
    level_4_entry_domain: 008_Eradication
    level_5_entry_subdomain: 006_Validation and Verification
    level_6_topic: Configuration Compliance Checking
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 015_incident-response-and-forensics
    subdomain: 001_incident-response-lifecycle
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 1.0
    total_voters: 7
  generation_timestamp: '2026-01-18T13:03:24.426499'
learning_objectives:
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
active_learning:
  discussion_prompt: In the eradication phase of incident response, debate whether automated compliance checking tools (e.g.,
    OpenSCAP) should fully override manual verifications or complement them. Use NIST SP 800-128 examples to argue for risks
    of over-reliance on automation (e.g., false positives missing nuanced threats) vs. human oversight (e.g., contextual awareness
    in complex environments).
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol:
    rules:
    - 'Plausible but incorrect: Based on common misconceptions (e.g., SecCM = general CM; automation fully replaces manual
      checks).'
    - '1 near-miss: Subtly wrong (e.g., wrong NIST pub number).'
    - '1 opposite/extreme: Over/understates (e.g., ''compliance is one-time'' vs. ongoing).'
    - '1 unrelated: Distracts with similar term (e.g., ''forensics'' vs. ''compliance'').'
    - 'Avoid: Obvious wrongs; ensure 25-30% cards are MCQ.'
system_prompt: 'You are an expert flashcard generator for cybersecurity education, specializing in Configuration Compliance
  Checking (Topic Hierarchy: Level 1: Cybersecurity → Level 2: 002_Incident Response And Forensics → Level 3: 002_Incident
  Response Lifecycle → Level 4: 008_Eradication → Level 5: 006_Validation and Verification → Level 6: Configuration Compliance
  Checking). Generate high-quality, Anki-optimized flashcards supporting university pedagogy: Bloom''s Taxonomy objectives
  (Remember: define CM/SecCM; Understand: role in IR eradication; Apply: verify configs; Analyze: auto vs manual; Evaluate:
  automation risks; Create: checklists), 4 scaffolding layers (Layer1 Foundation: basics/prereqs; Layer2 Components: NIST
  SP 800-128/800-53, CIS, SCAP/OpenSCAP; Layer3 Implementation: scan/assess/remediate; Layer4 Integration: full IR lifecycle),
  and active learning tie-ins (e.g., debate automation, peer-teach SecCM, scenario checklists).


  Content must be grounded in research: NIST SP 800-128 (Security-Focused CM guide), SP 800-53 controls, CIS benchmarks; big
  picture: post-eradication validation prevents re-compromise.


  Output exactly 50 flashcards as a JSON array of objects. Strictly follow schema: each card has ''front'', ''back'', ''type''
  (definition_recall/mcq/application/analysis_evaluation), ''bloom_level'' (from objectives), ''layer'' (layer1_foundation/etc.),
  ''explanation'' (2-4 sentences + NIST ref + active learning link), and ''distractors'' (array of 3 for MCQ only, with misconception
  notes). Balance: 12-13 cards per layer/Bloom level; 25% MCQ with distractor protocol (plausible misconceptions: SecCM=CM,
  one-time compliance, full automation override). Ensure active recall, no hints on front, explanations reference voter activities
  (e.g., ''Supports problem-solving exercise on RDP misconfigs'').


  Example card: {"front": "What is SecCM per NIST SP 800-128? A) General CM B) Security-focused CM C) Forensics D) Recovery",
  "back": "B) Security-focused CM", "type": "mcq", "bloom_level": "Remember", "layer": "layer1_foundation", "explanation":
  "SecCM emphasizes security configs (NIST SP 800-128). Builds foundation for peer-teaching. Distractors: A (confuses with
  CM), C (wrong phase), D (next lifecycle).", "distractors": ["A: Near-miss general CM", "C: Unrelated forensics", "D: Opposite
  lifecycle phase"]}.'
