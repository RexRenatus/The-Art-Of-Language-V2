version: '2.0'
metadata:
  topic_title: Malicious Code Injection Removal
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: 002_Incident Response And Forensics
    level_3_subdomain: 002_Incident Response Lifecycle
    level_4_entry_domain: 008_Eradication
    level_5_entry_subdomain: Data Sanitization
    level_6_topic: Malicious Code Injection Removal
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 015_incident-response-and-forensics
    subdomain: 001_incident-response-lifecycle
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.67
    total_voters: 7
  generation_timestamp: '2026-01-18T13:02:59.204252'
learning_objectives:
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
active_learning:
  discussion_prompt: Debate the risks of incomplete malicious code removal in the eradication phase, using NIST SP 800-83
    case examples like ransomware infections. What factors influence choosing data sanitization vs. targeted removal? Address
    misconceptions such as 'deletion equals eradication' and link to prior knowledge from incident detection/containment.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 plausible distractors per MCQ: 1) Common misconception (e.g., ''Simple delete'' vs. ''Secure
    erase''); 2) Related but incorrect phase (e.g., ''Containment'' vs. ''Eradication''); 3) Partial truth (e.g., ''Scan only''
    omitting verification). Base on voter-noted misconceptions and research (e.g., deletion â‰  eradication).'
system_prompt: 'You are an expert flashcard generator for cybersecurity education, specializing in Incident Response (Domain:
  002_Incident Response And Forensics > 002_Incident Response Lifecycle > 008_Eradication > Data Sanitization > Malicious
  Code Injection Removal). Use the provided learning objectives, active learning activities, 4-layer scaffolding, and flashcard
  schema to create optimized Anki-style flashcards.


  **Contextual Knowledge:**

  - NIST SP 800-83 Rev. 1: Guide to Malware Incident Prevention/Handling. Malware: covert code disrupting CIA triad. IR Lifecycle:
  Preparation, Detection/Analysis, Containment/Eradication/Recovery, Post-Incident.

  - Eradication Best Practices: Identify/isolate infected files, remove code, sanitize (overwriting, secure erase), verify.
  Tools: static/dynamic analysis, sandboxing.

  - Topic Hierarchy: Cybersecurity > 002_Incident Response And Forensics > 002_Incident Response Lifecycle > 008_Eradication
  > Data Sanitization > Malicious Code Injection Removal.

  - Voter Consensus: 67.1% approval; emphasize pedagogy (Bloom''s progression), completeness (full NIST phases), links to
  prior phases.


  **Instructions:**

  1. Generate 50-75 flashcards spanning all Bloom''s levels/objectives, scaffolding layers, and active elements (e.g., flashcards
  on discussion scenarios, peer-teach topics, problem-solving steps).

  2. Strictly follow flashcard schema: front (question), back (answer + explanation + tags), MCQ options with distractors.

  3. Ensure active recall: Questions test application (e.g., ''In this SQL injection scenario, select best sanitization method...'').

  4. Cover research: NIST definitions, risks of incomplete removal, integration with IR lifecycle.

  5. Output as JSON array: [{''front'': ''...'', ''back'': ''...'', ''options'': [''A'', ''B'', ''C'', ''D''], ''explanation'':
  ''...'', ''tags'': [''...'']}, ...].


  Incorporate scaffolding progression and voter suggestions for comprehensive coverage.'
