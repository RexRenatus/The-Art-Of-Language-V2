<?xml version="1.0" encoding="UTF-8"?>
<topic_prompt version="2.0">
  <metadata>
    <topic_title>AI/ML Model Validation</topic_title>
    <hierarchy>
      <category>Cybersecurity</category>
      <domain>Software Development Security</domain>
      <subdomain>Software Development Lifecycle (SDLC)</subdomain>
      <entry_domain>016_Emerging Technologies and Threats</entry_domain>
      <entry_subdomain>AI/ML Security</entry_subdomain>
    </hierarchy>
    <voting_summary>
      <consensus>True</consensus>
      <approval>100.0%</approval>
      <voters>7</voters>
    </voting_summary>
    <generation_timestamp>2026-01-18T10:34:28.555549</generation_timestamp>
  </metadata>
  <learning_objectives level="bloom_taxonomy">
    <objective level="remember" measurable="true" verbs="define">Define key terminology</objective>
    <objective level="understand" measurable="true" verbs="explain">Explain core concepts</objective>
    <objective level="apply" measurable="true" verbs="apply">Apply knowledge to scenarios</objective>
    <objective level="analyze" measurable="true" verbs="analyze">Analyze relationships</objective>
  </learning_objectives>
  <active_learning>
    <discussion_prompt>In a group discussion, debate the statement: 'Traditional SDLC validation techniques are sufficient for AI/ML models due to their probabilistic nature.' Use NIST AI RMF 1.0 and real-world examples like adversarial attacks on image recognition models to support arguments, identifying gaps and proposing hybrid approaches.</discussion_prompt>
    <peer_teaching>Explain the key concepts to a partner without using technical jargon.</peer_teaching>
    <problem_solving>Given a scenario, apply the framework to solve the problem.</problem_solving>
  </active_learning>
  <scaffolding>
    <layer level="1" name="Foundation">
      <focus>Basic terminology and definitions</focus>
      <content/>
    </layer>
    <layer level="2" name="Components">
      <focus>Framework components and structure</focus>
      <content/>
    </layer>
    <layer level="3" name="Implementation">
      <focus>Practical implementation steps</focus>
      <content/>
    </layer>
    <layer level="4" name="Integration">
      <focus>Advanced integration and optimization</focus>
      <content/>
    </layer>
  </scaffolding>
  <flashcard_generation>
    <output_schema>
      <field name="question" type="string"/>
      <field name="correct_answer" type="string"/>
      <field name="distractors" type="[{'text': 'string', 'explanation': 'string'}]"/>
      <field name="explanation" type="string"/>
      <field name="bloom_level" type="enum"/>
      <field name="topic_hierarchy" type="object"/>
    </output_schema>
    <distractor_protocol>
      <step number="1">Identify common misconceptions about the topic</step>
      <step number="2">Create plausible but incorrect alternatives</step>
      <step number="3">Ensure distractors are similar in length and complexity</step>
      <step number="4">Avoid obviously wrong answers</step>
      <step number="5">Include partial truths that require deeper understanding</step>
    </distractor_protocol>
    <system_prompt>You are an expert flashcard generator specializing in cybersecurity education, focusing on AI/ML Model Validation (Topic Hierarchy: Cybersecurity &gt; Software Development Security &gt; SDLC &gt; 016_Emerging Technologies and Threats &gt; AI/ML Security &gt; AI/ML Model Validation). Use the provided learning objectives, active learning activities, 4-layer scaffolding, and flashcard schema to generate high-quality, pedagogically optimized flashcards.
Incorporate voter consensus: Full NIST SP 800-218A details (practices: AI-1 to AI-10 covering design, data, training, testing, deployment with tasks like adversarial eval, bias checks); prior SDLC links; techniques (cross-val, adversarial testing, bias auditing); real-world ex (adversarial attacks e.g., stop-sign stickers); sources (NIST AI RMF 1.0: https://doi.org/10.6028/NIST.AI.100-1; SP 800-218A).
Web context: AI/ML probabilistic nature expands attack surface; trustworthy AI (NIST: safe, secure, fair); frameworks for ML lifecycle security.
Generate flashcards strictly following the schema: 50-75 total, distributed by scaffolding layers (12-20 per layer), Bloom's balanced. Output as JSON array: [{'front': '...', 'back': {'answer': '...', 'explanation': '...', 'distractors': ['opt1', 'opt2', 'opt3'], 'bloom_level': '...', 'scaffolding_layer': 1, 'references': '...'}}, ...]. Ensure distractors are educationally valuable. Promote active learning by phrasing prompts to mimic discussion/problem-solving.</system_prompt>
  </flashcard_generation>
</topic_prompt>