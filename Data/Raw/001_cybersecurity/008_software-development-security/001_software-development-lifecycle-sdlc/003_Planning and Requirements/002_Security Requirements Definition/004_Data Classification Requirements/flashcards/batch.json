{
  "topic_title": "Data Classification Requirements",
  "category": "Software Development Security - Software Development Lifecycle (SDLC)",
  "flashcards": [
    {
      "question_text": "According to NIST guidance, what is the primary goal of data classification in software development security?",
      "correct_answer": "To identify and categorize data based on its sensitivity and criticality to determine appropriate security controls.",
      "distractors": [
        {
          "text": "To ensure all data is encrypted at rest and in transit.",
          "misconception": "Targets [over-application of controls]: Assumes a single control (encryption) is sufficient for all data types."
        },
        {
          "text": "To create a comprehensive inventory of all software assets.",
          "misconception": "Targets [scope confusion]: Confuses data classification with software asset management."
        },
        {
          "text": "To define user roles and permissions for accessing data.",
          "misconception": "Targets [implementation detail confusion]: Access control is an outcome of classification, not its primary goal."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data classification is foundational because it informs the selection of appropriate security controls. It works by categorizing data based on sensitivity, enabling tailored protection strategies.",
        "distractor_analysis": "The first distractor suggests a universal control, ignoring varying data needs. The second conflates data inventory with data sensitivity. The third focuses on access control, which is a consequence, not the primary objective of classification.",
        "analogy": "Data classification is like sorting mail: you put urgent bills in a special pile, junk mail in another, and personal letters in a third, so you know how to handle each type appropriately."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "DATA_CLASSIFICATION_BASICS",
        "SDLC_SECURITY_REQUIREMENTS"
      ]
    },
    {
      "question_text": "Which NIST publication provides guidance on mapping types of information and systems to security categories, relevant to data classification in software development?",
      "correct_answer": "NIST SP 800-60 Rev. 2, Guide for Mapping Types of Information and Information Systems to Security Categories.",
      "distractors": [
        {
          "text": "NIST SP 800-53 Rev. 5, Security and Privacy Controls.",
          "misconception": "Targets [related but distinct standard]: Confuses a control catalog with a mapping guide."
        },
        {
          "text": "NIST IR 8496 ipd, Data Classification Concepts and Considerations.",
          "misconception": "Targets [draft/retired document confusion]: Uses a document that is no longer actively developed."
        },
        {
          "text": "NIST SP 800-53B, Control Baselines.",
          "misconception": "Targets [control baseline confusion]: Focuses on baselines rather than the mapping process itself."
        }
      ],
      "detailed_explanation": {
        "core_logic": "NIST SP 800-60 Rev. 2 is specifically designed to guide organizations in mapping information types and systems to security categories, which is a core aspect of data classification for security purposes.",
        "distractor_analysis": "SP 800-53 Rev. 5 lists controls, not mapping guidance. IR 8496 ipd is a retired draft. SP 800-53B provides baselines, not the mapping methodology.",
        "analogy": "If data classification is about deciding what kind of security 'uniform' each piece of data needs, NIST SP 800-60 Rev. 2 is the guide that helps you determine which uniform goes with which data 'job'."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "NIST_STANDARDS",
        "DATA_CLASSIFICATION_MAPPING"
      ]
    },
    {
      "question_text": "In the context of software development security, what is the primary implication of classifying data as 'Confidential'?",
      "correct_answer": "The data requires the most stringent security controls to prevent unauthorized disclosure.",
      "distractors": [
        {
          "text": "The data is primarily for internal use and requires minimal protection.",
          "misconception": "Targets [misinterpretation of 'confidential']: Confuses 'confidential' with 'internal use only' which might be 'private' or 'sensitive'."
        },
        {
          "text": "The data must be publicly accessible to all users.",
          "misconception": "Targets [opposite of confidentiality]: Directly contradicts the meaning of confidential data."
        },
        {
          "text": "The data's integrity is paramount, but disclosure is less critical.",
          "misconception": "Targets [confidentiality vs. integrity confusion]: Prioritizes integrity over the core risk of disclosure for confidential data."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Classifying data as 'Confidential' signifies that unauthorized disclosure could cause severe damage to an organization. Therefore, it necessitates the highest level of security controls, including strong encryption and strict access limitations.",
        "distractor_analysis": "The first distractor misinterprets 'confidential' as low protection. The second is the direct opposite. The third incorrectly prioritizes integrity over the primary risk of disclosure.",
        "analogy": "Classifying data as 'Confidential' is like handling state secrets; extreme measures are taken to ensure no one unauthorized sees them."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION_LEVELS",
        "SECURITY_CONTROLS"
      ]
    },
    {
      "question_text": "When defining security requirements for software, how does data classification influence the selection of encryption algorithms?",
      "correct_answer": "Higher classification levels (e.g., 'Confidential') mandate stronger, more robust encryption algorithms (e.g., AES-256) compared to lower levels.",
      "distractors": [
        {
          "text": "Data classification dictates the use of symmetric encryption for all sensitive data.",
          "misconception": "Targets [algorithm type confusion]: Assumes classification dictates a specific type (symmetric) rather than strength."
        },
        {
          "text": "Encryption algorithms are chosen based on data volume, not its classification level.",
          "misconception": "Targets [misplaced priority]: Ignores sensitivity as the primary driver for encryption strength."
        },
        {
          "text": "Data classification only requires encryption for 'Public' data to ensure accessibility.",
          "misconception": "Targets [opposite of intended use]: Incorrectly applies encryption to public data for accessibility, ignoring confidential data needs."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data classification directly informs the required strength of security controls, including encryption. More sensitive data (higher classification) requires more robust algorithms like AES-256 to protect against sophisticated attacks, whereas less sensitive data might use less computationally intensive methods.",
        "distractor_analysis": "The first distractor wrongly specifies symmetric encryption. The second incorrectly prioritizes data volume over sensitivity. The third reverses the purpose of encryption for public data.",
        "analogy": "If data classification is like assigning security clearances, then encryption algorithms are the specific locks used for each clearance level â€“ higher clearances need stronger, more complex locks."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "ENCRYPTION_ALGORITHMS",
        "SDLC_SECURITY_REQUIREMENTS"
      ]
    },
    {
      "question_text": "What is the role of data labeling in supporting data classification within a software development project?",
      "correct_answer": "Labels provide a mechanism to tag data with its classification level, enabling automated policy enforcement and auditing.",
      "distractors": [
        {
          "text": "Labels are used solely for organizing files in a file system.",
          "misconception": "Targets [limited scope of labeling]: Confuses data labeling with basic file organization."
        },
        {
          "text": "Labels are a substitute for defining security requirements.",
          "misconception": "Targets [misunderstanding of purpose]: Views labels as a replacement for the classification process itself."
        },
        {
          "text": "Labels are only applied to data after it has been compromised.",
          "misconception": "Targets [reactive vs. proactive approach]: Incorrectly positions labeling as a post-breach activity."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data labeling is crucial because it operationalizes data classification. By applying labels (e.g., 'Confidential', 'Public'), systems can automatically enforce policies, track data movement, and facilitate compliance, thereby supporting the security requirements defined by classification.",
        "distractor_analysis": "The first distractor limits labeling to file system use. The second suggests labels replace the classification process. The third incorrectly frames labeling as a reactive measure.",
        "analogy": "Data labeling is like putting a 'Fragile' or 'This Side Up' sticker on a package; it clearly communicates handling instructions derived from its classification."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "DATA_LABELING",
        "POLICY_ENFORCEMENT"
      ]
    },
    {
      "question_text": "Consider a scenario where a software application handles both user PII (Personally Identifiable Information) and public product information. How should data classification guide the development of access controls?",
      "correct_answer": "Access controls must be more restrictive for PII, allowing only authorized personnel, while public product information can have broader access.",
      "distractors": [
        {
          "text": "Access controls should be identical for both PII and public product information to ensure consistency.",
          "misconception": "Targets [lack of risk-based access]: Fails to differentiate access based on data sensitivity and risk."
        },
        {
          "text": "PII should be made public to ensure transparency, while product information is restricted.",
          "misconception": "Targets [misunderstanding of PII sensitivity]: Reverses the typical security posture for PII."
        },
        {
          "text": "Access controls are determined by the application's user interface, not data classification.",
          "misconception": "Targets [ignoring data sensitivity]: Focuses on UI design over the inherent risk of the data being handled."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data classification dictates the sensitivity of information. PII is typically classified as highly sensitive, requiring strict access controls (e.g., Role-Based Access Control - RBAC) to prevent unauthorized disclosure, whereas public data requires minimal restrictions, aligning with the principle of least privilege.",
        "distractor_analysis": "The first distractor ignores risk-based access. The second incorrectly treats PII as public. The third dismisses data classification as a driver for access control.",
        "analogy": "When handling mail, you'd put a sensitive legal document (PII) in a locked box, but a public flyer (product info) can be left on a table."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "ACCESS_CONTROL_PRINCIPLES",
        "PII_PROTECTION"
      ]
    },
    {
      "question_text": "What is the purpose of a 'Data Handling Ruleset' in conjunction with data classification?",
      "correct_answer": "To specify the approved methods for storing, processing, transmitting, and disposing of data based on its classification level.",
      "distractors": [
        {
          "text": "To define the technical architecture of the data storage system.",
          "misconception": "Targets [scope confusion]: Confuses operational handling rules with system architecture design."
        },
        {
          "text": "To outline the legal requirements for data privacy regulations.",
          "misconception": "Targets [confusing ruleset with compliance]: While related, rulesets are operational, not solely legal mandates."
        },
        {
          "text": "To create a user interface for data access.",
          "misconception": "Targets [confusing ruleset with UI design]: Rulesets govern data handling, not the user interface itself."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data handling rulesets operationalize data classification by providing specific, actionable guidance on how data of a certain classification must be managed throughout its lifecycle. This ensures that the security controls mandated by the classification are consistently applied.",
        "distractor_analysis": "The first distractor confuses operational rules with system architecture. The second conflates specific handling rules with broader legal compliance. The third incorrectly links rulesets to UI design.",
        "analogy": "If data classification is deciding a package is 'Fragile', the data handling ruleset is the instruction manual telling you 'Do not drop', 'Keep upright', 'Store in cool place'."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "DATA_HANDLING_POLICIES"
      ]
    },
    {
      "question_text": "How does data classification contribute to compliance with regulations like GDPR or PCI-DSS during software development?",
      "correct_answer": "By identifying sensitive data (e.g., PII, cardholder data), classification ensures that appropriate security controls are implemented to meet regulatory requirements.",
      "distractors": [
        {
          "text": "Data classification is a regulatory requirement itself, mandating specific controls.",
          "misconception": "Targets [misunderstanding of regulatory relationship]: Classification is a means to compliance, not always a direct mandate itself."
        },
        {
          "text": "Classification ensures all data is anonymized, satisfying privacy regulations.",
          "misconception": "Targets [over-simplification of compliance]: Anonymization is one technique, not a universal outcome of classification for all regulations."
        },
        {
          "text": "Classification is only relevant for data stored outside the organization's network.",
          "misconception": "Targets [limited scope of data protection]: Ignores the need to protect data regardless of location."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Regulations like GDPR and PCI-DSS mandate specific protections for certain types of data (e.g., PII, financial data). Data classification identifies this sensitive data, allowing developers to implement the necessary controls (like encryption, access restrictions) required for compliance.",
        "distractor_analysis": "The first distractor overstates classification as a direct regulatory mandate. The second oversimplifies compliance by suggesting universal anonymization. The third incorrectly limits the scope of data protection.",
        "analogy": "Data classification is like identifying which items in your house are valuable art pieces; this knowledge helps you decide where to put your security cameras and alarms to comply with insurance policies (regulations)."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "REGULATORY_COMPLIANCE",
        "GDPR",
        "PCI_DSS"
      ]
    },
    {
      "question_text": "Which of the following best describes the 'Zero Trust' principle's relationship with data classification in modern software security?",
      "correct_answer": "Zero Trust assumes no implicit trust, requiring data classification to inform granular access controls and security policies for every data access request.",
      "distractors": [
        {
          "text": "Zero Trust eliminates the need for data classification by encrypting everything.",
          "misconception": "Targets [misunderstanding of Zero Trust scope]: Assumes encryption alone fulfills Zero Trust, ignoring data context."
        },
        {
          "text": "Data classification is only relevant for legacy systems, not Zero Trust architectures.",
          "misconception": "Targets [outdated concept misconception]: Believes classification is obsolete in modern security models."
        },
        {
          "text": "Zero Trust focuses solely on network segmentation, making data classification redundant.",
          "misconception": "Targets [narrow view of Zero Trust]: Overlooks the data-centric aspect of Zero Trust."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Zero Trust operates on the principle of 'never trust, always verify.' Data classification is essential because it provides the context needed to make informed, granular access decisions for each data interaction, ensuring that even within a trusted network, access is granted based on verified identity and data sensitivity.",
        "distractor_analysis": "The first distractor wrongly suggests encryption replaces classification in Zero Trust. The second incorrectly dismisses classification as outdated. The third narrows Zero Trust to only network segmentation.",
        "analogy": "In a Zero Trust model, data classification is like having a security guard at every door (not just the main entrance) who checks your ID and the reason for your visit (data sensitivity) before letting you pass."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "ZERO_TRUST_ARCHITECTURE",
        "DATA_CLASSIFICATION",
        "ACCESS_CONTROL"
      ]
    },
    {
      "question_text": "During the requirements phase of SDLC, what is a common pitfall related to data classification?",
      "correct_answer": "Treating data classification as a one-time activity rather than an ongoing process that needs review and updates.",
      "distractors": [
        {
          "text": "Over-classifying all data as 'Confidential' to be safe.",
          "misconception": "Targets [over-simplification/lazy approach]: Leads to unnecessary costs and complexity."
        },
        {
          "text": "Focusing solely on classifying data at rest, ignoring data in transit or in use.",
          "misconception": "Targets [incomplete lifecycle view]: Neglects data states where it is also vulnerable."
        },
        {
          "text": "Assuming that third-party data handling processes do not require classification.",
          "misconception": "Targets [supply chain risk ignorance]: Fails to account for data shared externally."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data classification is not static; business needs, threats, and regulations evolve. Treating it as a one-time task during requirements means classifications can become outdated, leading to inadequate security controls. Continuous review ensures ongoing alignment with current risks and policies.",
        "distractor_analysis": "The first distractor describes a common but inefficient approach. The second highlights a scope limitation in classification. The third points to a failure in considering external data handling.",
        "analogy": "Thinking data classification is a one-time task is like setting your alarm system once and never checking if the batteries are dead or if new security vulnerabilities have emerged."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION_PROCESS",
        "SDLC_REQUIREMENTS"
      ]
    },
    {
      "question_text": "How can data classification requirements directly impact the choice of data storage solutions in a software project?",
      "correct_answer": "Highly classified data may require specialized, secure storage solutions with robust encryption and access controls, whereas less sensitive data can use more standard options.",
      "distractors": [
        {
          "text": "Data classification has no impact on storage solutions; cost is the only factor.",
          "misconception": "Targets [ignoring security implications]: Dismisses security needs driven by data sensitivity."
        },
        {
          "text": "All data must be stored in cloud-based solutions for scalability, regardless of classification.",
          "misconception": "Targets [technology bias]: Assumes a specific technology (cloud) is universally applicable."
        },
        {
          "text": "Classification only dictates whether data is stored locally or remotely.",
          "misconception": "Targets [limited scope of impact]: Reduces the impact to a simple location choice."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The sensitivity and regulatory requirements associated with data classification directly influence the choice of storage. High-classification data (e.g., PII, financial data) necessitates storage solutions offering strong encryption, audit trails, and granular access controls, often leading to more specialized or hardened environments.",
        "distractor_analysis": "The first distractor wrongly dismisses security as a factor. The second promotes a single technology solution without considering data needs. The third oversimplifies the impact to just location.",
        "analogy": "If data classification is deciding if a document is a personal diary or a public notice, the storage solution is like choosing between a locked safe for the diary and an open bulletin board for the notice."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "DATA_STORAGE_SECURITY",
        "SDLC_PLANNING"
      ]
    },
    {
      "question_text": "What is the primary benefit of integrating data classification early in the software development lifecycle (SDLC)?",
      "correct_answer": "It enables proactive security design, ensuring that security controls are built into the software from the outset, rather than being added later.",
      "distractors": [
        {
          "text": "It simplifies the testing phase by reducing the number of security test cases.",
          "misconception": "Targets [misunderstanding of security integration]: Security integration aims for better coverage, not reduction."
        },
        {
          "text": "It guarantees that the software will be compliant with all regulations.",
          "misconception": "Targets [overstated outcome]: Classification is a key step, but not a guarantee of full compliance alone."
        },
        {
          "text": "It allows developers to ignore security concerns until the deployment phase.",
          "misconception": "Targets [opposite of proactive security]: Directly contradicts the goal of early integration."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Integrating data classification early in the SDLC (during requirements and design) allows security to be a foundational element. This 'security by design' approach is more effective and cost-efficient than retrofitting security controls, ensuring that data protection mechanisms are inherent to the software's architecture.",
        "distractor_analysis": "The first distractor suggests a false benefit of reduced testing. The second overpromises full compliance. The third advocates for a reactive, insecure approach.",
        "analogy": "Building a house with plumbing and electrical wiring integrated from the foundation is far more effective than trying to add them after the walls are up; data classification early in SDLC is like that integrated planning."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "SDLC_SECURITY",
        "SECURITY_BY_DESIGN"
      ]
    },
    {
      "question_text": "Consider a software project developing a financial transaction application. Which data classification level would most likely apply to transaction details and user account numbers?",
      "correct_answer": "Highly Sensitive / Confidential",
      "distractors": [
        {
          "text": "Public",
          "misconception": "Targets [misunderstanding of financial data sensitivity]: Incorrectly assumes financial data is not sensitive."
        },
        {
          "text": "Internal Use Only",
          "misconception": "Targets [underestimation of risk]: Underestimates the severe impact of unauthorized disclosure or modification."
        },
        {
          "text": "Restricted",
          "misconception": "Targets [granularity confusion]: While potentially applicable, 'Highly Sensitive/Confidential' is more precise for financial PII."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Financial transaction details and account numbers constitute Personally Identifiable Information (PII) and sensitive financial data. Unauthorized disclosure or modification could lead to significant financial loss, identity theft, and severe regulatory penalties, thus warranting the highest classification level ('Highly Sensitive' or 'Confidential').",
        "distractor_analysis": "Classifying financial PII as 'Public' or 'Internal Use Only' is a critical security failure. 'Restricted' might apply to some internal data, but financial PII typically demands the highest level of protection.",
        "analogy": "Handling financial transaction data is like handling bearer bonds; extreme security measures are required because their compromise has severe consequences."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "DATA_CLASSIFICATION_LEVELS",
        "FINANCIAL_DATA_SECURITY",
        "PII_PROTECTION"
      ]
    },
    {
      "question_text": "What is the role of data classification in defining security requirements for data validation within a software application?",
      "correct_answer": "It dictates the rigor of validation checks; highly classified data requires more stringent validation to ensure integrity and prevent manipulation.",
      "distractors": [
        {
          "text": "Data classification determines the programming language used for validation.",
          "misconception": "Targets [irrelevant factor]: Classification does not dictate programming language choice."
        },
        {
          "text": "Validation is only necessary for data classified as 'Public'.",
          "misconception": "Targets [opposite of intended use]: Validation is critical for sensitive data to maintain integrity."
        },
        {
          "text": "Data classification is irrelevant to data validation processes.",
          "misconception": "Targets [misunderstanding of data integrity]: Ignores the link between data sensitivity and the need for integrity."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data classification informs the level of trust and protection required for data. For highly classified data, ensuring its integrity through robust validation is paramount because manipulation could lead to severe consequences. Therefore, classification drives the necessity for more rigorous input validation and integrity checks.",
        "distractor_analysis": "The first distractor links classification to an unrelated technical choice. The second incorrectly limits validation to public data. The third dismisses the importance of classification for data integrity.",
        "analogy": "If data classification is deciding if a document is a draft or a final signed contract, data validation is like checking the signature and seal meticulously for the contract, but only a quick glance for the draft."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_CLASSIFICATION",
        "DATA_VALIDATION",
        "DATA_INTEGRITY"
      ]
    },
    {
      "question_text": "How does the NIST Cybersecurity Framework (CSF) relate to data classification requirements in software development?",
      "correct_answer": "The CSF's 'Protect' function includes categories like Access Control (PR.AC) and Data Security (PR.DS), which are directly informed by data classification to implement appropriate safeguards.",
      "distractors": [
        {
          "text": "The CSF mandates specific data classification levels for all software.",
          "misconception": "Targets [misunderstanding of framework scope]: CSF provides a high-level framework, not specific classification levels."
        },
        {
          "text": "Data classification is only relevant to the CSF's 'Identify' function.",
          "misconception": "Targets [limited scope of CSF functions]: Ignores how classification supports protection and detection."
        },
        {
          "text": "The CSF replaces the need for detailed data classification by providing generic security controls.",
          "misconception": "Targets [over-simplification of CSF]: CSF requires tailoring, which data classification enables."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST CSF provides a strategic approach to cybersecurity risk management. Data classification is a critical input for the 'Protect' function, specifically guiding the implementation of controls within categories like Access Control and Data Security (PR.AC, PR.DS) to ensure data is safeguarded according to its sensitivity.",
        "distractor_analysis": "The first distractor misrepresents CSF as prescriptive on classification levels. The second limits classification's role to only the 'Identify' function. The third incorrectly suggests CSF makes classification redundant.",
        "analogy": "The NIST CSF is like a general building code; data classification is like specifying the type of fire-resistant materials needed for different parts of the building (e.g., server room vs. lobby) to meet those codes."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "NIST_CSF",
        "DATA_CLASSIFICATION",
        "CYBERSECURITY_FRAMEWORKS"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 15,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "Data Classification Requirements Software Development Security best practices",
    "latency_ms": 27249.779
  },
  "timestamp": "2026-01-18T10:22:04.399890"
}