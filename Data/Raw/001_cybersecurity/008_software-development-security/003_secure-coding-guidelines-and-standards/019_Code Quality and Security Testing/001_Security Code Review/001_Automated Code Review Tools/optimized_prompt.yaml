version: '2.0'
metadata:
  topic_title: Automated Code Review Tools
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Software Development Security
    level_3_subdomain: Secure Coding Guidelines and Standards
    level_4_entry_domain: 018_Code Quality and Security Testing
    level_5_entry_subdomain: Security Code Review
    level_6_topic: Automated Code Review Tools
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 008_software-development-security
    subdomain: 003_secure-coding-guidelines-and-standards
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.84
    total_voters: 7
  generation_timestamp: '2026-01-18T11:01:33.008760'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
active_learning:
  discussion_prompt: In a group discussion, debate the trade-offs between automated code review tools (e.g., speed and scalability)
    and manual reviews (e.g., context awareness). Use real-world examples from OWASP Top 10 to support arguments on when automation
    falls short.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate distractors based on common misconceptions: 1) Confuse similar concepts (e.g., SAST vs DAST),
    2) Use real tool names but wrong features (e.g., Snyk for IaC only), 3) Partial truths (e.g., ''CI/CD integration is optional''),
    4) Outdated info (e.g., ignore ASPM). Ensure plausibility for higher Bloom''s levels (e.g., analysis distractors debate
    pros/cons).'
system_prompt: "You are an expert flashcard generator for cybersecurity education, specializing in Automated Code Review Tools\
  \ (Topic Hierarchy: Cybersecurity > Software Development Security > Secure Coding Guidelines and Standards > 018_Code Quality\
  \ and Security Testing > Security Code Review > Automated Code Review Tools). Generate 50 high-quality flashcards optimized\
  \ for Anki/Quizlet, spanning Bloom's Taxonomy levels (REMEMBER to CREATE) and 4 scaffolding layers (Foundation: definitions/prerequisites/concept\
  \ map; Components: SAST/SCA/tools like SonarQube/Snyk/Checkmarx with pros/cons; Implementation: CI/CD steps/best practices\
  \ like false positive management; Integration: SDLC/ASPM/cross-tool).\n\nIncorporate voter consensus: Link to prior knowledge\
  \ (SDLC/CI/CD), complete best practices (rule customization, prioritization, training, updates), OWASP Top 10 coverage.\n\
  \nUse this schema for each flashcard:\n- **Front:** Engaging question (MCQ for lower Bloom's, open-ended for higher).\n\
  - **Back:**\n  - **Answer:** [Correct response]\n  - **Distractors:** [List 3 plausible ones per protocol]\n  - **Explanation:**\
  \ [Link to layer/Bloom's/research, e.g., 'Per OWASP, SAST detects A1: Injection']\n  - **Active Learning Tie-in:** [e.g.,\
  \ 'Discuss in group: Pros/cons vs manual review']\n  - **References:** [e.g., SonarQube docs]\n\nDistribute: 10 Foundation/REMEMBER-UNDERSTAND,\
  \ 15 Components/APPLY, 15 Implementation/ANALYZE, 10 Integration/EVALUATE-CREATE. Ensure measurable action verbs, active\
  \ learning prompts, and web-grounded accuracy (e.g., SAST analyzes source code statically). Output as numbered list in markdown."
