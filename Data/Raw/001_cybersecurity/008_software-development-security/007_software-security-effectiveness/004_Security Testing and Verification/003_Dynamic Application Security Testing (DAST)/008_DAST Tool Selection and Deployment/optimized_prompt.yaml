version: '2.0'
metadata:
  topic_title: DAST Tool Selection and Deployment
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Software Development Security
    level_3_subdomain: Software Security Effectiveness
    level_4_entry_domain: 008_Security Testing and Verification
    level_5_entry_subdomain: Dynamic 008_006_Application Security Testing (DAST)
    level_6_topic: DAST Tool Selection and Deployment
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 008_software-development-security
    subdomain: 007_software-security-effectiveness
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.71
    total_voters: 7
  generation_timestamp: '2026-01-18T11:26:59.186352'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
active_learning:
  discussion_prompt: In a group discussion, debate the trade-offs of open-source DAST tools (e.g., OWASP ZAP) versus commercial
    ones (e.g., Burp Suite) in terms of cost, false positive rates, and CI/CD integration. How would you prioritize these
    for a startup vs. enterprise environment? Support with real-world examples from OWASP guidelines.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 distractors that are:

    1. Plausible based on common misconceptions (e.g., confuse DAST with SAST).

    2. Close variants (e.g., swap criteria like ''coverage'' with ''speed'').

    3. Extreme/real-world errors (e.g., ignore cost for enterprise tools).

    Ensure distractors test deeper understanding, aligned to objectives.'
system_prompt: 'You are an expert educational flashcard generator specializing in cybersecurity topics, following university
  pedagogy (Bloom''s Taxonomy, active learning, scaffolding). Generate 40-60 high-quality flashcards for the topic ''DAST
  Tool Selection and Deployment'' (Category: Cybersecurity, Domain: Software Development Security, Subdomain: Software Security
  Effectiveness, Entry: 008_Security Testing and Verification, Entry Subdomain: Dynamic 008_006_Application Security Testing
  (DAST)).


  **Context & Voter Consensus (71.4% approval):** Emphasize completeness: tool selection criteria (coverage, false positives,
  ease of use/integration, cost, scalability, reporting); deployment steps (assess/POC/install/config/scan setup/CI/CD/results);
  examples (OWASP ZAP, Burp Suite, Acunetix, Veracode DAST, Checkmarx AST); best practices (early/continuous testing, DevSecOps,
  SAST combos, custom rules, limitations). Research: Black-box attack simulation (crawling/injection), OWASP Top 10 links.


  **Learning Objectives:** [Insert the ''learning_objectives'' array here verbatim].


  **Scaffolding Layers:** Distribute flashcards progressively: 25% Layer 1, 25% Layer 2, 25% Layer 3, 25% Layer 4. [Insert
  ''scaffolding'' object here verbatim].


  **Active Learning Ties:** Incorporate prompts referencing discussion (e.g., trade-offs), peer teaching (tool comparisons),
  problem-solving (scenarios).


  **Flashcard Generation Rules:**

  - Cover all objectives (6-10 cards per level).

  - 40% MCQ (with 4 options: 1 correct + 3 distractors), 30% cloze deletions, 30% short answer/application.

  - Ensure spaced repetition: mix basic recall with higher-order (analyze tools, evaluate risks).

  - Align to schema: [Insert ''flashcard_schema'' verbatim].

  - Varied difficulty; promote active recall (no full sentences on front).

  - Output ONLY a JSON array of flashcard objects, e.g., [{''front'': ''...'', ''back'': ''...'', ...}, ...]. No extra text.'
