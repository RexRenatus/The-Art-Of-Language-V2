version: '2.0'
metadata:
  topic_title: Code Review Metrics Tracking
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Software Development Security
    level_3_subdomain: Software Security Effectiveness
    level_4_entry_domain: Secure 005_Implementation and Development
    level_5_entry_subdomain: Code Review and Analysis
    level_6_topic: Code Review Metrics Tracking
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 008_software-development-security
    subdomain: 007_software-security-effectiveness
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.83
    total_voters: 7
  generation_timestamp: '2026-01-18T11:24:28.021813'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
active_learning:
  discussion_prompt: 'Debate: Is defect detection rate more critical than code coverage for security in code reviews? Use
    OWASP metrics and NIST SP 800-55 examples to argue your position, considering shift-left security in SDLC.'
  peer_teaching: In pairs, one student explains absolute vs. relative metrics with examples (e.g., LOC vs. Defect Density),
    then switches roles and quizzes the partner on formulas and applications.
  problem_solving: Given sample data (500 LOC reviewed in 2 hours, 3 vulnerabilities found, 80% code coverage), compute metrics
    (Defect Density, Inspection Rate, Defect Detection Rate), analyze trends, and suggest process improvements like tool integration
    (e.g., SonarQube).
  additional_activities: []
scaffolding:
- level: 1
  name: Layer 1
  focus: ''
  content: 'Basic terms: Code Review (systematic code examination for defects/vulnerabilities); Metrics (quantifiable measures);
    Absolute (e.g., LOC, total defects); Relative (e.g., Defect Density = Defects / KLOC; Inspection Rate = LOC/hour; Defect
    Detection Rate = Defects found / Total defects; Code Coverage %). Formulas and examples provided.'
- level: 2
  name: Layer 2
  focus: ''
  content: 'Metrics types (absolute/relative); Relationships (e.g., high Defect Density links to poor review effectiveness;
    Code Coverage correlates with Defect Detection). Frameworks: OWASP Code Review Guide (emphasizes secure coding metrics);
    NIST SP 800-55 Vol. 2 (Performance Measurement Guide for Information Security, defines measurement methods for security
    controls including defect tracking).'
- level: 3
  name: Layer 3
  focus: ''
  content: 'Step-by-step tracking: 1) Collect data (LOC via tools); 2) Review code (track time/defects); 3) Compute metrics;
    4) Tools: GitHub PR metrics (comments/changes), SonarQube (coverage/defects). Procedures with examples.'
- level: 4
  name: Layer 4
  focus: ''
  content: Connect to SDLC (shift-left in implementation); OWASP ASVS/NIST optimization; Dashboards (e.g., Grafana for trends);
    Compliance reporting.
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 plausible distractors per MCQ: 1) Common misconception (e.g., Defect Density = Defects
    * LOC); 2) Confused term (e.g., mix Inspection Rate with Coverage); 3) Extreme value (e.g., unrealistic rate); Ensure
    distractors are realistic based on voter-noted pitfalls like over-relying on LOC.'
system_prompt: 'You are an expert flashcard generator for cybersecurity education, specializing in Bloom''s Taxonomy and active
  learning. Topic: Code Review Metrics Tracking (Hierarchy: Cybersecurity > Software Development Security > Software Security
  Effectiveness > Secure 005_Implementation and Development > Code Review and Analysis > Code Review Metrics Tracking). Voter
  consensus: 82.9% approval, priorities on completeness (NIST SP 800-55 Vol. 2: Performance Measurement Guide for Information
  Security - defines metrics for security effectiveness like defect tracking; OWASP Code Review Guide/ASVS: secure coding
  metrics), pedagogy, scaffolding.


  Sources (full list with summaries/links):

  - NIST SP 800-55 Vol. 2: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-55v2.pdf (Metrics for security
  program performance, e.g., defect removal efficiency).

  - OWASP Code Review Guide: https://owasp.org/www-project-code-review-guide/ (Metrics like review coverage, defect rates).

  - SonarQube Docs: https://docs.sonarsource.com/ (Code coverage, security hotspots).

  - GitHub PR Metrics: https://docs.github.com/en/pull-requests (Review time, comments).


  Use provided Learning Objectives, Active Learning activities, Scaffolding (prereqs, big picture, 4 layers), and Flashcard
  Schema exactly. Cover key metrics: Defect Density (Defects/KLOC), LOC, Inspection Rate (LOC/hour), Defect Detection Rate
  (Found/Total Defects), Code Coverage (% tested). Address misconceptions (e.g., LOC â‰  quality).


  Output ONLY a JSON array of flashcards, each as: {''type'': ''...'', ''front'': ''...'', ''back'': ''...'', ''bloom_level'':
  ''...'', ''layer'': ''1-4'', ''explanation'': ''...''}.

  Generate 45 flashcards balanced per schema. Ensure active recall, no hints on front.'
