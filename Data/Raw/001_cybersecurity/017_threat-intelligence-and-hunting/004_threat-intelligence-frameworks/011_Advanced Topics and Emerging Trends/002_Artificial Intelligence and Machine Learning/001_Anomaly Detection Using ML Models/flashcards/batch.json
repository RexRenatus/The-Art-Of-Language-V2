{
  "topic_title": "Anomaly Detection Using ML Models",
  "category": "Threat Intelligence And Hunting - Threat Intelligence Frameworks",
  "flashcards": [
    {
      "question_text": "According to NIST AI 100-2 E2025, which of the following is a primary goal of Adversarial Machine Learning (AML) in the context of AI systems?",
      "correct_answer": "To study attacks that exploit the statistical, data-based nature of ML systems.",
      "distractors": [
        {
          "text": "To develop AI systems that are immune to all forms of cyberattacks.",
          "misconception": "Targets [overgeneralization]: Assumes absolute immunity is achievable, which is not realistic in cybersecurity."
        },
        {
          "text": "To ensure AI systems only process data from trusted sources.",
          "misconception": "Targets [scope limitation]: Focuses only on data source control, ignoring other attack vectors like model manipulation."
        },
        {
          "text": "To automate the entire threat hunting process without human oversight.",
          "misconception": "Targets [automation fallacy]: Overestimates the current capabilities of AI in fully replacing human threat hunting expertise."
        }
      ],
      "detailed_explanation": {
        "core_logic": "AML specifically studies attacks that exploit the inherent statistical and data-driven nature of ML, aiming to understand vulnerabilities and develop defenses, rather than guaranteeing absolute immunity.",
        "distractor_analysis": "The first distractor overgeneralizes AML's goal to absolute immunity. The second limits AML's scope too narrowly to data sources. The third incorrectly assumes full automation of threat hunting.",
        "analogy": "AML is like studying how a lock can be picked (exploiting its mechanism) rather than trying to build an unpickable lock."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 categorizes attacks on Predictive AI (PredAI) systems. Which category primarily focuses on disrupting the availability of an AI system's services?",
      "correct_answer": "Availability breakdown",
      "distractors": [
        {
          "text": "Integrity violation",
          "misconception": "Targets [objective confusion]: Confuses availability attacks with attacks that alter system performance or output."
        },
        {
          "text": "Privacy compromise",
          "misconception": "Targets [objective confusion]: Confuses availability attacks with attacks aimed at extracting sensitive information."
        },
        {
          "text": "Model extraction",
          "misconception": "Targets [attack type misclassification]: Classifies a specific privacy attack as a general availability attack."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Availability breakdown attacks aim to disrupt access to an AI system's services, unlike integrity attacks (which alter performance) or privacy attacks (which steal data).",
        "distractor_analysis": "Each distractor represents a different category of AML attack, testing the understanding of distinct attacker objectives.",
        "analogy": "An availability breakdown is like a power outage for a service, making it inaccessible, whereas an integrity violation is like the service giving wrong information, and a privacy compromise is like someone stealing customer data."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_PREDAI_ATTACKS"
      ]
    },
    {
      "question_text": "In the context of Adversarial Machine Learning (AML), what is the primary characteristic of an 'evasion attack' on a PredAI system?",
      "correct_answer": "Modifying testing samples to alter model predictions while remaining stealthy.",
      "distractors": [
        {
          "text": "Inserting malicious data into the training set to corrupt the model.",
          "misconception": "Targets [attack stage confusion]: Confuses deployment-time evasion attacks with training-time poisoning attacks."
        },
        {
          "text": "Extracting sensitive information about the training data from the model.",
          "misconception": "Targets [attack objective confusion]: Confuses evasion attacks with privacy attacks like membership inference."
        },
        {
          "text": "Directly manipulating the model's parameters during training.",
          "misconception": "Targets [attack mechanism confusion]: Confuses evasion attacks with model poisoning attacks that alter parameters."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks occur during the deployment stage by subtly altering input data (adversarial examples) to trick a trained model into misclassifying, unlike poisoning attacks which target the training data.",
        "distractor_analysis": "The first distractor describes poisoning, the second describes privacy attacks, and the third describes model poisoning, all distinct from evasion.",
        "analogy": "An evasion attack is like subtly changing a stop sign's appearance so a self-driving car misinterprets it, while poisoning is like secretly altering the car's training manual to teach it wrong rules."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_PREDAI_ATTACKS",
        "AML_EVASION_ATTACKS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2 E2025, what is the defining characteristic of a 'poisoning attack' in machine learning?",
      "correct_answer": "Interfering with the model during its training stage, often by manipulating training data.",
      "distractors": [
        {
          "text": "Altering the model's predictions on new, unseen data after training.",
          "misconception": "Targets [attack stage confusion]: Describes evasion attacks, which occur post-training, not poisoning attacks during training."
        },
        {
          "text": "Extracting sensitive information about the model's architecture or weights.",
          "misconception": "Targets [attack objective confusion]: Describes model extraction or privacy attacks, not poisoning."
        },
        {
          "text": "Exploiting vulnerabilities in the model's API during inference.",
          "misconception": "Targets [attack vector confusion]: Describes prompt injection or API abuse, not training-stage poisoning."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Poisoning attacks specifically target the training stage of ML by corrupting the data or model parameters, fundamentally altering the model's learned behavior, unlike deployment-time attacks.",
        "distractor_analysis": "Each distractor describes a different type of attack: evasion (deployment-time), model extraction (privacy/deployment-time), and API exploitation (inference-time).",
        "analogy": "Poisoning a model is like secretly adding bad ingredients to a recipe while it's being cooked, ensuring the final dish is flawed, rather than trying to tamper with the finished dish."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_POISONING_ATTACKS"
      ]
    },
    {
      "question_text": "Which type of AML attack, as described in NIST AI 100-2 E2025, aims to infer whether a specific data sample was part of the training dataset?",
      "correct_answer": "Membership inference attack",
      "distractors": [
        {
          "text": "Data reconstruction attack",
          "misconception": "Targets [attack objective confusion]: Membership inference confirms presence; reconstruction attempts to recover the data itself."
        },
        {
          "text": "Property inference attack",
          "misconception": "Targets [attack scope confusion]: Property inference targets aggregate dataset characteristics, not individual sample membership."
        },
        {
          "text": "Model extraction attack",
          "misconception": "Targets [attack target confusion]: Model extraction targets the model's structure/weights, not training data membership."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Membership inference attacks specifically aim to determine if a given data point was used during model training, differentiating it from data reconstruction or property inference.",
        "distractor_analysis": "The distractors represent other privacy attacks: data reconstruction (recovering data), property inference (dataset characteristics), and model extraction (model details).",
        "analogy": "Membership inference is like asking if a specific student attended a particular class (yes/no), while data reconstruction is like trying to guess what notes that student took, and property inference is like asking about the average attendance of the class."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_PRIVACY_ATTACKS"
      ]
    },
    {
      "question_text": "In NIST AI 100-2 E2025, 'Query Access' is listed as an attacker capability. What does this capability typically involve in the context of AML attacks?",
      "correct_answer": "Submitting crafted queries to a deployed model and observing its predictions or generations.",
      "distractors": [
        {
          "text": "Directly modifying the model's parameters in its training environment.",
          "misconception": "Targets [capability confusion]: This describes 'Model Control', not 'Query Access'."
        },
        {
          "text": "Injecting malicious code into the model's source code repository.",
          "misconception": "Targets [capability confusion]: This relates to 'Source Code Control', not 'Query Access'."
        },
        {
          "text": "Controlling a subset of the data used during the model's training phase.",
          "misconception": "Targets [capability confusion]: This describes 'Training Data Control', not 'Query Access'."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Query access allows attackers to interact with a deployed model via its interface (like an API) to probe its behavior, which is crucial for black-box attacks and privacy inference, unlike direct control over training or code.",
        "distractor_analysis": "Each distractor describes a different attacker capability: Model Control, Source Code Control, and Training Data Control, contrasting with Query Access.",
        "analogy": "Query access is like being able to ask a black box a question and get an answer, without knowing how the box works internally. Direct modification is like having the blueprints and tools to change the box's internal workings."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_ATTACKER_CAPABILITIES"
      ]
    },
    {
      "question_text": "Consider a scenario where an attacker modifies a few training samples with incorrect labels to degrade the overall performance of a malware classification model. Which type of poisoning attack does this describe, according to NIST AI 100-2 E2025?",
      "correct_answer": "Availability poisoning",
      "distractors": [
        {
          "text": "Targeted poisoning",
          "misconception": "Targets [attack objective confusion]: Targeted poisoning aims to affect specific samples, not degrade overall performance indiscriminately."
        },
        {
          "text": "Backdoor poisoning",
          "misconception": "Targets [attack mechanism confusion]: Backdoor poisoning introduces a trigger for specific misclassifications, not general performance degradation."
        },
        {
          "text": "Model poisoning",
          "misconception": "Targets [attack vector confusion]: Model poisoning directly manipulates model parameters, whereas this describes data manipulation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Availability poisoning aims to indiscriminately degrade the ML model's performance on all samples, making it less useful, unlike targeted or backdoor poisoning which have specific misclassification goals.",
        "distractor_analysis": "The distractors represent attacks with different objectives: targeted (specific samples), backdoor (trigger-based misclassification), and model poisoning (parameter manipulation).",
        "analogy": "Availability poisoning is like sabotaging a chef's entire pantry so all dishes turn out poorly, whereas targeted poisoning is like spoiling just one specific dish, and backdoor poisoning is like adding a secret ingredient that makes a dish taste bad only when a specific guest eats it."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_POISONING_ATTACKS"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 discusses Generative AI (GenAI) attacks. What is a key difference in attacker objectives compared to PredAI systems?",
      "correct_answer": "Misuse enablement, aiming to circumvent safety restrictions.",
      "distractors": [
        {
          "text": "Only integrity violations, as GenAI outputs are less critical than predictions.",
          "misconception": "Targets [scope limitation]: GenAI outputs can have significant integrity and availability impacts, and misuse is a distinct objective."
        },
        {
          "text": "Exclusively privacy compromises, as GenAI models handle more sensitive user data.",
          "misconception": "Targets [overgeneralization]: While privacy is a concern, misuse enablement and integrity are also key GenAI-specific objectives."
        },
        {
          "text": "Focus solely on data poisoning, as GenAI training data is more vulnerable.",
          "misconception": "Targets [attack vector limitation]: GenAI faces various attacks beyond just data poisoning, including prompt injection and misuse enablement."
        }
      ],
      "detailed_explanation": {
        "core_logic": "GenAI systems often have built-in safety restrictions, making 'misuse enablement'—circumventing these restrictions—a distinct and critical attacker objective not as prominent in PredAI.",
        "distractor_analysis": "The first distractor incorrectly downplays GenAI integrity risks. The second overemphasizes privacy while ignoring other GenAI-specific threats. The third limits GenAI attacks to only data poisoning.",
        "analogy": "For PredAI, the goal might be to trick a security camera (integrity). For GenAI, an additional goal is to trick a chatbot into saying something harmful it's programmed not to say (misuse enablement)."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_GENAI_ATTACKS",
        "AML_PREDAI_ATTACKS"
      ]
    },
    {
      "question_text": "In the context of Generative AI (GenAI) attacks, what is 'Prompt Injection'?",
      "correct_answer": "Exploiting the concatenation of untrusted user input with higher-trust system prompts to induce unintended behavior.",
      "distractors": [
        {
          "text": "Modifying the training data of a GenAI model to insert backdoors.",
          "misconception": "Targets [attack vector confusion]: Describes data poisoning or backdoor attacks, not prompt injection."
        },
        {
          "text": "Extracting the model's weights or architecture through API queries.",
          "misconception": "Targets [attack type confusion]: Describes model extraction, not prompt injection."
        },
        {
          "text": "Causing the GenAI model to consume excessive computational resources.",
          "misconception": "Targets [attack objective confusion]: Describes an availability attack (e.g., denial-of-service), not prompt injection."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Prompt injection exploits how GenAI models process concatenated inputs, allowing untrusted user data to override or manipulate the intended system prompts, leading to unintended outputs.",
        "distractor_analysis": "The distractors describe data poisoning, model extraction, and availability attacks, respectively, which are distinct from prompt injection.",
        "analogy": "Prompt injection is like tricking a customer service agent by embedding a hidden instruction within a customer's request that makes the agent bypass its usual protocols."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_GENAI_ATTACKS",
        "PROMPT_INJECTION"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2 E2025, what is the primary risk associated with 'Indirect Prompt Injection' attacks in GenAI systems?",
      "correct_answer": "Attackers can manipulate external resources ingested by the model to inject prompts without direct user interaction.",
      "distractors": [
        {
          "text": "Attackers directly interact with the model's API to inject malicious commands.",
          "misconception": "Targets [attack vector confusion]: This describes direct prompt injection, not indirect."
        },
        {
          "text": "Attackers modify the model's training data to alter its core behavior.",
          "misconception": "Targets [attack stage confusion]: This describes data poisoning, which occurs during training, not inference-time indirect injection."
        },
        {
          "text": "Attackers steal the model's weights to replicate its functionality.",
          "misconception": "Targets [attack objective confusion]: This describes model extraction, not indirect prompt injection."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Indirect prompt injection leverages the model's interaction with external resources (like web pages or documents) to inject malicious prompts, bypassing the primary user's direct input channel.",
        "distractor_analysis": "The first distractor describes direct prompt injection. The second describes data poisoning. The third describes model extraction.",
        "analogy": "Direct prompt injection is like whispering a secret command to a guard. Indirect prompt injection is like leaving a note on a bulletin board that the guard is programmed to read and act upon, without the guard knowing it's a malicious instruction."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_GENAI_ATTACKS",
        "INDIRECT_PROMPT_INJECTION"
      ]
    },
    {
      "question_text": "NIST AI 100-1, the Artificial Intelligence Risk Management Framework (AI RMF), outlines characteristics of trustworthy AI. Which characteristic is MOST directly related to preventing unauthorized access and use of AI systems and their data?",
      "correct_answer": "Secure and Resilient",
      "distractors": [
        {
          "text": "Valid and Reliable",
          "misconception": "Targets [characteristic confusion]: Focuses on accuracy and robustness, not protection against attacks."
        },
        {
          "text": "Accountable and Transparent",
          "misconception": "Targets [characteristic confusion]: Focuses on explainability and traceability, not direct protection from attacks."
        },
        {
          "text": "Fair – with Harmful Bias Managed",
          "misconception": "Targets [characteristic confusion]: Focuses on equity and non-discrimination, not security against malicious actors."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Secure and Resilient' characteristic directly addresses protection against attacks, unauthorized access, and the ability to maintain function despite adverse events, aligning with cybersecurity principles.",
        "distractor_analysis": "Each distractor represents a different trustworthiness characteristic: Valid/Reliable (accuracy), Accountable/Transparent (explainability), and Fair (bias management).",
        "analogy": "Being 'Secure and Resilient' is like having strong locks on your house (security) and a backup generator for power outages (resilience), protecting against break-ins and disruptions."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "NIST_AI_RMF",
        "AI_TRUSTWORTHINESS"
      ]
    },
    {
      "question_text": "According to the NIST AI RMF, what is a key challenge in 'Risk Measurement' for AI systems?",
      "correct_answer": "The lack of consensus on robust and verifiable measurement methods for risk and trustworthiness.",
      "distractors": [
        {
          "text": "AI systems are too fast to measure their performance accurately.",
          "misconception": "Targets [technical misunderstanding]: Speed is not the primary measurement challenge; it's the complexity and lack of standardized metrics."
        },
        {
          "text": "Risk measurement is only possible in controlled laboratory settings.",
          "misconception": "Targets [scope limitation]: The NIST AI RMF emphasizes measuring risk in real-world settings, not just labs."
        },
        {
          "text": "AI risks are always static and do not change over time.",
          "misconception": "Targets [dynamic nature misunderstanding]: AI risks are dynamic and evolve, requiring continuous measurement."
        }
      ],
      "detailed_explanation": {
        "core_logic": "A significant challenge in AI risk management is the absence of universally agreed-upon, robust metrics for quantifying AI risks and trustworthiness, making consistent measurement difficult.",
        "distractor_analysis": "The first distractor misattributes the challenge to speed. The second incorrectly limits measurement to lab settings. The third denies the dynamic nature of AI risks.",
        "analogy": "Trying to measure AI risk without standard metrics is like trying to weigh different objects without a scale – you know there's a difference, but you can't quantify it reliably."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "NIST_AI_RMF",
        "AI_RISK_MEASUREMENT"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes four functions. Which function is described as 'cross-cutting' and intended to be infused throughout the other three functions (MAP, MEASURE, MANAGE)?",
      "correct_answer": "GOVERN",
      "distractors": [
        {
          "text": "MAP",
          "misconception": "Targets [functional role confusion]: MAP focuses on context establishment, not overarching governance."
        },
        {
          "text": "MEASURE",
          "misconception": "Targets [functional role confusion]: MEASURE focuses on assessment and benchmarking, not overarching governance."
        },
        {
          "text": "MANAGE",
          "misconception": "Targets [functional role confusion]: MANAGE focuses on risk treatment and response, not overarching governance."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The GOVERN function establishes the organizational culture, policies, and accountability structures that underpin and guide all other AI risk management activities (MAP, MEASURE, MANAGE).",
        "distractor_analysis": "Each distractor represents one of the other core functions of the AI RMF, testing the understanding of their distinct roles and the cross-cutting nature of governance.",
        "analogy": "GOVERN is like the company's board of directors, setting the overall strategy and ethical guidelines, while MAP, MEASURE, and MANAGE are like different departments executing specific tasks based on those guidelines."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "remember",
      "prerequisites": [
        "NIST_AI_RMF_CORE"
      ]
    },
    {
      "question_text": "In the NIST AI RMF, the 'MAP' function is crucial for framing risks. What is a key outcome of effectively performing the MAP function?",
      "correct_answer": "Sufficient contextual knowledge to inform an initial go/no-go decision about designing, developing, or deploying an AI system.",
      "distractors": [
        {
          "text": "Quantifiable metrics for all identified AI risks.",
          "misconception": "Targets [functional outcome confusion]: Quantifiable metrics are outcomes of the MEASURE function, not MAP."
        },
        {
          "text": "Implemented risk treatment plans for all identified risks.",
          "misconception": "Targets [functional outcome confusion]: Risk treatment plans are outcomes of the MANAGE function, not MAP."
        },
        {
          "text": "A fully documented AI system governance structure.",
          "misconception": "Targets [functional outcome confusion]: Governance structures are established in the GOVERN function, not MAP."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The MAP function's primary goal is to establish context and understand potential impacts, providing the necessary information to decide whether an AI system should proceed through development and deployment.",
        "distractor_analysis": "Each distractor describes an outcome of a different AI RMF function: MEASURE (metrics), MANAGE (treatment plans), and GOVERN (governance structure).",
        "analogy": "The MAP function is like surveying the land before building a house – understanding the terrain, potential hazards, and intended use to decide if building is feasible and where to start."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "NIST_AI_RMF_CORE",
        "AI_RISK_MAPPING"
      ]
    },
    {
      "question_text": "Which NIST AI RMF function focuses on analyzing, assessing, benchmarking, and monitoring AI risk and related impacts?",
      "correct_answer": "MEASURE",
      "distractors": [
        {
          "text": "GOVERN",
          "misconception": "Targets [functional role confusion]: GOVERN establishes policies and culture, not direct risk measurement."
        },
        {
          "text": "MAP",
          "misconception": "Targets [functional role confusion]: MAP focuses on context and risk identification, not quantitative assessment."
        },
        {
          "text": "MANAGE",
          "misconception": "Targets [functional role confusion]: MANAGE focuses on risk treatment and response, not measurement."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The MEASURE function is dedicated to the quantitative and qualitative assessment of AI risks and impacts, using tools and methodologies to analyze, benchmark, and monitor them, thereby informing risk management decisions.",
        "distractor_analysis": "Each distractor represents a different AI RMF function: GOVERN (policy), MAP (context), and MANAGE (treatment), contrasting with MEASURE's focus on assessment.",
        "analogy": "The MEASURE function is like conducting scientific experiments and taking readings to understand the properties and performance of a system, whereas GOVERN sets the rules, MAP defines the environment, and MANAGE decides what to do with the results."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "remember",
      "prerequisites": [
        "NIST_AI_RMF_CORE",
        "AI_RISK_MEASUREMENT"
      ]
    },
    {
      "question_text": "In the NIST AI RMF, the 'MANAGE' function involves allocating resources and planning responses. What is a key aspect of risk treatment within this function?",
      "correct_answer": "Developing plans to respond to, recover from, and communicate about incidents or events.",
      "distractors": [
        {
          "text": "Identifying all potential AI risks before development begins.",
          "misconception": "Targets [timing error]: Risk identification is primarily part of the MAP function; MANAGE focuses on response to identified risks."
        },
        {
          "text": "Establishing the organizational culture for risk management.",
          "misconception": "Targets [functional role confusion]: Establishing culture is a key aspect of the GOVERN function."
        },
        {
          "text": "Measuring the accuracy and reliability of AI system outputs.",
          "misconception": "Targets [functional role confusion]: Measuring performance is the focus of the MEASURE function."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The MANAGE function focuses on actionable steps after risks are mapped and measured, including developing strategies for responding to, recovering from, and communicating about incidents, thereby treating the identified risks.",
        "distractor_analysis": "The first distractor describes risk identification (MAP). The second describes governance (GOVERN). The third describes measurement (MEASURE).",
        "analogy": "The MANAGE function is like having an emergency preparedness plan: deciding how to respond to a fire (risk treatment), how to recover afterward, and how to communicate with everyone involved."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "procedure",
      "bloom_level": "apply",
      "prerequisites": [
        "NIST_AI_RMF_CORE",
        "AI_RISK_MANAGEMENT"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2 E2025, which capability is essential for mounting 'black-box evasion attacks'?",
      "correct_answer": "Query access to the model to observe its predictions or confidence scores.",
      "distractors": [
        {
          "text": "Full knowledge of the model's architecture and parameters.",
          "misconception": "Targets [attacker knowledge confusion]: This describes white-box attacks, not black-box."
        },
        {
          "text": "Control over the training data used to build the model.",
          "misconception": "Targets [attacker capability confusion]: This describes training data control, relevant for poisoning, not black-box evasion."
        },
        {
          "text": "Ability to modify the model's source code directly.",
          "misconception": "Targets [attacker capability confusion]: This describes source code control, not query access for black-box attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Black-box attacks, including evasion, operate under the assumption that the attacker has minimal knowledge of the model, relying instead on observing its outputs through query access to craft adversarial examples.",
        "distractor_analysis": "The distractors describe white-box attacks (full knowledge), training data control (poisoning), and source code control, all distinct from the limited interaction of black-box attacks.",
        "analogy": "A black-box attack is like trying to figure out a vending machine's logic by only inserting coins and pressing buttons (query access), without knowing its internal wiring (model architecture)."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_EVASION_ATTACKS",
        "AML_ATTACKER_KNOWLEDGE"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 17,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "Anomaly Detection Using ML Models Threat Intelligence And Hunting best practices",
    "latency_ms": 34531.49
  },
  "timestamp": "2026-01-04T02:52:50.798027"
}