{
  "topic_title": "Machine Learning Analytics",
  "category": "Threat Intelligence And Hunting - Threat Intelligence Types",
  "flashcards": [
    {
      "question_text": "According to NIST AI 100-2 E2025, what is the primary goal of an 'evasion attack' in Adversarial Machine Learning (AML)?",
      "correct_answer": "To generate adversarial examples that alter the ML model's predictions while remaining imperceptible or minimally different from the original input.",
      "distractors": [
        {
          "text": "To corrupt the training data to degrade the overall performance of the ML model.",
          "misconception": "Targets [attack type confusion]: Confuses evasion attacks with poisoning attacks."
        },
        {
          "text": "To extract sensitive information about the model's training data or architecture.",
          "misconception": "Targets [attack objective confusion]: Confuses evasion attacks with privacy attacks."
        },
        {
          "text": "To cause a denial-of-service by overwhelming the ML system with queries.",
          "misconception": "Targets [attack goal confusion]: Confuses evasion attacks with availability attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks aim to fool a trained model at inference time by subtly modifying inputs, because the goal is to alter predictions without being detected. This works by crafting inputs that exploit model vulnerabilities, often by targeting decision boundaries.",
        "distractor_analysis": "The distractors wrongly attribute the goals of poisoning, privacy, and availability attacks to evasion attacks, which are distinct categories within AML.",
        "analogy": "An evasion attack is like a skilled pickpocket subtly lifting a wallet from a crowd without anyone noticing, whereas poisoning is like sabotaging the entire market to make everyone's goods unsellable."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "ML_INFERENCE"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 categorizes 'poisoning attacks' as occurring during which stage of the ML lifecycle?",
      "correct_answer": "Training stage",
      "distractors": [
        {
          "text": "Deployment stage",
          "misconception": "Targets [lifecycle confusion]: Confuses poisoning attacks with evasion or privacy attacks."
        },
        {
          "text": "Evaluation stage",
          "misconception": "Targets [lifecycle confusion]: Misunderstands when training data manipulation occurs."
        },
        {
          "text": "Data preprocessing stage",
          "misconception": "Targets [lifecycle granularity]: While related, poisoning specifically targets the model's learning process during training, not just initial data prep."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Poisoning attacks occur during the training stage because they involve corrupting the data or model parameters used for learning, therefore degrading the model's integrity or availability. This works by injecting malicious samples or modifying model updates before or during training.",
        "distractor_analysis": "The distractors incorrectly place poisoning attacks in the deployment, evaluation, or preprocessing stages, failing to recognize that these attacks directly interfere with the model's learning process.",
        "analogy": "Poisoning an ML model is like adding bad ingredients to a recipe while it's being cooked (training), which ruins the final dish (model), rather than trying to alter the dish after it's served (deployment)."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "remember",
      "prerequisites": [
        "AML_BASICS",
        "ML_LIFECYCLE"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2 E2025, what is the primary objective of a 'privacy compromise' attack in Adversarial Machine Learning (AML)?",
      "correct_answer": "To cause the unintended leakage of restricted or proprietary information from an AI system, such as details about its training data, weights, or architecture.",
      "distractors": [
        {
          "text": "To degrade the overall performance and availability of the AI system.",
          "misconception": "Targets [attack objective confusion]: Confuses privacy attacks with availability attacks."
        },
        {
          "text": "To force the AI system to produce incorrect predictions or outputs.",
          "misconception": "Targets [attack objective confusion]: Confuses privacy attacks with integrity attacks."
        },
        {
          "text": "To manipulate the training data to introduce specific vulnerabilities.",
          "misconception": "Targets [attack type confusion]: Confuses privacy attacks with poisoning attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Privacy compromise attacks aim to extract sensitive information because ML models can inadvertently memorize or reveal details about their training data or internal structure. This works by exploiting model behavior through query access or by analyzing model outputs.",
        "distractor_analysis": "The distractors incorrectly assign the objectives of availability, integrity, and poisoning attacks to privacy compromise attacks, which specifically focus on information leakage.",
        "analogy": "A privacy compromise attack is like an eavesdropper listening in on confidential conversations (model's internal data) rather than trying to disrupt the meeting (availability) or change what's being said (integrity)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "DATA_PRIVACY"
      ]
    },
    {
      "question_text": "In the context of Adversarial Machine Learning (AML), what does 'attacker capability' refer to, as outlined in NIST AI 100-2 E2025?",
      "correct_answer": "The specific types of control or access an adversary has over the ML system's data, model, or processes, which they can leverage to mount an attack.",
      "distractors": [
        {
          "text": "The attacker's motivation or goal for launching the attack.",
          "misconception": "Targets [definition confusion]: Confuses capability with attacker goals or objectives."
        },
        {
          "text": "The level of knowledge the attacker possesses about the ML system's internal workings.",
          "misconception": "Targets [definition confusion]: Confuses capability with attacker knowledge (e.g., white-box vs. black-box)."
        },
        {
          "text": "The specific type of ML model being targeted (e.g., CNN, RNN, Transformer).",
          "misconception": "Targets [scope confusion]: Capability refers to attacker's control, not the target model's architecture."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Attacker capabilities define the resources and access an adversary possesses, such as training data control or query access, because these are the means by which attacks are executed. This framework helps categorize attacks based on what an attacker can realistically do.",
        "distractor_analysis": "The distractors incorrectly equate attacker capability with motivation, knowledge, or the target system's architecture, rather than the attacker's actionable control or access.",
        "analogy": "Attacker capability is like the tools a burglar has (lock picks, crowbar) â€“ it defines what they can do to gain access or cause damage, not why they are robbing the place (motivation) or how much they know about the security system (knowledge)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "ML_SYSTEMS"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 identifies 'query access' as a key attacker capability. What does this capability entail in the context of AML?",
      "correct_answer": "The ability to submit queries to a deployed ML model and receive predictions or generations, often seen in cloud-based ML services (MLaaS).",
      "distractors": [
        {
          "text": "The ability to directly modify the model's parameters during training.",
          "misconception": "Targets [capability confusion]: Confuses query access with model control during training."
        },
        {
          "text": "The ability to inject malicious data into the model's training dataset.",
          "misconception": "Targets [capability confusion]: Confuses query access with training data control."
        },
        {
          "text": "The ability to alter the source code of the ML algorithm.",
          "misconception": "Targets [capability confusion]: Confuses query access with source code control."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Query access is crucial because many ML models are deployed as services, allowing attackers to interact with them through APIs to infer behavior or elicit specific outputs. This works by submitting crafted inputs and observing the model's responses.",
        "distractor_analysis": "The distractors incorrectly describe capabilities related to model control, training data manipulation, and source code modification, which are distinct from the ability to simply query a deployed model.",
        "analogy": "Query access is like being able to ask a black-box machine questions and get answers, but without knowing how the machine works internally. It's different from being able to open the machine and change its parts (model control) or feed it bad ingredients (data poisoning)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "MLaaS"
      ]
    },
    {
      "question_text": "What is the main challenge in developing effective mitigations against 'evasion attacks', as highlighted by NIST AI 100-2 E2025?",
      "correct_answer": "Adversarial examples are widespread across various ML architectures and domains, and many proposed defenses are quickly broken by stronger adaptive attacks.",
      "distractors": [
        {
          "text": "Evasion attacks are too computationally expensive to mount in practice.",
          "misconception": "Targets [plausibility error]: While some attacks are costly, many are practical, and the challenge is defense, not attack cost."
        },
        {
          "text": "Evasion attacks only affect niche ML applications, not mainstream ones.",
          "misconception": "Targets [scope error]: Evasion attacks are demonstrated across many domains, including critical ones."
        },
        {
          "text": "The underlying ML algorithms are inherently secure and require no special defenses.",
          "misconception": "Targets [fundamental misunderstanding]: ML algorithms are known to be vulnerable to adversarial examples."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Developing robust defenses against evasion attacks is challenging because adversarial examples exploit fundamental properties of ML models, and attackers continuously develop adaptive strategies. Therefore, defenses must be rigorously tested against strong, evolving attacks.",
        "distractor_analysis": "The distractors present inaccurate claims about the cost, scope, and inherent security of ML models, overlooking the core difficulty of creating defenses that withstand adaptive adversarial techniques.",
        "analogy": "Defending against evasion attacks is like trying to build a fortress against an enemy that constantly invents new siege weapons; the defenses must be adaptable and strong, not static or based on outdated assumptions."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_EVASION",
        "ML_DEFENSES"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 discusses 'clean-label poisoning attacks'. What is the key characteristic of this attack type?",
      "correct_answer": "The attacker can only control the training examples but not their labels, making it a more realistic threat in scenarios where labeling is external.",
      "distractors": [
        {
          "text": "The attacker controls both the training examples and their labels.",
          "misconception": "Targets [attack type confusion]: This describes standard poisoning, not clean-label poisoning."
        },
        {
          "text": "The attacker only modifies the model parameters, not the training data.",
          "misconception": "Targets [attack type confusion]: This describes model poisoning, not data poisoning."
        },
        {
          "text": "The attacker's modifications are only effective during the deployment stage.",
          "misconception": "Targets [lifecycle confusion]: Poisoning attacks occur during the training stage."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Clean-label poisoning is a significant threat because attackers can manipulate data without altering labels, which is common in external labeling scenarios, therefore posing a realistic challenge. This works by subtly altering data samples to achieve a desired outcome during training.",
        "distractor_analysis": "The distractors incorrectly describe standard poisoning, model poisoning, or misplace the attack's timing in the lifecycle, failing to capture the specific constraint of label control in clean-label poisoning.",
        "analogy": "Clean-label poisoning is like subtly altering the ingredients in a dish without changing the recipe card; the cook (model) follows the original recipe but produces a flawed outcome because the ingredients were tampered with."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_POISONING",
        "ML_TRAINING"
      ]
    },
    {
      "question_text": "What is the primary concern with 'backdoor poisoning attacks' as described in NIST AI 100-2 E2025?",
      "correct_answer": "They cause a targeted model to misclassify samples containing a specific, often imperceptible, backdoor pattern or trigger, leading to malicious behavior on command.",
      "distractors": [
        {
          "text": "They indiscriminately degrade the performance of the entire ML model.",
          "misconception": "Targets [attack type confusion]: This describes availability poisoning, not backdoor attacks."
        },
        {
          "text": "They require the attacker to have full white-box access to the model's architecture.",
          "misconception": "Targets [capability assumption error]: Backdoor attacks can be mounted in black-box settings too."
        },
        {
          "text": "They are easily detectable through standard model performance monitoring.",
          "misconception": "Targets [detectability error]: Backdoor triggers are often designed to be stealthy and bypass standard detection."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Backdoor poisoning attacks are concerning because they create a hidden vulnerability, allowing attackers to trigger malicious behavior on demand, therefore posing a significant security risk. This works by associating a specific trigger pattern with a desired misclassification during training.",
        "distractor_analysis": "The distractors misrepresent backdoor attacks by confusing them with availability attacks, imposing unnecessary capability requirements, or incorrectly stating their detectability.",
        "analogy": "A backdoor attack is like a hidden switch in a system that, when flipped (trigger pattern), causes a specific malfunction, rather than a general system failure (availability poisoning)."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_POISONING",
        "ML_SECURITY"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2 E2025, what is 'model extraction' in the context of AML?",
      "correct_answer": "An attack where an adversary attempts to learn information about an ML model's architecture and parameters by submitting specially crafted queries to it.",
      "distractors": [
        {
          "text": "An attack that corrupts the model's parameters during the training phase.",
          "misconception": "Targets [attack type confusion]: Confuses model extraction with model poisoning."
        },
        {
          "text": "An attack that reconstructs sensitive training data from the model's outputs.",
          "misconception": "Targets [attack objective confusion]: Confuses model extraction with data reconstruction attacks."
        },
        {
          "text": "An attack that modifies the model's predictions on specific inputs.",
          "misconception": "Targets [attack objective confusion]: Confuses model extraction with evasion attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Model extraction is a concern because it can reveal proprietary model details, enabling downstream attacks, therefore necessitating defenses. This works by analyzing the model's responses to carefully designed queries to infer its internal structure and weights.",
        "distractor_analysis": "The distractors incorrectly describe model poisoning, data reconstruction, and evasion attacks, failing to identify that model extraction specifically targets learning about the model itself.",
        "analogy": "Model extraction is like trying to reverse-engineer a complex machine by observing its inputs and outputs, aiming to understand its design, rather than trying to break it (evasion) or sabotage its manufacturing process (poisoning)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "ML_MODELS"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 highlights 'membership inference attacks'. What is the primary goal of such an attack?",
      "correct_answer": "To determine whether a specific data sample was part of the dataset used to train a machine learning model.",
      "distractors": [
        {
          "text": "To reconstruct the exact sensitive data records from the training set.",
          "misconception": "Targets [attack objective confusion]: Confuses membership inference with data reconstruction."
        },
        {
          "text": "To infer global properties or attributes of the entire training dataset.",
          "misconception": "Targets [attack objective confusion]: Confuses membership inference with property inference."
        },
        {
          "text": "To cause the model to misclassify the specific data sample.",
          "misconception": "Targets [attack objective confusion]: Confuses membership inference with evasion attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Membership inference attacks are a privacy concern because knowing if an individual's data was used in training can have privacy implications, therefore requiring robust defenses. This works by observing the model's confidence or loss on a sample to infer its training status.",
        "distractor_analysis": "The distractors incorrectly attribute the goals of data reconstruction, property inference, and evasion attacks to membership inference, which specifically focuses on determining training set inclusion.",
        "analogy": "A membership inference attack is like trying to find out if a specific person attended a private meeting by observing how they react to questions about the meeting, rather than trying to get a transcript of the meeting (data reconstruction) or learn the meeting's overall agenda (property inference)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_PRIVACY",
        "ML_TRAINING_DATA"
      ]
    },
    {
      "question_text": "In Generative AI (GenAI), what is 'prompt injection' as defined by NIST AI 100-2 E2025?",
      "correct_answer": "An attack where an attacker exploits the concatenation of untrusted user output with higher-trust instructions (like a system prompt) to induce unintended behavior.",
      "distractors": [
        {
          "text": "An attack that corrupts the training data used to build the GenAI model.",
          "misconception": "Targets [attack type confusion]: Confuses prompt injection with data poisoning."
        },
        {
          "text": "An attack that modifies the GenAI model's parameters after deployment.",
          "misconception": "Targets [attack type confusion]: Confuses prompt injection with model poisoning or manipulation."
        },
        {
          "text": "An attack that aims to extract the GenAI model's architecture or weights.",
          "misconception": "Targets [attack objective confusion]: Confuses prompt injection with model extraction."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Prompt injection is a critical vulnerability because it exploits how GenAI models process instructions and data together, therefore allowing attackers to hijack model behavior. This works by crafting malicious inputs that override or manipulate the intended system prompts.",
        "distractor_analysis": "The distractors incorrectly associate prompt injection with data poisoning, model parameter modification, or model extraction, failing to recognize its focus on manipulating model instructions via input.",
        "analogy": "Prompt injection is like tricking a helpful assistant (GenAI) into performing a harmful task by embedding a malicious instruction within a seemingly normal request, rather than corrupting the assistant's training manual (data poisoning) or altering their brain (model poisoning)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_GENAI",
        "PROMPT_ENGINEERING"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 introduces 'indirect prompt injection' for GenAI. What distinguishes this from direct prompt injection?",
      "correct_answer": "It is mounted by a third party manipulating external resources that the GenAI system interacts with, rather than by the primary user directly querying the model.",
      "distractors": [
        {
          "text": "It only affects the availability of the GenAI system, not its integrity or privacy.",
          "misconception": "Targets [scope error]: Indirect prompt injection can impact availability, integrity, and privacy."
        },
        {
          "text": "It requires white-box access to the GenAI model's internal workings.",
          "misconception": "Targets [capability assumption error]: Indirect prompt injection often relies on resource control, not necessarily white-box access."
        },
        {
          "text": "It is exclusively used to enable misuse of the GenAI system's capabilities.",
          "misconception": "Targets [attack objective confusion]: Indirect prompt injection can lead to availability, integrity, or privacy compromises, not solely misuse enablement."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Indirect prompt injection is distinct because it leverages the GenAI system's interaction with external data sources, therefore allowing remote manipulation without direct user interaction. This works by compromising resources that the GenAI system ingests as context.",
        "distractor_analysis": "The distractors incorrectly limit the impact of indirect prompt injection, impose unwarranted technical requirements, or misrepresent its primary objectives, failing to capture the core mechanism of manipulating external resources.",
        "analogy": "Direct prompt injection is like whispering a secret command to the assistant yourself. Indirect prompt injection is like planting a malicious note in a document the assistant is supposed to read, causing them to act on your command without you directly interacting with them."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_GENAI",
        "PROMPT_ENGINEERING"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2 E2025, what is a key challenge in managing risks associated with Adversarial Machine Learning (AML) mitigations?",
      "correct_answer": "Many AML mitigations are empirical and lack theoretical guarantees, making it difficult to ensure robustness against unforeseen attacks.",
      "distractors": [
        {
          "text": "AML mitigations are universally effective against all types of attacks.",
          "misconception": "Targets [overgeneralization error]: Mitigations have limitations and varying effectiveness."
        },
        {
          "text": "The cost of implementing AML mitigations is prohibitively high for most organizations.",
          "misconception": "Targets [cost assumption error]: While costs vary, the primary challenge is effectiveness and guarantees, not just cost."
        },
        {
          "text": "There is a lack of interest from researchers in developing new AML defenses.",
          "misconception": "Targets [research landscape misunderstanding]: AML is an active and growing research area."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The empirical nature of many AML mitigations poses a challenge because they may not provide provable guarantees, therefore leaving systems vulnerable to novel or adaptive attacks. This works by relying on observed effectiveness rather than theoretical soundness.",
        "distractor_analysis": "The distractors present inaccurate claims about the universal effectiveness, prohibitive cost, or lack of research interest in AML mitigations, overlooking the core issue of their empirical basis and lack of theoretical guarantees.",
        "analogy": "Relying solely on empirical AML mitigations is like using a weather forecast based only on past observations without understanding the underlying meteorological principles; it might be right often, but it can fail unexpectedly when conditions change."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_MITIGATION",
        "ML_SECURITY_PRINCIPLES"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 discusses the trade-offs between attributes of trustworthy AI. Which pair of attributes is often in tension, meaning improving one may negatively impact the other?",
      "correct_answer": "Accuracy and Adversarial Robustness",
      "distractors": [
        {
          "text": "Accuracy and Fairness",
          "misconception": "Targets [trade-off confusion]: While trade-offs exist, accuracy and fairness are not always inversely proportional in the same way as robustness."
        },
        {
          "text": "Explainability and Privacy",
          "misconception": "Targets [trade-off confusion]: While complex, these are not the primary trade-off highlighted for adversarial robustness."
        },
        {
          "text": "Scalability and Interpretability",
          "misconception": "Targets [trade-off confusion]: These are important considerations but not the core trade-off discussed concerning adversarial robustness."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Accuracy and adversarial robustness often exhibit a trade-off because models optimized solely for accuracy may rely on non-robust features, therefore making them vulnerable to adversarial perturbations. This works by optimizing for average-case performance (accuracy) potentially at the expense of worst-case performance (robustness).",
        "distractor_analysis": "The distractors suggest other pairs of AI attributes as primary trade-offs, whereas NIST AI 100-2 specifically highlights the tension between accuracy and adversarial robustness as a key challenge.",
        "analogy": "Optimizing for accuracy is like training a sprinter to be the fastest on a clear track; optimizing for robustness is like training them to also perform well on a difficult, obstacle-filled course. Excelling at both simultaneously is extremely difficult."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "TRUSTWORTHY_AI",
        "AML_CHALLENGES"
      ]
    },
    {
      "question_text": "In the context of Generative AI (GenAI) security, what is a 'jailbreak' according to NIST AI 100-2 E2025?",
      "correct_answer": "A direct prompting attack designed to circumvent restrictions placed on model outputs, enabling the generation of harmful or undesirable content.",
      "distractors": [
        {
          "text": "An attack that corrupts the training data with malicious examples.",
          "misconception": "Targets [attack type confusion]: Confuses jailbreaking with data poisoning."
        },
        {
          "text": "An attack that extracts the model's system prompt or internal configuration.",
          "misconception": "Targets [attack objective confusion]: Confuses jailbreaking with prompt extraction or model extraction."
        },
        {
          "text": "An attack that causes the GenAI model to become unavailable.",
          "misconception": "Targets [attack objective confusion]: Confuses jailbreaking with availability attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Jailbreaking is a critical concern because it bypasses safety mechanisms, therefore allowing GenAI models to produce harmful outputs. This works by crafting specific prompts that exploit the model's alignment training or instruction-following capabilities to override safety protocols.",
        "distractor_analysis": "The distractors incorrectly associate jailbreaking with data poisoning, information extraction, or availability attacks, failing to recognize its specific goal of circumventing safety restrictions to enable misuse.",
        "analogy": "A jailbreak is like finding a loophole in a robot's safety programming that allows it to perform actions it was explicitly forbidden from doing, rather than disabling the robot (availability) or stealing its blueprints (extraction)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_GENAI",
        "PROMPT_ENGINEERING"
      ]
    },
    {
      "question_text": "NIST AI 100-2 E2025 suggests that 'supply chain attacks' in AI are complex. Which of the following is an AI-specific supply chain risk, beyond traditional software vulnerabilities?",
      "correct_answer": "The risk of using third-party-developed AI models that are poisoned or contain malicious backdoors.",
      "distractors": [
        {
          "text": "Vulnerabilities in the cloud infrastructure hosting the AI models.",
          "misconception": "Targets [scope confusion]: This is a traditional IT infrastructure risk, not AI-specific supply chain."
        },
        {
          "text": "Weaknesses in the API security protocols used for model access.",
          "misconception": "Targets [scope confusion]: This is a general API security issue, not specific to AI model supply chain."
        },
        {
          "text": "Insecure coding practices by developers of AI applications.",
          "misconception": "Targets [scope confusion]: This is a general software development risk, not specific to AI model supply chain."
        }
      ],
      "detailed_explanation": {
        "core_logic": "AI supply chain attacks are complex because they involve AI-specific dependencies like pre-trained models, therefore introducing risks beyond traditional software vulnerabilities. This works by attackers compromising models or data before they reach the end-user or developer.",
        "distractor_analysis": "The distractors describe general cybersecurity risks (infrastructure, API, coding practices) rather than the AI-specific risks of compromised models or data within the supply chain.",
        "analogy": "An AI supply chain attack is like buying a pre-assembled component for a complex machine, only to find out that component was secretly designed to fail or malfunction under certain conditions, unlike traditional software risks which are more about the assembly process itself."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_SUPPLY_CHAIN",
        "AML_POISONING"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 16,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "Machine Learning Analytics Threat Intelligence And Hunting best practices",
    "latency_ms": 26805.434999999998
  },
  "timestamp": "2026-01-04T01:42:01.954476"
}