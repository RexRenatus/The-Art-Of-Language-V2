{
  "topic_title": "AI/ML System Threat Intelligence",
  "category": "Threat Intelligence And Hunting - Threat Intelligence Platforms",
  "flashcards": [
    {
      "question_text": "According to NIST, what is a primary goal of establishing standardized terminology in Adversarial Machine Learning (AML)?",
      "correct_answer": "To create a common language for understanding and discussing AML concepts among researchers and practitioners.",
      "distractors": [
        {
          "text": "To develop proprietary attack methodologies for commercial use.",
          "misconception": "Targets [misaligned objective]: Confuses research goals with commercial exploitation."
        },
        {
          "text": "To standardize the implementation of AI models across different industries.",
          "misconception": "Targets [scope confusion]: Mixes AML terminology with general AI model deployment standards."
        },
        {
          "text": "To create a universal defense mechanism against all AI threats.",
          "misconception": "Targets [overly broad solution]: Assumes a single, universal defense exists for all AML attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "NIST's AI 100-2 report emphasizes that standardized terminology is crucial because it establishes a common language, enabling better communication and understanding of the rapidly evolving AML landscape among diverse stakeholders.",
        "distractor_analysis": "The first distractor suggests a commercial, rather than academic/research, goal. The second misinterprets the scope, confusing terminology with implementation standards. The third proposes an unrealistic universal defense.",
        "analogy": "Establishing standardized terminology in AML is like creating a shared dictionary for a new scientific field; it ensures everyone is speaking the same language when discussing complex concepts and challenges."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS"
      ]
    },
    {
      "question_text": "Which type of adversarial attack, as identified by NIST, involves manipulating the training data of an AI model to compromise its performance?",
      "correct_answer": "Poisoning attacks",
      "distractors": [
        {
          "text": "Evasion attacks",
          "misconception": "Targets [attack type confusion]: Evasion attacks target model inference with manipulated inputs, not training data."
        },
        {
          "text": "Privacy attacks",
          "misconception": "Targets [attack objective confusion]: Privacy attacks aim to extract sensitive information, not corrupt training data."
        },
        {
          "text": "Abuse attacks",
          "misconception": "Targets [attack vector confusion]: Abuse attacks repurpose AI systems for malicious outputs, not data manipulation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Poisoning attacks directly target the integrity of the AI model's learning process by injecting malicious data during training, thereby corrupting its performance and decision-making capabilities.",
        "distractor_analysis": "Evasion attacks occur during inference, privacy attacks focus on data extraction, and abuse attacks misuse the model's output, differentiating them from poisoning attacks that corrupt the training data itself.",
        "analogy": "Poisoning an AI model's training data is like giving a chef bad ingredients; the resulting dish (the model's output) will be flawed because the foundational elements were compromised."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "AI_TRAINING_PROCESS"
      ]
    },
    {
      "question_text": "In the context of AI/ML system threat intelligence, what is the primary benefit of integrating threat feeds from sources like NIST's AI 100-2 report?",
      "correct_answer": "To gain insights into emerging adversarial attack techniques and potential mitigation strategies.",
      "distractors": [
        {
          "text": "To automate the deployment of AI models in production environments.",
          "misconception": "Targets [functional mismatch]: Threat intelligence focuses on security, not deployment automation."
        },
        {
          "text": "To provide real-time performance monitoring of AI algorithms.",
          "misconception": "Targets [operational vs. security focus]: Performance monitoring is distinct from threat intelligence."
        },
        {
          "text": "To generate synthetic data for training new AI models.",
          "misconception": "Targets [data generation vs. threat analysis]: Threat intelligence informs security, not data synthesis."
        }
      ],
      "detailed_explanation": {
        "core_logic": "NIST's AI 100-2 report details adversarial attacks and mitigations, providing crucial threat intelligence that helps organizations understand and defend against evolving threats to AI/ML systems, thereby enhancing their security posture.",
        "distractor_analysis": "The distractors describe functions unrelated to threat intelligence: deployment automation, performance monitoring, and synthetic data generation, which do not align with the purpose of analyzing adversarial ML threats.",
        "analogy": "Integrating NIST's AI threat intelligence is like subscribing to a security bulletin for your AI systems; it warns you about new vulnerabilities and how to protect yourself, rather than helping you build the system faster."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "THREAT_INTEL_BASICS",
        "AML_BASICS"
      ]
    },
    {
      "question_text": "What is the main difference between 'evasion attacks' and 'poisoning attacks' in AI/ML systems, according to NIST's taxonomy?",
      "correct_answer": "Evasion attacks aim to deceive a trained model during inference, while poisoning attacks corrupt the model during its training phase.",
      "distractors": [
        {
          "text": "Evasion attacks target the model's architecture, while poisoning attacks target its output.",
          "misconception": "Targets [attack phase confusion]: Both can affect output, but the core difference is the phase of attack (training vs. inference)."
        },
        {
          "text": "Evasion attacks are always white-box, while poisoning attacks are always black-box.",
          "misconception": "Targets [attack knowledge confusion]: Both attack types can occur in white-box or black-box scenarios."
        },
        {
          "text": "Evasion attacks are used for data reconstruction, while poisoning attacks are used for model extraction.",
          "misconception": "Targets [attack objective confusion]: These are distinct attack types with different goals (deception vs. corruption)."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks manipulate input data to cause misclassification during inference, exploiting the model's learned patterns. Poisoning attacks, conversely, inject malicious data into the training set, fundamentally altering the model's learned parameters and behavior.",
        "distractor_analysis": "The first distractor incorrectly assigns targeting of architecture vs. output as the primary difference. The second wrongly assumes fixed knowledge requirements for each attack type. The third confuses the objectives of evasion/poisoning with privacy/extraction attacks.",
        "analogy": "An evasion attack is like trying to trick a security guard by wearing a disguise (manipulating input at inference). A poisoning attack is like sabotaging the guard's training manual (corrupting data during training)."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "AI_TRAINING_PROCESS",
        "AI_INFERENCE_PROCESS"
      ]
    },
    {
      "question_text": "Consider an AI system used for medical image analysis. If an attacker subtly alters training images to cause misdiagnosis of certain conditions, what type of attack is this?",
      "correct_answer": "Poisoning attack",
      "distractors": [
        {
          "text": "Evasion attack",
          "misconception": "Targets [attack phase confusion]: This targets training data, not inference inputs."
        },
        {
          "text": "Privacy attack",
          "misconception": "Targets [attack objective confusion]: The goal is to corrupt the model, not extract data."
        },
        {
          "text": "Denial-of-service attack",
          "misconception": "Targets [attack type mismatch]: This attack aims to corrupt the model's function, not make it unavailable."
        }
      ],
      "detailed_explanation": {
        "core_logic": "By altering the training data (medical images), the attacker directly corrupts the AI model's learning process, leading to flawed diagnostic capabilities. This manipulation of the training set is the hallmark of a poisoning attack.",
        "distractor_analysis": "Evasion attacks would involve manipulating a specific image during inference to cause a misdiagnosis. Privacy attacks would aim to extract patient data. Denial-of-service attacks would aim to make the system unusable.",
        "analogy": "This is like intentionally teaching a student doctor with incorrect medical textbooks; their future diagnoses will be flawed because their foundational knowledge was corrupted."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "AI_TRAINING_PROCESS"
      ]
    },
    {
      "question_text": "What is the purpose of 'adversarial training' as a mitigation technique against AI/ML cyberattacks?",
      "correct_answer": "To improve the model's robustness by exposing it to adversarial examples during the training process.",
      "distractors": [
        {
          "text": "To automatically detect and remove malicious data from the training set.",
          "misconception": "Targets [mitigation mechanism confusion]: Detection/removal is a separate process; adversarial training uses adversarial data."
        },
        {
          "text": "To encrypt the AI model's parameters to prevent unauthorized access.",
          "misconception": "Targets [security control confusion]: Encryption protects model integrity, but adversarial training focuses on robustness against attacks."
        },
        {
          "text": "To generate diverse datasets for improving model generalization.",
          "misconception": "Targets [objective confusion]: While diversity is good, adversarial training specifically uses adversarial examples for robustness."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Adversarial training works by incorporating adversarial examples into the training data, forcing the model to learn to correctly classify or process these perturbed inputs, thereby increasing its resilience against such attacks during deployment.",
        "distractor_analysis": "The first distractor describes data sanitization, not adversarial training. The second describes model protection through encryption. The third describes general data augmentation, not the specific use of adversarial examples for robustness.",
        "analogy": "Adversarial training is like a boxer sparring with opponents who use unusual techniques; it prepares them to handle unexpected moves in a real fight."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "procedure",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "AI_TRAINING_PROCESS",
        "AML_MITIGATION"
      ]
    },
    {
      "question_text": "According to NIST, which category of attacks specifically targets Generative AI (GenAI) systems by repurposing their intended use for malicious purposes?",
      "correct_answer": "Abuse attacks",
      "distractors": [
        {
          "text": "Evasion attacks",
          "misconception": "Targets [attack category confusion]: Evasion attacks primarily affect predictive models by manipulating inputs."
        },
        {
          "text": "Poisoning attacks",
          "misconception": "Targets [attack category confusion]: Poisoning attacks corrupt the training data of any ML model."
        },
        {
          "text": "Membership inference attacks",
          "misconception": "Targets [attack category confusion]: These are a type of privacy attack, not abuse."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Abuse attacks are defined by NIST as those where attackers repurpose a GenAI system's capabilities for malicious ends, such as generating hate speech or malicious code, directly exploiting its generative function.",
        "distractor_analysis": "Evasion and poisoning attacks are broader AML categories. Membership inference is a privacy attack. Abuse attacks specifically describe the malicious repurposing of GenAI outputs.",
        "analogy": "An abuse attack on a GenAI is like giving a powerful speech synthesizer the ability to generate hate speech instead of informative content; the tool's function is perverted for harm."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "GENAI_BASICS"
      ]
    },
    {
      "question_text": "Which of the following best describes a 'privacy attack' in the context of AI/ML systems, as outlined by NIST?",
      "correct_answer": "Techniques used to exploit vulnerabilities in AI systems to compromise users' sensitive data.",
      "distractors": [
        {
          "text": "Attacks that manipulate the model's decision boundaries during training.",
          "misconception": "Targets [attack objective confusion]: This describes poisoning attacks, not privacy attacks."
        },
        {
          "text": "Attacks designed to cause the AI model to produce incorrect outputs during inference.",
          "misconception": "Targets [attack objective confusion]: This describes evasion attacks, not privacy attacks."
        },
        {
          "text": "Attacks that aim to make the AI system unavailable to legitimate users.",
          "misconception": "Targets [attack objective confusion]: This describes denial-of-service attacks, not privacy attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Privacy attacks in AI/ML systems focus on extracting or inferring sensitive information about individuals or the training data itself, exploiting how the model processes and potentially reveals data characteristics.",
        "distractor_analysis": "The distractors describe poisoning (corrupting training), evasion (deceiving inference), and denial-of-service (making unavailable) attacks, all of which have different objectives than compromising user data privacy.",
        "analogy": "A privacy attack on an AI system is like a nosy neighbor trying to piece together personal information from discarded mail; they're not breaking in, but they're trying to learn secrets from the data."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "DATA_PRIVACY_BASICS"
      ]
    },
    {
      "question_text": "What is the role of the U.S. AI Safety Institute and the U.K. AI Security Institute in relation to NIST's AI 100-2 report?",
      "correct_answer": "They collaborate with NIST in developing and maintaining the report on adversarial machine learning.",
      "distractors": [
        {
          "text": "They are responsible for enforcing the guidelines outlined in the report.",
          "misconception": "Targets [role confusion]: NIST reports provide guidance, not enforcement mandates."
        },
        {
          "text": "They provide funding for NIST's research into AI security.",
          "misconception": "Targets [funding vs. collaboration]: Collaboration is about joint development, not solely financial support."
        },
        {
          "text": "They independently develop similar reports without NIST involvement.",
          "misconception": "Targets [collaboration vs. independence]: The report explicitly states collaboration."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI 100-2 report explicitly mentions collaboration with the U.S. AI Safety Institute and the U.K. AI Security Institute, indicating their active participation in the development and ongoing maintenance of the AML guidance.",
        "distractor_analysis": "The distractors misrepresent the institutes' roles as enforcement, sole funding, or independent development, rather than collaborative contribution to the NIST report.",
        "analogy": "These institutes are like co-authors and reviewers for NIST's AI security guide, contributing expertise to ensure its accuracy and relevance, rather than being regulators or independent publishers."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "GOVERNMENT_STANDARDS_BODIES"
      ]
    },
    {
      "question_text": "Which NIST AI 100-2 report concept refers to attacks that manipulate the training data by injecting malicious samples, potentially leading to backdoor vulnerabilities?",
      "correct_answer": "Data poisoning",
      "distractors": [
        {
          "text": "Model extraction",
          "misconception": "Targets [attack type confusion]: Model extraction aims to steal the model, not corrupt its training."
        },
        {
          "text": "Membership inference",
          "misconception": "Targets [attack type confusion]: Membership inference determines if a data point was in the training set."
        },
        {
          "text": "Adversarial input generation",
          "misconception": "Targets [attack phase confusion]: This relates to evasion attacks during inference, not training data manipulation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data poisoning is a specific type of poisoning attack where malicious data is injected into the training set, corrupting the model's learning process and potentially creating backdoors that can be exploited later.",
        "distractor_analysis": "Model extraction and membership inference are privacy attacks. Adversarial input generation is related to evasion attacks. Data poisoning directly addresses the manipulation of training data for malicious outcomes.",
        "analogy": "Data poisoning is like slipping a student a falsified textbook before an exam; their knowledge is corrupted from the start, leading to incorrect answers."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "AI_TRAINING_PROCESS"
      ]
    },
    {
      "question_text": "In the context of AI/ML threat intelligence, what does 'white-box evasion attack' imply about the attacker's knowledge?",
      "correct_answer": "The attacker has full knowledge of the AI model's internal workings, parameters, and architecture.",
      "distractors": [
        {
          "text": "The attacker has no knowledge of the AI model's internals.",
          "misconception": "Targets [knowledge level confusion]: This describes a black-box attack."
        },
        {
          "text": "The attacker only knows the model's input and output.",
          "misconception": "Targets [knowledge level confusion]: This is a limited form of black-box knowledge."
        },
        {
          "text": "The attacker has access to the model's training data but not its architecture.",
          "misconception": "Targets [knowledge component confusion]: White-box implies knowledge of architecture and parameters, not just training data."
        }
      ],
      "detailed_explanation": {
        "core_logic": "A white-box attack assumes the attacker has complete visibility into the target system, including its algorithms, parameters, and architecture, allowing for precise manipulation to achieve evasion.",
        "distractor_analysis": "The distractors describe scenarios ranging from no knowledge (black-box) to partial knowledge, contrasting with the comprehensive internal understanding characteristic of a white-box attack.",
        "analogy": "A white-box attacker is like a mechanic who can fully disassemble and inspect a car's engine to find a weakness, whereas a black-box attacker can only observe how the car drives from the outside."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS",
        "ATTACK_VECTORS"
      ]
    },
    {
      "question_text": "How can threat intelligence derived from NIST's AI 100-2 report be practically applied to enhance the security of an AI/ML system?",
      "correct_answer": "By informing the selection and implementation of appropriate defense mechanisms, such as adversarial training or differential privacy.",
      "distractors": [
        {
          "text": "By automatically patching vulnerabilities in the AI model's code.",
          "misconception": "Targets [automation vs. strategy]: Threat intelligence informs strategy, not direct automated patching of complex AI models."
        },
        {
          "text": "By replacing the AI model with a less complex, more secure alternative.",
          "misconception": "Targets [overly simplistic solution]: Threat intelligence guides defense, not necessarily model replacement."
        },
        {
          "text": "By increasing the computational resources allocated to the AI system.",
          "misconception": "Targets [resource allocation vs. defense]: Increased resources don't inherently improve security against specific threats."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Understanding the types of attacks and their characteristics from sources like NIST's report allows security teams to proactively choose and implement targeted defenses, such as adversarial training to counter evasion or differential privacy for privacy attacks.",
        "distractor_analysis": "The distractors suggest automated patching, model replacement, or increased resources, which are not direct applications of threat intelligence insights into attack types and defenses.",
        "analogy": "Applying threat intelligence is like a doctor using knowledge of a specific disease to prescribe the right medicine; understanding the threat (attack type) leads to the correct treatment (defense mechanism)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "procedure",
      "bloom_level": "apply",
      "prerequisites": [
        "THREAT_INTEL_BASICS",
        "AML_MITIGATION"
      ]
    },
    {
      "question_text": "What is the primary concern addressed by 'privacy attacks' like membership inference and data reconstruction in AI/ML systems?",
      "correct_answer": "The potential for unauthorized disclosure or inference of sensitive information about individuals or the training dataset.",
      "distractors": [
        {
          "text": "The degradation of the AI model's accuracy and performance.",
          "misconception": "Targets [attack objective confusion]: This is the goal of poisoning or evasion attacks."
        },
        {
          "text": "The inability of the AI system to process new data inputs.",
          "misconception": "Targets [attack objective confusion]: This relates to denial-of-service or severe poisoning attacks."
        },
        {
          "text": "The manipulation of the AI model's decision-making process for malicious outputs.",
          "misconception": "Targets [attack objective confusion]: This describes evasion or abuse attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Privacy attacks are specifically designed to compromise the confidentiality of data by inferring sensitive attributes or reconstructing individual records from the AI model or its training data, thus violating user privacy.",
        "distractor_analysis": "The distractors describe the objectives of other attack types: performance degradation (poisoning/evasion), unavailability (DoS), and malicious output generation (evasion/abuse), none of which are the primary goal of privacy attacks.",
        "analogy": "Privacy attacks are like trying to learn a person's secrets by analyzing their digital footprint or overheard conversations, rather than directly stealing their belongings."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "DATA_PRIVACY_BASICS"
      ]
    },
    {
      "question_text": "According to NIST's AI 100-2 report, what is a key challenge in the lifecycle of AI systems concerning adversarial machine learning?",
      "correct_answer": "The rapid evolution of attack techniques and the difficulty in developing universally effective mitigations.",
      "distractors": [
        {
          "text": "The lack of interest from researchers in studying AI security.",
          "misconception": "Targets [research landscape misrepresentation]: AI security is a highly active research area."
        },
        {
          "text": "The excessive cost of developing AI models, hindering security investments.",
          "misconception": "Targets [economic factor confusion]: While cost is a factor, the primary challenge is the dynamic nature of threats and defenses."
        },
        {
          "text": "The limited availability of AI hardware for security testing.",
          "misconception": "Targets [unknown]: Not specified"
        }
      ],
      "detailed_explanation": {
        "core_logic": "The dynamic nature of AI/ML, with constantly emerging attack vectors and the ongoing arms race between attackers and defenders, presents a significant challenge in establishing robust and lasting security measures.",
        "distractor_analysis": "The distractors present inaccurate or less significant challenges, such as lack of research interest, excessive development costs hindering security, or limited hardware, which are not the core issues highlighted by NIST regarding AML challenges.",
        "analogy": "The challenge in AI security is like trying to build a fortress against an enemy that constantly invents new siege weapons; defenses must continuously adapt to evolving threats."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "evaluate",
      "prerequisites": [
        "AML_BASICS",
        "AI_LIFECYCLE"
      ]
    },
    {
      "question_text": "In the context of AI/ML threat intelligence, what is the significance of NIST's AI 100-2 report for organizations developing or deploying AI systems?",
      "correct_answer": "It provides a foundational understanding of AML threats and terminology, enabling better risk assessment and defense strategy development.",
      "distractors": [
        {
          "text": "It mandates specific security controls that must be implemented.",
          "misconception": "Targets [regulatory vs. guidance confusion]: NIST reports offer voluntary guidance, not mandatory controls."
        },
        {
          "text": "It offers a complete toolkit for automatically securing all AI systems.",
          "misconception": "Targets [overly simplistic solution]: No single toolkit can automatically secure all AI systems against evolving threats."
        },
        {
          "text": "It focuses solely on the ethical implications of AI, not technical security.",
          "misconception": "Targets [scope confusion]: The report explicitly covers technical attacks and mitigations."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI 100-2 report serves as a crucial resource by defining AML concepts and categorizing attacks, thereby equipping organizations with the knowledge needed to identify potential risks and formulate effective security strategies for their AI systems.",
        "distractor_analysis": "The distractors misrepresent the report's nature as a mandate, an automated solution, or solely focused on ethics, rather than its actual purpose as a foundational guide for understanding and addressing AML threats.",
        "analogy": "The NIST report is like a comprehensive threat assessment guide for a new technology; it helps you understand the dangers and plan your defenses, rather than providing a ready-made security system."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "THREAT_INTEL_BASICS"
      ]
    },
    {
      "question_text": "What is the primary difference between 'Generative AI (GenAI)' and 'Predictive AI (PredAI)' systems in the context of adversarial attacks, as discussed in NIST's AI 100-2 report?",
      "correct_answer": "GenAI systems are susceptible to 'abuse attacks' where their generative capabilities are repurposed, a category not typically applied to PredAI.",
      "distractors": [
        {
          "text": "PredAI systems are only vulnerable to evasion attacks, while GenAI is vulnerable to all attack types.",
          "misconception": "Targets [attack applicability confusion]: Both can be vulnerable to evasion and poisoning; abuse is specific to GenAI."
        },
        {
          "text": "GenAI systems cannot be subjected to poisoning attacks, only PredAI can.",
          "misconception": "Targets [attack applicability confusion]: Both types of AI can be vulnerable to poisoning."
        },
        {
          "text": "Privacy attacks are exclusive to PredAI systems, not GenAI.",
          "misconception": "Targets [attack applicability confusion]: Privacy attacks can affect both types of AI systems."
        }
      ],
      "detailed_explanation": {
        "core_logic": "While both PredAI and GenAI can be vulnerable to evasion, poisoning, and privacy attacks, GenAI introduces a unique attack vector: 'abuse attacks,' where the system's ability to generate content is maliciously exploited.",
        "distractor_analysis": "The distractors incorrectly claim exclusivity of certain attacks to one AI type or the other, overlooking that many AML categories apply broadly, while abuse attacks are a distinct threat primarily associated with GenAI.",
        "analogy": "Predictive AI is like a calculator that gives wrong answers if tampered with (evasion/poisoning). Generative AI is like a writer who can be forced to write harmful content (abuse), in addition to being susceptible to similar tampering."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "GENAI_BASICS",
        "PREDICTIVE_AI_BASICS"
      ]
    },
    {
      "question_text": "When building an AI/ML system threat intelligence program, which practice, aligned with NIST guidance, is crucial for effective defense?",
      "correct_answer": "Continuously monitoring and updating threat intelligence to adapt to evolving AML techniques.",
      "distractors": [
        {
          "text": "Implementing a single, robust defense mechanism at system deployment.",
          "misconception": "Targets [static defense fallacy]: Security requires continuous adaptation, not a one-time implementation."
        },
        {
          "text": "Focusing solely on known, historical attack patterns.",
          "misconception": "Targets [unknown]: Not specified"
        },
        {
          "text": "Sharing threat intelligence only internally within the organization.",
          "misconception": "Targets [information sharing limitations]: Collaboration and sharing (where appropriate) enhance collective defense."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The rapidly evolving nature of adversarial machine learning, as highlighted by NIST, necessitates a dynamic approach where threat intelligence is continuously gathered, analyzed, and used to update defenses, ensuring ongoing protection.",
        "distractor_analysis": "The distractors propose static defenses, a reactive approach to historical threats only, and limited information sharing, all of which are contrary to best practices for managing dynamic AI/ML threats.",
        "analogy": "An effective AI threat intelligence program is like staying updated on weather patterns to prepare for storms; you can't just prepare for yesterday's weather; you need to anticipate what's coming."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "procedure",
      "bloom_level": "apply",
      "prerequisites": [
        "THREAT_INTEL_BASICS",
        "AML_BASICS",
        "AI_SECURITY_PROGRAMS"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 17,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "AI/ML System Threat Intelligence Threat Intelligence And Hunting best practices",
    "latency_ms": 42658.507
  },
  "timestamp": "2026-01-04T03:25:28.943243"
}