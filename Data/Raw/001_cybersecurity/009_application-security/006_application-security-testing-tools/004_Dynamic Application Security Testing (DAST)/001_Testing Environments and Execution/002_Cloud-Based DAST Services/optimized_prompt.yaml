version: '2.0'
metadata:
  topic_title: Cloud-Based DAST Services
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: 008_Application Security
    level_3_subdomain: 008_006_Application Security Testing Tools
    level_4_entry_domain: Dynamic 008_006_Application Security Testing (DAST)
    level_5_entry_subdomain: Testing Environments and Execution
    level_6_topic: Cloud-Based DAST Services
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 009_application-security
    subdomain: 006_application-security-testing-tools
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.84
    total_voters: 7
  generation_timestamp: '2026-01-18T12:44:21.074707'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
active_learning:
  discussion_prompt: Compare and contrast cloud-based DAST services with on-premises DAST tools in a debate format. Argue
    for scenarios where one is preferable over the other, considering factors like scalability, cost, compliance (e.g., OWASP
    Top 10 mitigation), and integration with CI/CD pipelines. Support arguments with real-world examples such as Veracode
    or Checkmarx.
  peer_teaching: 'Think-Pair-Share: Pair up with a partner. One student explains core DAST concepts (black-box testing, vulnerability
    detection like XSS/SQLi) using simple analogies (e.g., ''like a burglar testing locks without seeing the blueprint''),
    then switches to explain cloud-based advantages (scalability, no infrastructure setup). Partners teach back and correct
    misconceptions, such as confusing DAST (dynamic, runtime) with SAST (static, source code).'
  problem_solving: 'Real-world scenario: Your team deploys a web application to AWS. Design a testing plan using a cloud-based
    DAST service (e.g., Synopsys or Veracode) to simulate OWASP Top 10 attacks. Identify 3 potential vulnerabilities, outline
    execution steps in cloud environments, and propose mitigation strategies. Groups present findings and discuss integration
    challenges.'
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 distractors per MCQ: 1) Common misconception (e.g., ''DAST requires source code'' â€“ confuses
    with SAST); 2) Partial truth (e.g., ''Scalable but expensive upfront''); 3) Extreme/opposite (e.g., ''No integration with
    CI/CD''). Ensure plausible for cybersecurity learners.'
system_prompt: 'You are an expert flashcard generator for cybersecurity education, specializing in Application Security (Domain:
  008_Application Security Testing Tools, Subdomain: 008_006_DAST, Focus: Cloud-Based DAST Services in Testing Environments
  and Execution).


  Topic Hierarchy: Cybersecurity > 008_Application Security > 008_006_Application Security Testing Tools > Dynamic 008_006_Application
  Security Testing (DAST) > Testing Environments and Execution > Cloud-Based DAST Services.


  Voter Consensus (84.3% approval): Emphasize completeness (examples: Veracode, Synopsys, Checkmarx; NIST SP 800-115), pedagogy
  (Bloom''s progression), scaffolding (4 layers), active learning, OWASP Top 10.


  Use this structure to generate 25 high-quality flashcards:


  **Learning Objectives:** [Insert the learning_objectives array here]


  **Active Learning Context:** Incorporate elements from discussion, peer teaching, problem-solving to inspire scenario questions.


  **Scaffolding Coverage:** Distribute flashcards: 6 L1 Foundation, 7 L2 Components, 6 L3 Implementation, 6 L4 Integration.
  Tag each: [BLOOM-LAYER].


  **Flashcard Schema:** Strictly follow [front/back structure]. Use distractor protocol for MCQs. Explanations must reference
  standards (OWASP DevSecOps, NIST SP 800-115) and build on black-box testing prior knowledge.


  Output ONLY a JSON array of flashcards: [{''front'': ''...'', ''back'': {''answer'': ''...'', ''explanation'': ''...'',
  ''distractors'': [''opt1'', ''opt2'', ''opt3''], ''references'': [''OWASP'']}, ''bloom_layer'': ''REMEMBER-L1''}]. Ensure
  active recall, no hints on front, optimized for spaced repetition.'
