version: '2.0'
metadata:
  topic_title: Automated Web Application Scanners
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Penetration Testing And Ethical Hacking
    level_3_subdomain: Penetration Testing Tools
    level_4_entry_domain: 003_Vulnerability Assessment Tools
    level_5_entry_subdomain: Web Application Vulnerability Scanners
    level_6_topic: Automated Web Application Scanners
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 016_penetration-testing-and-ethical-hacking
    subdomain: 005_penetration-testing-tools
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 1.0
    total_voters: 7
  generation_timestamp: '2026-01-18T15:15:25.022809'
learning_objectives:
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
active_learning:
  discussion_prompt: To what extent can automated web scanners replace manual penetration testing? Debate using OWASP WSTG
    categories (e.g., Error Handling, Business Logic, Client-Side, API Testing), citing real-world examples like the Equifax
    breach where automation might have helped or failed. Consider false positives and coverage gaps.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 plausible distractors per MCQ:

    - Based on common misconceptions (e.g., confuse XSS payloads with SQLi; mix DAST/SAST; overlook OWASP phases like ERRH).

    - Use partial truths (e.g., correct tool but wrong phase).

    - Draw from voter/research context (false positives, tool limits, truncated phases like Business Logic).

    - Ensure distractors are realistic for cybersecurity learners.'
system_prompt: "You are an expert flashcard generator for cybersecurity education, specializing in Penetration Testing Tools.\
  \ Generate 40-60 high-quality, spaced-repetition optimized flashcards on 'Automated Web Application Scanners' (Topic Hierarchy:\
  \ Cybersecurity > Penetration Testing And Ethical Hacking > Penetration Testing Tools > 003_Vulnerability Assessment Tools\
  \ > Web Application Vulnerability Scanners > Automated Web Application Scanners).\n\nIncorporate:\n- Learning Objectives:\
  \ [PASTE THE FULL LIST HERE].\n- Active Learning: Design flashcards to support discussion (e.g., pros/cons debates), peer\
  \ teaching (payload comparisons), problem-solving (scan interpretation).\n- Scaffolding: Distribute evenly across 4 layers\
  \ [PASTE FULL SCAFFOLDING HERE]. Ensure progression from foundation to integration, completing OWASP phases (e.g., WSTG-ERRH:\
  \ error messages/stack traces; add Business Logic, Client-Side, API).\n- Tools: OWASP ZAP, Burp Suite Scanner, Nikto, Acunetix;\
  \ apps: DVWA, Juice Shop.\n- Research: DAST definition, OWASP WSTG alignment, vulns (SQLi, XSS), limitations (false positives,\
  \ logic flaws).\n\nUse EXACT Flashcard Schema: [PASTE FULL SCHEMA HERE].\nOutput ONLY a JSON array of flashcards, e.g.,\
  \ [\n  {\n    \"front\": \"What is DAST?\",\n    \"back\": \"Dynamic Application Security Testing...\",\n    \"options\"\
  : [\"A. Runtime testing\", \"B. Static code analysis\", ...],\n    \"correct_index\": 0,\n    \"explanation\": \"...\",\n\
  \    \"bloom_level\": \"REMEMBER\",\n    \"scaffolding_layer\": 1,\n    \"tags\": [\"DAST\"]\n  }\n].\nVary types: 60% MCQ,\
  \ 20% cloze/short answer, 20% scenario-based. Cover voter priorities: completeness (full OWASP), pedagogy (Bloom's progression).\
  \ Ensure distractors are educationally valuable."
