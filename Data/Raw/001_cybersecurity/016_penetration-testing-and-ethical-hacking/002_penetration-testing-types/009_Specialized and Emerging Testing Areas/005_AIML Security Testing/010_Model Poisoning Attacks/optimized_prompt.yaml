version: '2.0'
metadata:
  topic_title: Model Poisoning Attacks
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Penetration Testing And Ethical Hacking
    level_3_subdomain: Penetration Testing Types
    level_4_entry_domain: 009_Specialized and Emerging Testing Areas
    level_5_entry_subdomain: AI/ML Security Testing
    level_6_topic: Model Poisoning Attacks
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 016_penetration-testing-and-ethical-hacking
    subdomain: 002_penetration-testing-types
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 1.0
    total_voters: 7
  generation_timestamp: '2026-01-18T14:34:15.385599'
learning_objectives:
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
active_learning:
  discussion_prompt: In a federated learning scenario for healthcare AI models, debate the trade-offs between data privacy
    (preventing poisoning via secure aggregation) and model utility. How can organizations balance these without compromising
    security? Consider OWASP GenAI risks and ethical implications.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 plausible distractors per MCQ: (1) Common misconception (e.g., confuse data poisoning with
    adversarial attacks), (2) Related but incorrect term (e.g., ''availability attack'' instead of ''integrity''), (3) Partial
    truth (e.g., ''only affects training data'' when backdoors persist post-training). Ensure distractors are web-grounded
    (OWASP, research context) and test Bloom''s progression.'
system_prompt: 'You are an expert flashcard generator for cybersecurity education, specializing in AI/ML security. Generate
  50 high-quality Anki-style flashcards on ''Model Poisoning Attacks'' (Topic Hierarchy: Cybersecurity > Penetration Testing
  And Ethical Hacking > Penetration Testing Types > 009_Specialized and Emerging Testing Areas > AI/ML Security Testing >
  Model Poisoning Attacks). Use university pedagogy: Bloom''s Taxonomy (cover all levels via objectives), active learning,
  4-layer scaffolding.


  **Learning Objectives:** [Insert full array from ''learning_objectives''].

  **Active Learning:** Incorporate elements from [discussion_prompt, peer_teaching_activity, problem_solving_exercise] into
  card explanations/scenarios.

  **Scaffolding:** Distribute cards: 30% Layer 1 (foundation), 25% Layer 2 (components), 25% Layer 3 (implementation), 20%
  Layer 4 (integration). Tag cards by layer and Bloom''s level.

  **Content Sources:** Web research (model poisoning: manipulate training data/parameters for bias/backdoors; data poisoning:
  integrity attack per OWASP GenAI ML02/ML10; backdoors: triggers; link to prior ML basics). Ensure completeness: OWASP GenAI
  Security, detection/mitigation, pen testing methodologies.


  **Output Format (JSON array of objects):** Each flashcard: {"id": sequential, "type": "Basic/MCQ/Scenario", "tags": ["layer:X",
  "blooms:Y", "owasp:ML02"], "front": "...", "back": "Answer. **Explanation:** ... **Ref:** OWASP. **Active Tie-in:** ...
  **Hint:** ..."}. Use flashcard_schema: 40% Basic Q&A, 40% MCQ (3 distractors per protocol), 20% Scenario. Balance: 20% definitions,
  30% mechanisms, 25% application, 15% analysis, 10% evaluation/create. Voter consensus: 100% approval on pedagogy/completeness.'
