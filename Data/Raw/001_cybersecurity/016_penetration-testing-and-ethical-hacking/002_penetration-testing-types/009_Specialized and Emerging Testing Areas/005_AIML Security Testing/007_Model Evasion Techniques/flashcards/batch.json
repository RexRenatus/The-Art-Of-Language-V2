{
  "topic_title": "Model Evasion Techniques",
  "category": "Penetration Testing And Ethical Hacking - Penetration Testing Types",
  "flashcards": [
    {
      "question_text": "In the context of Adversarial Machine Learning (AML), what is the primary goal of an evasion attack during the deployment phase of an AI model?",
      "correct_answer": "To cause the model to make incorrect predictions on new, unseen data by subtly manipulating inputs.",
      "distractors": [
        {
          "text": "To corrupt the training dataset to degrade model performance over time.",
          "misconception": "Targets [attack phase confusion]: Confuses evasion attacks (deployment) with poisoning attacks (training)."
        },
        {
          "text": "To extract sensitive information about the model's architecture or training data.",
          "misconception": "Targets [attack objective confusion]: Confuses evasion attacks with model extraction or privacy attacks."
        },
        {
          "text": "To cause the model to reveal its internal decision-making process.",
          "misconception": "Targets [attack outcome confusion]: Confuses evasion attacks with model interpretability or explainability attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks aim to fool a deployed model into misclassifying inputs, because the attacker manipulates data subtly to bypass detection or classification mechanisms. This functions by exploiting model vulnerabilities learned during training, impacting the model's real-time performance.",
        "distractor_analysis": "The distractors incorrectly associate evasion attacks with training-time corruption (poisoning), data extraction (privacy/extraction attacks), or revealing internal workings, rather than the core goal of causing misclassification during deployment.",
        "analogy": "An evasion attack is like a skilled pickpocket subtly altering their appearance or movements to avoid security cameras and blend into a crowd, successfully bypassing detection."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "attack",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "ML_DEPLOYMENT"
      ]
    },
    {
      "question_text": "Which technique is commonly used by attackers to craft adversarial examples for evasion attacks, aiming to find small perturbations that cause misclassification?",
      "correct_answer": "Gradient-based optimization methods, such as the Fast Gradient Sign Method (FGSM).",
      "distractors": [
        {
          "text": "Differential privacy mechanisms to obscure training data.",
          "misconception": "Targets [defense vs. attack confusion]: Confuses a privacy-preserving technique with an attack generation method."
        },
        {
          "text": "Ensemble methods to combine predictions from multiple models.",
          "misconception": "Targets [technique purpose confusion]: Ensembles are typically for improving robustness or performance, not direct attack generation."
        },
        {
          "text": "Data augmentation to increase the size and diversity of the training set.",
          "misconception": "Targets [technique purpose confusion]: Data augmentation is a training technique to improve generalization, not an attack generation method."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Gradient-based methods like FGSM leverage the model's gradients to find the direction of maximum loss increase, thus identifying minimal perturbations that cause misclassification. This functions by calculating how changes in input features affect the model's output, enabling targeted evasion.",
        "distractor_analysis": "The distractors describe techniques related to privacy, model improvement, or data preparation, none of which are primary methods for generating adversarial examples for evasion attacks.",
        "analogy": "Imagine trying to find the smallest nudge to a perfectly balanced tower of blocks that will make it topple. Gradient-based methods are like systematically testing those nudges in the most 'sensitive' directions."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "attack",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "GRADIENTS_IN_ML"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2, what is a key characteristic of evasion attacks in Adversarial Machine Learning (AML)?",
      "correct_answer": "They are typically performed during the inference or deployment phase of an AI model.",
      "distractors": [
        {
          "text": "They primarily target the integrity of the training data.",
          "misconception": "Targets [attack phase confusion]: Misattributes training-time data integrity attacks (poisoning) to evasion."
        },
        {
          "text": "They aim to extract the model's parameters or architecture.",
          "misconception": "Targets [attack objective confusion]: Confuses evasion with model extraction or inversion attacks."
        },
        {
          "text": "They require direct access to the model's training environment.",
          "misconception": "Targets [access requirement confusion]: Evasion attacks often only require query access to the deployed model."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks are designed to fool a model that is already deployed and making predictions, because the attacker manipulates input data at inference time. This functions by exploiting the model's learned decision boundaries without altering the model itself.",
        "distractor_analysis": "The distractors incorrectly place evasion attacks during training, confuse their objective with model extraction, or impose an unnecessary requirement for direct access to the training environment.",
        "analogy": "An evasion attack is like a driver subtly changing their driving style to avoid a speed camera that's already active, rather than tampering with the camera's calibration."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "NIST_AI_100_2"
      ]
    },
    {
      "question_text": "What is the main difference between an evasion attack and a poisoning attack in the context of Machine Learning security?",
      "correct_answer": "Evasion attacks occur during model deployment to cause misclassification, while poisoning attacks occur during training to corrupt the model.",
      "distractors": [
        {
          "text": "Evasion attacks modify the model's weights, while poisoning attacks alter input data.",
          "misconception": "Targets [mechanism confusion]: Reverses the typical mechanisms; evasion manipulates input, poisoning manipulates training data/labels."
        },
        {
          "text": "Evasion attacks are only effective against generative models, while poisoning attacks affect predictive models.",
          "misconception": "Targets [model type confusion]: Both attack types can affect various ML model types (predictive and generative)."
        },
        {
          "text": "Evasion attacks aim to improve model accuracy, while poisoning attacks aim to decrease it.",
          "misconception": "Targets [objective confusion]: Both attack types aim to degrade model performance or cause specific incorrect outputs."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The fundamental difference lies in the attack phase: evasion targets the deployed model's inference phase by manipulating inputs, whereas poisoning targets the model's training phase by corrupting data or labels. This functions by exploiting different vulnerabilities – model robustness for evasion, and data integrity for poisoning.",
        "distractor_analysis": "The distractors incorrectly swap attack mechanisms, wrongly restrict attack types to specific model categories, or misrepresent the attackers' objectives.",
        "analogy": "A poisoning attack is like secretly adding a bad ingredient to a recipe while it's being cooked (training), ruining the final dish. An evasion attack is like subtly changing the presentation of a finished dish to make someone think it's something it's not (deployment)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "ML_TRAINING_VS_DEPLOYMENT"
      ]
    },
    {
      "question_text": "Consider a scenario where an autonomous vehicle's object detection model is targeted. An attacker subtly alters a stop sign's appearance with minimal visual changes, causing the model to classify it as a speed limit sign. What type of adversarial attack is this?",
      "correct_answer": "Evasion Attack",
      "distractors": [
        {
          "text": "Data Poisoning Attack",
          "misconception": "Targets [attack phase confusion]: Assumes the attack happened during training, not at inference time on a deployed model."
        },
        {
          "text": "Model Inversion Attack",
          "misconception": "Targets [attack objective confusion]: Confuses evasion with attacks aimed at reconstructing training data."
        },
        {
          "text": "Membership Inference Attack",
          "misconception": "Targets [attack objective confusion]: Confuses evasion with attacks aimed at determining if specific data was in the training set."
        }
      ],
      "detailed_explanation": {
        "core_logic": "This is an evasion attack because the attacker manipulates input data (the stop sign image) during the model's operational phase (inference) to cause a specific misclassification. This functions by exploiting the model's learned patterns and decision boundaries.",
        "distractor_analysis": "The distractors describe attacks that occur during training (poisoning) or have different objectives like data reconstruction (inversion) or data membership inference.",
        "analogy": "It's like a magician subtly changing a playing card's appearance just enough for the audience to misread it, without altering the deck itself."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "ML_OBJECT_DETECTION"
      ]
    },
    {
      "question_text": "What is the primary challenge in defending against evasion attacks, as highlighted by the evolving landscape of Adversarial Machine Learning?",
      "correct_answer": "The continuous arms race between attackers developing new evasion methods and defenders creating countermeasures.",
      "distractors": [
        {
          "text": "The difficulty in obtaining sufficient labeled data for training robust models.",
          "misconception": "Targets [defense challenge confusion]: This is a general ML challenge, not specific to evasion defense."
        },
        {
          "text": "The high computational cost of implementing advanced encryption for model inputs.",
          "misconception": "Targets [defense mechanism confusion]: Encryption is not a primary defense against evasion; other methods like adversarial training are used."
        },
        {
          "text": "The lack of standardized benchmarks for evaluating adversarial robustness.",
          "misconception": "Targets [evaluation challenge confusion]: While benchmarks are evolving, the core defense challenge is the dynamic nature of attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Defending against evasion attacks is challenging because attackers constantly innovate new perturbation techniques, requiring continuous updates to defense strategies. This functions as an 'arms race' where defenses must adapt to novel evasion methods.",
        "distractor_analysis": "The distractors point to general ML data issues, misapply encryption as a direct evasion defense, or focus on evaluation challenges rather than the core dynamic nature of the attack-defense cycle.",
        "analogy": "It's like trying to build a stronger lock while the burglars are simultaneously inventing new lock-picking tools – the defense must constantly evolve."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "evaluate",
      "prerequisites": [
        "AML_BASICS",
        "AML_DEFENSE_STRATEGIES"
      ]
    },
    {
      "question_text": "Which of the following is a common defense strategy against evasion attacks that involves training the model on adversarial examples?",
      "correct_answer": "Adversarial Training",
      "distractors": [
        {
          "text": "Data Sanitization",
          "misconception": "Targets [defense strategy confusion]: Data sanitization typically removes outliers or noise, not specifically crafted adversarial inputs."
        },
        {
          "text": "Feature Engineering",
          "misconception": "Targets [defense strategy confusion]: Feature engineering focuses on creating better input features, not directly defending against adversarial perturbations."
        },
        {
          "text": "Regularization Techniques",
          "misconception": "Targets [defense strategy confusion]: Regularization helps prevent overfitting but doesn't directly train the model to resist adversarial perturbations."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Adversarial training explicitly exposes the model to adversarial examples during the training process, forcing it to learn more robust features and decision boundaries. This functions by making the model more resilient to the types of subtle input manipulations used in evasion attacks.",
        "distractor_analysis": "The distractors describe techniques that address general model robustness or data quality but do not specifically involve training the model with adversarial examples to counter evasion.",
        "analogy": "It's like training a boxer by having them spar with opponents who use unusual or tricky fighting styles, so they are better prepared for unexpected moves in a real match."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "defense",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "ADVERSARIAL_TRAINING"
      ]
    },
    {
      "question_text": "What is the purpose of 'gradient masking' in the context of adversarial attacks?",
      "correct_answer": "To make it difficult for attackers to compute the gradients needed to generate effective adversarial examples.",
      "distractors": [
        {
          "text": "To increase the accuracy of the machine learning model.",
          "misconception": "Targets [objective confusion]: Gradient masking is an attacker technique to hinder defense, not improve model accuracy."
        },
        {
          "text": "To ensure the model's predictions are always consistent.",
          "misconception": "Targets [outcome confusion]: Gradient masking is about obscuring gradients, not ensuring prediction consistency."
        },
        {
          "text": "To reduce the computational cost of model training.",
          "misconception": "Targets [mechanism confusion]: Gradient masking is related to attack generation/defense, not training efficiency."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Gradient masking is a technique used by some defenses or by attackers to obscure the model's gradients, making it harder for gradient-based attacks to find effective perturbations. This functions by creating a non-differentiable or piecewise constant gradient landscape.",
        "distractor_analysis": "The distractors misrepresent the purpose of gradient masking, associating it with model accuracy improvement, prediction consistency, or training efficiency, rather than its role in hindering gradient-based adversarial attacks.",
        "analogy": "It's like an attacker trying to find a hidden treasure by following a map (gradients), but someone has smudged or erased crucial parts of the map (gradient masking), making the search much harder."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "defense",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "GRADIENTS_IN_ML"
      ]
    },
    {
      "question_text": "Which type of adversarial attack specifically targets Generative AI (GenAI) systems by manipulating prompts or inputs to produce undesirable or harmful outputs?",
      "correct_answer": "Evasion Attacks (or Prompt Injection/Manipulation)",
      "distractors": [
        {
          "text": "Data Poisoning Attacks",
          "misconception": "Targets [attack phase confusion]: Primarily targets training data, not direct prompt manipulation during generation."
        },
        {
          "text": "Model Extraction Attacks",
          "misconception": "Targets [attack objective confusion]: Aims to steal the model, not to influence its output generation."
        },
        {
          "text": "Privacy Attacks",
          "misconception": "Targets [attack objective confusion]: Focuses on extracting sensitive information from the model or its training data."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks, often manifesting as prompt injection or manipulation in GenAI, aim to bypass safety filters or guide the model to generate harmful content by crafting specific inputs. This functions by exploiting the model's understanding of prompts and its learned response patterns.",
        "distractor_analysis": "The distractors describe attacks that target training data integrity (poisoning), model theft (extraction), or data privacy, rather than the direct manipulation of generative model outputs via input prompts.",
        "analogy": "It's like tricking a storyteller into weaving a dark or inappropriate subplot into their tale by subtly guiding their narrative choices with leading questions."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "attack",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "GENAI_BASICS"
      ]
    },
    {
      "question_text": "What is the concept of 'transferability' in the context of adversarial evasion attacks?",
      "correct_answer": "Adversarial examples crafted for one model can often fool other models, even with different architectures or training data.",
      "distractors": [
        {
          "text": "The ability of a defense mechanism to transfer robustness across different attack types.",
          "misconception": "Targets [concept scope confusion]: Transferability refers to attack effectiveness across models, not defense effectiveness across attacks."
        },
        {
          "text": "The process of transferring a trained model to a new domain or task.",
          "misconception": "Targets [concept scope confusion]: This describes transfer learning, not adversarial attack transferability."
        },
        {
          "text": "The ease with which an attacker can transfer their malicious code to a target system.",
          "misconception": "Targets [concept scope confusion]: This relates to malware propagation, not adversarial ML evasion."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Transferability means that adversarial examples generated against one ML model are often effective against other models, even if they are different. This functions because many models learn similar underlying features or decision boundaries, making them vulnerable to the same perturbations.",
        "distractor_analysis": "The distractors confuse transferability with defense robustness, transfer learning, or general malware propagation, misrepresenting its specific meaning in adversarial ML.",
        "analogy": "It's like a master key that can open not just one specific lock, but also many similar locks made by the same manufacturer, even if they look slightly different."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "ML_MODEL_DIFFERENCES"
      ]
    },
    {
      "question_text": "How does the NIST AI Risk Management Framework (AI RMF) address adversarial attacks like evasion?",
      "correct_answer": "By emphasizing risk assessment and management throughout the AI lifecycle, including identifying and mitigating potential adversarial threats.",
      "distractors": [
        {
          "text": "By mandating specific cryptographic algorithms for all AI model inputs.",
          "misconception": "Targets [solution confusion]: AI RMF focuses on risk management, not mandating specific technical controls like crypto for all inputs."
        },
        {
          "text": "By providing a definitive list of all known adversarial attack vectors.",
          "misconception": "Targets [scope confusion]: AI RMF provides a framework for managing risks, not an exhaustive catalog of all attacks."
        },
        {
          "text": "By requiring all AI models to achieve perfect robustness against all known attacks.",
          "misconception": "Targets [feasibility confusion]: Perfect robustness is often unattainable; AI RMF focuses on managing risks to acceptable levels."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI RMF promotes a risk-based approach, encouraging organizations to map, measure, and manage AI risks, including those from adversarial attacks. This functions by integrating security considerations into the AI system's design, development, and deployment phases.",
        "distractor_analysis": "The distractors propose overly specific technical mandates, an unrealistic scope of attack enumeration, or an unattainable goal of perfect robustness, rather than the framework's focus on systematic risk management.",
        "analogy": "The NIST AI RMF is like a safety manual for building and operating complex machinery; it guides you on identifying potential hazards (like adversarial attacks) and implementing controls to manage the risks, rather than dictating every single bolt and wire."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "defense",
      "bloom_level": "apply",
      "prerequisites": [
        "AML_BASICS",
        "NIST_AI_RMF"
      ]
    },
    {
      "question_text": "What is the primary goal of 'red teaming' an AI system in relation to adversarial attacks?",
      "correct_answer": "To proactively simulate adversarial behaviors and identify vulnerabilities before malicious actors exploit them.",
      "distractors": [
        {
          "text": "To optimize the AI model's performance for production environments.",
          "misconception": "Targets [objective confusion]: Red teaming is for security testing, not performance optimization."
        },
        {
          "text": "To ensure compliance with data privacy regulations like GDPR.",
          "misconception": "Targets [scope confusion]: While privacy is a concern, red teaming's primary goal is security vulnerability discovery."
        },
        {
          "text": "To develop new machine learning algorithms.",
          "misconception": "Targets [scope confusion]: Red teaming tests existing systems, it doesn't typically involve algorithm development."
        }
      ],
      "detailed_explanation": {
        "core_logic": "AI red teaming simulates adversarial attacks to uncover weaknesses and vulnerabilities in the AI system's security posture. This functions by adopting an attacker's mindset to proactively test defenses and identify potential exploitation paths.",
        "distractor_analysis": "The distractors misrepresent the objective of red teaming, confusing it with performance tuning, regulatory compliance, or algorithm research, rather than its core function of security testing and vulnerability discovery.",
        "analogy": "It's like hiring a 'burglar' to try and break into your house to find all the weak points in your security system before real burglars do."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "RED_TEAMING_CONCEPTS"
      ]
    },
    {
      "question_text": "In the context of AI security testing, what is the difference between conventional security testing (pentesting) and AI security testing?",
      "correct_answer": "Conventional pentesting focuses on traditional software vulnerabilities, while AI security testing specifically targets vulnerabilities unique to AI/ML models and their data.",
      "distractors": [
        {
          "text": "Conventional pentesting is performed during development, while AI security testing is done post-deployment.",
          "misconception": "Targets [timing confusion]: Both types of testing can occur at various stages of the lifecycle."
        },
        {
          "text": "Conventional pentesting uses automated tools, while AI security testing relies solely on manual analysis.",
          "misconception": "Targets [methodology confusion]: Both approaches can utilize automated and manual techniques."
        },
        {
          "text": "Conventional pentesting targets the AI model itself, while AI security testing targets the underlying hardware.",
          "misconception": "Targets [scope confusion]: Conventional pentesting can target various system components, and AI security testing focuses on AI-specific vulnerabilities, not just hardware."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Conventional pentesting addresses standard software vulnerabilities (e.g., injection flaws, insecure configurations), whereas AI security testing focuses on ML-specific threats like adversarial attacks, data poisoning, and model extraction. This functions by applying different testing methodologies tailored to the unique attack surfaces of AI systems.",
        "distractor_analysis": "The distractors incorrectly differentiate based on timing, methodology, or target scope, rather than the fundamental difference in the types of vulnerabilities addressed.",
        "analogy": "Conventional pentesting is like checking the locks, windows, and alarm system of a house. AI security testing is like checking if the house's smart home system can be tricked into unlocking doors or disabling cameras."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_BASICS",
        "PENTESTING_BASICS"
      ]
    },
    {
      "question_text": "What is the primary risk associated with 'model drift' in deployed AI systems, and how does it relate to adversarial robustness?",
      "correct_answer": "Model drift can degrade performance over time, potentially making a previously robust model more susceptible to adversarial evasion attacks.",
      "distractors": [
        {
          "text": "Model drift directly causes data poisoning attacks to become more effective.",
          "misconception": "Targets [causality confusion]: Drift is a performance degradation; it doesn't directly enable poisoning, which targets training data."
        },
        {
          "text": "Model drift is a type of evasion attack where the model intentionally misclassifies data.",
          "misconception": "Targets [definition confusion]: Drift is a natural degradation; evasion is an active, malicious manipulation."
        },
        {
          "text": "Model drift increases the computational cost of adversarial training defenses.",
          "misconception": "Targets [consequence confusion]: Drift impacts model accuracy and robustness, not directly the computational cost of defenses."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Model drift occurs when the statistical properties of the data change over time, degrading model performance. This degradation can inadvertently lower the model's defenses against evasion attacks, as its learned patterns may no longer align with real-world data distributions. This functions by creating a wider gap between the model's assumptions and reality.",
        "distractor_analysis": "The distractors incorrectly link drift to enabling poisoning, defining drift as an evasion attack itself, or misattributing its consequences to the computational cost of defenses.",
        "analogy": "It's like a map that was accurate when drawn but becomes less useful over time as roads change and new buildings appear; navigating with the old map becomes harder and more error-prone."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "analysis",
      "bloom_level": "evaluate",
      "prerequisites": [
        "AML_BASICS",
        "MODEL_DRIFT"
      ]
    },
    {
      "question_text": "Which of the following best describes the concept of 'adversarial robustness' in machine learning?",
      "correct_answer": "The ability of a machine learning model to maintain its performance and accuracy even when subjected to adversarial perturbations in its input data.",
      "distractors": [
        {
          "text": "The model's resistance to data poisoning attacks during training.",
          "misconception": "Targets [scope confusion]: Robustness primarily refers to performance against input manipulation (evasion), though related to other defenses."
        },
        {
          "text": "The model's ability to generalize well to unseen, but normally distributed, data.",
          "misconception": "Targets [generalization confusion]: Generalization is about handling natural data variations, not malicious perturbations."
        },
        {
          "text": "The speed at which the model can process inputs during inference.",
          "misconception": "Targets [performance metric confusion]: Speed is a performance metric, but not directly related to adversarial robustness."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Adversarial robustness is the measure of how well an ML model withstands intentional, malicious modifications to its input data designed to cause errors. This functions by ensuring the model's decision boundaries are stable and not easily crossed by small, crafted perturbations.",
        "distractor_analysis": "The distractors confuse robustness with resistance to poisoning, generalization to normal data, or inference speed, rather than its specific meaning concerning adversarial input manipulation.",
        "analogy": "It's like a building designed to withstand earthquakes; it maintains its structural integrity and function even when subjected to strong, disruptive forces."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_BASICS"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 15,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "Model Evasion Techniques Penetration Testing And Ethical Hacking best practices",
    "latency_ms": 26038.554
  },
  "timestamp": "2026-01-18T14:34:50.500276",
  "_av_safe_encoded": true,
  "_encoding_note": "Educational content encoded with HTML entities to prevent antivirus false positives. Content renders normally in Anki."
}