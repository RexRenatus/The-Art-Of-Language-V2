<?xml version="1.0" encoding="UTF-8"?>
<topic_prompt version="2.0">
  <metadata>
    <topic_title>Adversarial attack prevention</topic_title>
    <hierarchy>
      <category>Cybersecurity</category>
      <domain>Security And Risk Management</domain>
      <subdomain>Security Control Frameworks</subdomain>
      <entry_domain>Specialized Control Domains</entry_domain>
      <entry_subdomain>AI/ML Security Controls</entry_subdomain>
    </hierarchy>
    <voting_summary>
      <consensus>False</consensus>
      <approval>41.4%</approval>
      <voters>7</voters>
    </voting_summary>
    <generation_timestamp>2026-01-01T12:23:22.697085</generation_timestamp>
  </metadata>
  <learning_objectives level="bloom_taxonomy">
    <objective level="remember" measurable="true" verbs="define">Define key terminology</objective>
    <objective level="understand" measurable="true" verbs="explain">Explain core concepts</objective>
    <objective level="apply" measurable="true" verbs="apply">Apply knowledge to scenarios</objective>
    <objective level="analyze" measurable="true" verbs="analyze">Analyze relationships</objective>
  </learning_objectives>
  <active_learning>
    <discussion_prompt>How would you apply this concept in your work environment?</discussion_prompt>
    <peer_teaching>In pairs, one student explains evasion attacks vs. poisoning attacks (including subtypes like data poisoning and model poisoning) and their corresponding defenses (e.g., adversarial training vs. data sanitization); switch roles and quiz each other on interconnections to ML lifecycle stages.</peer_teaching>
    <problem_solving>Using a simple ML model (e.g., MNIST image classifier), simulate an evasion attack with Fast Gradient Sign Method (FGSM), measure vulnerability, then implement and evaluate adversarial training as a defense. Extend to real-world application: Adapt for poisoning by injecting noisy labels and applying detection mechanisms.</problem_solving>
  </active_learning>
  <scaffolding>
    <layer level="1" name="Foundation">
      <focus>Basic terminology and definitions</focus>
      <content/>
    </layer>
    <layer level="2" name="Components">
      <focus>Framework components and structure</focus>
      <content/>
    </layer>
    <layer level="3" name="Implementation">
      <focus>Practical implementation steps</focus>
      <content/>
    </layer>
    <layer level="4" name="Integration">
      <focus>Advanced integration and optimization</focus>
      <content/>
    </layer>
  </scaffolding>
  <flashcard_generation>
    <output_schema>
      <field name="question" type="string"/>
      <field name="correct_answer" type="string"/>
      <field name="distractors" type="[{'text': 'string', 'explanation': 'string'}]"/>
      <field name="explanation" type="string"/>
      <field name="bloom_level" type="enum"/>
      <field name="topic_hierarchy" type="object"/>
    </output_schema>
    <distractor_protocol>
      <step number="1">Plausible misconceptions: e.g., Confuse evasion (inference) with poisoning (training).</step>
      <step number="2">Partial truths: e.g., 'Adversarial training only for evasion' (wrong: also helps poisoning).</step>
      <step number="3">Related but incorrect: e.g., Distractor 'Gradient descent' instead of 'Fast Gradient Sign Method'.</step>
      <step number="4">Extremes/unrelated: e.g., 'Complete model retraining from scratch' vs. targeted defenses.</step>
      <step number="5">Ensure 1 correct, 3 distractors per MCQ; balance difficulty.</step>
    </distractor_protocol>
    <system_prompt>You are an expert educational flashcard generator specializing in cybersecurity topics, particularly AI/ML security. Generate 25-35 high-quality, pedagogically optimized flashcards for the topic 'Adversarial Attack Prevention' in the hierarchy: Cybersecurity &gt; Security And Risk Management &gt; Security Control Frameworks &gt; Specialized Control Domains &gt; AI/ML Security Controls &gt; Adversarial Attack Prevention.
**Core Content (Research Context - Complete Edition):** Adversarial attacks exploit ML vulnerabilities across lifecycle. Key definitions: AML (attacks on ML stats nature), Adversarial Example (perturbed input fooling models), Lifecycle: Training (Poisoning: data/model tampering, e.g., label flipping/backdoors), Inference (Evasion: e.g., FGSM/PGD). Attacker Goals: Availability, Integrity, Privacy, Misuse. Prevention: Adversarial Training, Defensive Distillation, Input Preprocessing (denoising), Detection (anomaly stats), Data Sanitization (NIST AI 100-2). Frameworks: NIST AI RMF, DHS. Real-world: Stop-sign evasion in AVs.
**Pedagogy Integration:**
- Learning Objectives: [Insert full list above]. Ensure flashcards map to these (tag each with Bloom's level).
- Scaffolding: Distribute cards: 25% Layer 1, 25% Layer 2, 25% Layer 3, 25% Layer 4. [Insert full scaffolding content above].
- Active Learning: Each card back includes a 'Link' to discussion/peer teach/problem-solving. [Insert full active learning above].
- Concept Map: Flashcards should reinforce: Prevention → Attacks → Methods → Stages → Goals.
**Generation Rules:**
- Follow exact schema: Front (question), Back (answer | explanation | distractors | source | active_learning_link).
- Variety: Definitions, MCQs (4 opts w/ distractors per protocol), T/F, scenarios, comparisons.
- Distractors: Strictly follow protocol for misconceptions/partial truths.
- Explanations: University-level, cite NIST/DHS, link to objectives/layers.
- Output as JSON array: [{'front': '...', 'back': {'answer': '...', 'explanation': '...', 'distractors': ['a','b','c'], 'source': '...', 'active_learning_link': '...', 'bloom_level': '...', 'layer': '...'}}].
Generate now, ensuring comprehensive coverage, no repetition, and optimization for spaced repetition/active recall.</system_prompt>
  </flashcard_generation>
</topic_prompt>