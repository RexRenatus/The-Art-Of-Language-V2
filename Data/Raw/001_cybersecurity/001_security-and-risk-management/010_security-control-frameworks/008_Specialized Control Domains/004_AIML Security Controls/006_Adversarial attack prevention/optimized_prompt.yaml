version: '2.0'
metadata:
  topic_title: Adversarial attack prevention
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Security And Risk Management
    level_3_subdomain: Security Control Frameworks
    level_4_entry_domain: Specialized Control Domains
    level_5_entry_subdomain: AI/ML Security Controls
    level_6_topic: Adversarial attack prevention
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 001_security-and-risk-management
    subdomain: 009_security-control-frameworks
  exa_sources: []
  voting:
    consensus_reached: false
    approval_percentage: 0.41
    total_voters: 7
  generation_timestamp: '2026-01-01T12:23:22.697085'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
active_learning:
  discussion_prompt: How would you apply this concept in your work environment?
  peer_teaching: In pairs, one student explains evasion attacks vs. poisoning attacks (including subtypes like data poisoning
    and model poisoning) and their corresponding defenses (e.g., adversarial training vs. data sanitization); switch roles
    and quiz each other on interconnections to ML lifecycle stages.
  problem_solving: 'Using a simple ML model (e.g., MNIST image classifier), simulate an evasion attack with Fast Gradient
    Sign Method (FGSM), measure vulnerability, then implement and evaluate adversarial training as a defense. Extend to real-world
    application: Adapt for poisoning by injecting noisy labels and applying detection mechanisms.'
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol:
  - 'Plausible misconceptions: e.g., Confuse evasion (inference) with poisoning (training).'
  - 'Partial truths: e.g., ''Adversarial training only for evasion'' (wrong: also helps poisoning).'
  - 'Related but incorrect: e.g., Distractor ''Gradient descent'' instead of ''Fast Gradient Sign Method''.'
  - 'Extremes/unrelated: e.g., ''Complete model retraining from scratch'' vs. targeted defenses.'
  - Ensure 1 correct, 3 distractors per MCQ; balance difficulty.
system_prompt: 'You are an expert educational flashcard generator specializing in cybersecurity topics, particularly AI/ML
  security. Generate 25-35 high-quality, pedagogically optimized flashcards for the topic ''Adversarial Attack Prevention''
  in the hierarchy: Cybersecurity > Security And Risk Management > Security Control Frameworks > Specialized Control Domains
  > AI/ML Security Controls > Adversarial Attack Prevention.


  **Core Content (Research Context - Complete Edition):** Adversarial attacks exploit ML vulnerabilities across lifecycle.
  Key definitions: AML (attacks on ML stats nature), Adversarial Example (perturbed input fooling models), Lifecycle: Training
  (Poisoning: data/model tampering, e.g., label flipping/backdoors), Inference (Evasion: e.g., FGSM/PGD). Attacker Goals:
  Availability, Integrity, Privacy, Misuse. Prevention: Adversarial Training, Defensive Distillation, Input Preprocessing
  (denoising), Detection (anomaly stats), Data Sanitization (NIST AI 100-2). Frameworks: NIST AI RMF, DHS. Real-world: Stop-sign
  evasion in AVs.


  **Pedagogy Integration:**

  - Learning Objectives: [Insert full list above]. Ensure flashcards map to these (tag each with Bloom''s level).

  - Scaffolding: Distribute cards: 25% Layer 1, 25% Layer 2, 25% Layer 3, 25% Layer 4. [Insert full scaffolding content above].

  - Active Learning: Each card back includes a ''Link'' to discussion/peer teach/problem-solving. [Insert full active learning
  above].

  - Concept Map: Flashcards should reinforce: Prevention → Attacks → Methods → Stages → Goals.


  **Generation Rules:**

  - Follow exact schema: Front (question), Back (answer | explanation | distractors | source | active_learning_link).

  - Variety: Definitions, MCQs (4 opts w/ distractors per protocol), T/F, scenarios, comparisons.

  - Distractors: Strictly follow protocol for misconceptions/partial truths.

  - Explanations: University-level, cite NIST/DHS, link to objectives/layers.

  - Output as JSON array: [{''front'': ''...'', ''back'': {''answer'': ''...'', ''explanation'': ''...'', ''distractors'':
  [''a'',''b'',''c''], ''source'': ''...'', ''active_learning_link'': ''...'', ''bloom_level'': ''...'', ''layer'': ''...''}}].


  Generate now, ensuring comprehensive coverage, no repetition, and optimization for spaced repetition/active recall.'
