{
  "topic_title": "ML Library and Framework Security",
  "category": "Cybersecurity - Security And Risk Management - Supply Chain Risk Management (SCRM) - Specialized Supply Chain Areas - Artificial Intelligence and Machine Learning Supply Chain",
  "flashcards": [
    {
      "question_text": "According to NIST AI 100-2e2025, what is a primary characteristic that differentiates AI risks from traditional software risks, making them harder to manage?",
      "correct_answer": "AI systems' dependency on data that can change over time, affecting functionality and trustworthiness in unpredictable ways.",
      "distractors": [
        {
          "text": "The inherent complexity of traditional software architectures.",
          "misconception": "Targets [domain confusion]: Attributes AI risks to general software complexity rather than AI-specific data dynamics."
        },
        {
          "text": "The reliance on static, well-defined codebases for security.",
          "misconception": "Targets [misunderstanding of AI lifecycle]: Assumes AI models are static like traditional software, ignoring data drift."
        },
        {
          "text": "The limited scope of traditional software vulnerabilities.",
          "misconception": "Targets [underestimation of AI risks]: Downplays the unique and evolving nature of AI-specific risks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "AI systems are trained on data that can change, leading to unpredictable shifts in functionality and trustworthiness, unlike static traditional software. This data dependency is a core differentiator for AI risk management.",
        "distractor_analysis": "Distractors incorrectly attribute AI risks to general software issues or static codebases, failing to recognize the dynamic data dependency unique to AI.",
        "analogy": "Imagine traditional software as a house built on a solid foundation, while AI is like a structure built on shifting sands; the foundation (data) can change, impacting the entire structure's stability."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_RISK_FUNDAMENTALS",
        "TRADITIONAL_SOFTWARE_RISKS"
      ]
    },
    {
      "question_text": "The OWASP Machine Learning Security Top Ten project aims to identify and categorize the most significant security risks associated with machine learning systems. Which of the following is listed as a top risk?",
      "correct_answer": "Data Poisoning Attack",
      "distractors": [
        {
          "text": "Insecure API Endpoints",
          "misconception": "Targets [related but distinct risk]: While APIs are a security concern, this is not a top ML-specific risk from the OWASP list."
        },
        {
          "text": "Insufficient Input Validation",
          "misconception": "Targets [related but distinct risk]: This is a general software security risk, not a top ML-specific risk from the OWASP list."
        },
        {
          "text": "Lack of Encryption for Training Data",
          "misconception": "Targets [related but distinct risk]: Data encryption is important, but data poisoning is a more specific ML security threat."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The OWASP ML Security Top Ten explicitly lists 'Data Poisoning Attack' (ML02:2023) as a critical risk, where adversaries inject malicious data into training sets to compromise model integrity or availability.",
        "distractor_analysis": "Distractors represent common cybersecurity concerns but are not among the top 10 ML-specific risks identified by OWASP, which focuses on attacks targeting the ML lifecycle itself.",
        "analogy": "Think of data poisoning like a chef intentionally spoiling the ingredients for a dish; the final meal (model) will be compromised, no matter how good the recipe (algorithm) is."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "remember",
      "prerequisites": [
        "OWASP_MLSEC_TOP10"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is the primary goal of an 'Evasion Attack' in the context of adversarial machine learning?",
      "correct_answer": "To generate adversarial examples that cause a model to misclassify inputs, often with minimal, imperceptible perturbations.",
      "distractors": [
        {
          "text": "To corrupt the training data to degrade overall model performance.",
          "misconception": "Targets [attack type confusion]: Describes data poisoning, not evasion attacks."
        },
        {
          "text": "To extract sensitive information about the model's training data.",
          "misconception": "Targets [attack type confusion]: Describes privacy attacks, not evasion attacks."
        },
        {
          "text": "To steal the model's architecture and parameters.",
          "misconception": "Targets [attack type confusion]: Describes model extraction attacks, not evasion attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks aim to fool a trained model at deployment time by creating adversarial examples that are subtly altered to cause misclassification. This is achieved by finding minimal perturbations that change the model's output.",
        "distractor_analysis": "Each distractor describes a different category of adversarial attack (poisoning, privacy, model extraction), misrepresenting the specific objective of evasion attacks.",
        "analogy": "An evasion attack is like a chameleon changing its colors to blend in with the wrong background, tricking observers (the ML model) into misidentifying it."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_ATTACK_TYPES"
      ]
    },
    {
      "question_text": "NIST AI 100-2e2025 categorizes adversarial attacks based on attacker goals. Which goal is specific to Generative AI (GenAI) systems, beyond availability, integrity, and privacy?",
      "correct_answer": "Misuse enablement",
      "distractors": [
        {
          "text": "Data reconstruction",
          "misconception": "Targets [attack type confusion]: This is a privacy attack applicable to both predictive and generative AI."
        },
        {
          "text": "Model extraction",
          "misconception": "Targets [attack type confusion]: This is a privacy attack applicable to both predictive and generative AI."
        },
        {
          "text": "Evasion",
          "misconception": "Targets [attack type confusion]: This is primarily associated with predictive AI systems."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Misuse enablement is a GenAI-specific attacker goal, focusing on circumventing safety restrictions to generate harmful or undesirable outputs. This differs from privacy, integrity, or availability violations common to both AI types.",
        "distractor_analysis": "The distractors represent attack goals that are either common to predictive AI or are sub-categories of broader privacy/integrity violations, not the unique misuse enablement goal for GenAI.",
        "analogy": "Imagine a tool designed for crafting, but an attacker uses it to bypass safety guards and create dangerous items; misuse enablement is about bypassing those guards."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "GENAI_ATTACK_TAXONOMY"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is the primary challenge in mitigating adversarial attacks like evasion, which often rely on subtle input perturbations?",
      "correct_answer": "There is an inherent trade-off between model robustness and accuracy, and defenses can decrease overall performance.",
      "distractors": [
        {
          "text": "Adversarial attacks are too computationally expensive to replicate.",
          "misconception": "Targets [practicality misunderstanding]: Many attacks are computationally feasible, and the cost of defense is often higher."
        },
        {
          "text": "Defenses are only effective against white-box attacks, not black-box.",
          "misconception": "Targets [defense scope confusion]: Effective defenses aim to work against various threat models, and black-box attacks are a significant concern."
        },
        {
          "text": "The underlying ML algorithms are fundamentally insecure and cannot be defended.",
          "misconception": "Targets [overgeneralization]: While challenges exist, research is actively developing mitigations and theoretical guarantees."
        }
      ],
      "detailed_explanation": {
        "core_logic": "A fundamental challenge in ML security is the trade-off between robustness and accuracy; improving resistance to adversarial examples often degrades performance on clean data. This necessitates careful balancing based on use-case risk.",
        "distractor_analysis": "Distractors oversimplify the problem by claiming attacks are too expensive, defenses are limited to white-box scenarios, or that ML is inherently undefendable, ignoring the nuanced accuracy-robustness trade-off.",
        "analogy": "It's like trying to make a car bulletproof; while it might stop bullets, it could also become too heavy and slow to be practical for everyday driving."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_MITIGATION_CHALLENGES",
        "ACCURACY_ROBUSTNESS_TRADEOFF"
      ]
    },
    {
      "question_text": "The NIST AI Risk Management Framework (AI RMF) Core is operationalized through four functions. Which function is described as 'cross-cutting' and 'infused throughout AI risk management,' enabling the other functions?",
      "correct_answer": "Govern",
      "distractors": [
        {
          "text": "Map",
          "misconception": "Targets [functional role confusion]: Map establishes context but is not the overarching governance function."
        },
        {
          "text": "Measure",
          "misconception": "Targets [functional role confusion]: Measure assesses risks but is guided by governance."
        },
        {
          "text": "Manage",
          "misconception": "Targets [functional role confusion]: Manage implements risk treatments, directed by governance."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The GOVERN function in the AI RMF is designed to be cross-cutting, establishing the culture, policies, and accountability structures that guide and enable the MAP, MEASURE, and MANAGE functions throughout the AI lifecycle.",
        "distractor_analysis": "Each distractor represents another core function of the AI RMF but fails to capture the cross-cutting, foundational role of governance in directing and integrating risk management activities.",
        "analogy": "Think of 'Govern' as the steering wheel and navigation system of a car, guiding the journey (risk management) while 'Map,' 'Measure,' and 'Manage' are the road, the speedometer, and the braking system, respectively."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "In the context of adversarial machine learning, what is the primary difference between 'Data Poisoning' and 'Model Poisoning' attacks?",
      "correct_answer": "Data poisoning targets the training data itself, while model poisoning targets the model parameters or updates.",
      "distractors": [
        {
          "text": "Data poisoning affects availability, while model poisoning affects integrity.",
          "misconception": "Targets [goal confusion]: Both attack types can affect availability or integrity depending on the specific attack."
        },
        {
          "text": "Data poisoning requires white-box access, while model poisoning can be black-box.",
          "misconception": "Targets [access model confusion]: Both attack types can occur under various access models (white-box, black-box, gray-box)."
        },
        {
          "text": "Data poisoning is used for GenAI, while model poisoning is for predictive AI.",
          "misconception": "Targets [domain confusion]: Both attack types are applicable to both predictive and generative AI models."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Data poisoning attacks inject malicious data into the training set, corrupting the learning process, whereas model poisoning attacks directly manipulate the model's parameters or updates, often in federated learning or supply chain scenarios.",
        "distractor_analysis": "Distractors incorrectly conflate the attack mechanism with their goals (availability/integrity), required access levels, or specific AI model types, missing the core distinction in what is being manipulated.",
        "analogy": "Data poisoning is like sabotaging the ingredients before cooking, while model poisoning is like tampering with the recipe book itself after it's been written."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_ATTACK_TYPES"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes a 'Measure' function. What is the primary purpose of this function?",
      "correct_answer": "To employ quantitative, qualitative, or mixed-method tools to analyze, assess, benchmark, and monitor AI risk and related impacts.",
      "distractors": [
        {
          "text": "To establish the context and identify potential risks of an AI system.",
          "misconception": "Targets [functional role confusion]: This describes the 'Map' function."
        },
        {
          "text": "To implement risk treatments and allocate resources for identified risks.",
          "misconception": "Targets [functional role confusion]: This describes the 'Manage' function."
        },
        {
          "text": "To cultivate a culture of risk management and define organizational policies.",
          "misconception": "Targets [functional role confusion]: This describes the 'Govern' function."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Measure' function in the AI RMF is dedicated to the systematic assessment of AI risks and trustworthiness characteristics using various metrics and methodologies. This provides the data needed for informed risk management decisions.",
        "distractor_analysis": "Each distractor describes the purpose of a different AI RMF Core function (Map, Manage, Govern), misattributing the role of measurement and assessment.",
        "analogy": "If 'Govern' sets the rules, 'Map' identifies the terrain, and 'Manage' decides the route, then 'Measure' is like using a GPS and speedometer to track progress and identify hazards along the way."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is a key characteristic of 'Direct Prompting Attacks' in GenAI systems?",
      "correct_answer": "They are executed by the primary user of the system through query access, often by manipulating in-context instructions.",
      "distractors": [
        {
          "text": "They rely on manipulating external resources that the AI system accesses.",
          "misconception": "Targets [attack vector confusion]: This describes indirect prompt injection."
        },
        {
          "text": "They require control over the model's training data or parameters.",
          "misconception": "Targets [attack vector confusion]: This describes poisoning or model control attacks, not direct prompting."
        },
        {
          "text": "They are primarily used to steal the model's architecture.",
          "misconception": "Targets [attack goal confusion]: While prompt extraction can occur, the primary goal is often misuse enablement or integrity violation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Direct prompting attacks are initiated by the system's primary user via query access, often by injecting malicious instructions into the prompt context to override system prompts and induce unintended behavior.",
        "distractor_analysis": "Distractors incorrectly attribute the attack vector to external resource manipulation (indirect prompt injection), training data control (poisoning), or a specific goal (model extraction), mischaracterizing the direct user interaction.",
        "analogy": "It's like whispering a secret instruction to a performer during a play, directly influencing their actions on stage, rather than subtly altering the script backstage."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "GENAI_ATTACK_TYPES"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes a 'Map' function. What is its primary objective?",
      "correct_answer": "To establish the context for framing AI risks by understanding the system's intended purposes, use cases, and potential impacts.",
      "distractors": [
        {
          "text": "To quantify the likelihood and magnitude of identified risks.",
          "misconception": "Targets [functional role confusion]: This describes the 'Measure' function."
        },
        {
          "text": "To implement response plans for identified risks.",
          "misconception": "Targets [functional role confusion]: This describes the 'Manage' function."
        },
        {
          "text": "To define organizational policies for AI risk management.",
          "misconception": "Targets [functional role confusion]: This describes the 'Govern' function."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Map' function in the AI RMF is crucial for understanding the operational context, potential benefits, and harms of an AI system. It provides the foundational knowledge necessary to effectively identify and assess risks.",
        "distractor_analysis": "Each distractor describes the purpose of a different AI RMF Core function (Measure, Manage, Govern), misattributing the role of context establishment and risk framing.",
        "analogy": "If 'Govern' is the rulebook, 'Measure' is the scoreboard, and 'Manage' is the game strategy, then 'Map' is like studying the game board and understanding the players' positions before the game begins."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is a key challenge in achieving adversarial robustness in AI systems, particularly concerning trade-offs?",
      "correct_answer": "There is often a trade-off between adversarial robustness and other desirable AI characteristics like accuracy, fairness, and explainability.",
      "distractors": [
        {
          "text": "Adversarial robustness is easily achieved through standard software security practices.",
          "misconception": "Targets [misunderstanding of AI security]: AI robustness requires specialized techniques beyond standard software security."
        },
        {
          "text": "Adversarial attacks are only theoretical and have no real-world impact.",
          "misconception": "Targets [underestimation of threat]: NIST AI 100-2e2025 highlights real-world demonstrations of adversarial attacks."
        },
        {
          "text": "All AI systems are inherently robust due to their complex nature.",
          "misconception": "Targets [false assumption]: Complexity does not guarantee robustness; in fact, it can introduce new vulnerabilities."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Achieving high adversarial robustness in AI systems often comes at the cost of other critical attributes like accuracy, fairness, or explainability, creating complex trade-offs that must be managed based on the specific application's risk tolerance.",
        "distractor_analysis": "Distractors present misconceptions about the ease of achieving robustness, the reality of adversarial threats, or the inherent security of complex AI, failing to address the critical trade-off challenge.",
        "analogy": "It's like trying to make a car both extremely fast (accuracy) and incredibly safe with heavy armor (robustness); optimizing one often compromises the other."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_CHALLENGES",
        "TRUSTWORTHY_AI_ATTRIBUTES"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes a 'Manage' function. What is its primary role?",
      "correct_answer": "To allocate resources to prioritized risks and implement risk treatment plans, including response and recovery strategies.",
      "distractors": [
        {
          "text": "To identify and understand the context of AI risks.",
          "misconception": "Targets [functional role confusion]: This describes the 'Map' function."
        },
        {
          "text": "To measure and assess the severity of AI risks.",
          "misconception": "Targets [functional role confusion]: This describes the 'Measure' function."
        },
        {
          "text": "To establish overarching AI risk management policies.",
          "misconception": "Targets [functional role confusion]: This describes the 'Govern' function."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Manage' function in the AI RMF focuses on the practical implementation of risk mitigation strategies, including resource allocation, response planning, and recovery procedures, based on the risks identified and measured.",
        "distractor_analysis": "Each distractor describes the purpose of a different AI RMF Core function (Map, Measure, Govern), misattributing the role of risk treatment and resource allocation.",
        "analogy": "If 'Govern' sets the rules, 'Map' identifies the terrain, and 'Measure' tracks progress, then 'Manage' is about actively driving the vehicle, applying brakes, and navigating obstacles to reach the destination safely."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is a 'Backdoor Poisoning Attack'?",
      "correct_answer": "A poisoning attack that causes a model to misclassify samples containing a specific, attacker-defined pattern or trigger.",
      "distractors": [
        {
          "text": "An attack that degrades the model's performance indiscriminately on all inputs.",
          "misconception": "Targets [attack type confusion]: This describes an availability poisoning attack."
        },
        {
          "text": "An attack that modifies the model's parameters to cause specific misclassifications on targeted inputs.",
          "misconception": "Targets [attack type confusion]: This describes a targeted poisoning attack."
        },
        {
          "text": "An attack that extracts sensitive information about the training data.",
          "misconception": "Targets [attack type confusion]: This describes a privacy attack."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Backdoor poisoning attacks embed a hidden trigger within the model's learned behavior, causing it to misbehave only when that specific trigger is present in the input, while functioning normally otherwise.",
        "distractor_analysis": "Distractors describe other types of poisoning attacks (availability, targeted) or entirely different attack categories (privacy), failing to capture the specific trigger-based mechanism of backdoor attacks.",
        "analogy": "It's like a spy planting a hidden code word; when that word is spoken, the agent (model) performs a secret, malicious action, otherwise acting normally."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_ATTACK_TYPES"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes a 'Govern' function. Which of the following is a key outcome of this function?",
      "correct_answer": "Cultivating and implementing a culture of risk management within organizations designing, developing, deploying, evaluating, or acquiring AI systems.",
      "distractors": [
        {
          "text": "Measuring the accuracy and reliability of AI system outputs.",
          "misconception": "Targets [unknown]: Not specified"
        },
        {
          "text": "Identifying and mapping potential AI system vulnerabilities.",
          "misconception": "Targets [functional role confusion]: This describes the 'Map' function."
        },
        {
          "text": "Developing incident response and recovery plans.",
          "misconception": "Targets [functional role confusion]: This describes aspects of the 'Manage' function."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Govern' function establishes the organizational foundation for AI risk management by fostering a risk-aware culture, defining policies, and ensuring accountability, which then guides the other AI RMF functions.",
        "distractor_analysis": "Each distractor describes a core activity of another AI RMF function (Measure, Map, Manage), failing to capture the overarching, culture-setting role of the Govern function.",
        "analogy": "In a company, 'Govern' is like the CEO and board setting the ethical standards and overall strategy, while 'Map,' 'Measure,' and 'Manage' are departments executing specific operational tasks within that framework."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is the primary objective of a 'Membership Inference Attack'?",
      "correct_answer": "To determine whether a specific data sample was part of the training dataset used for an ML model.",
      "distractors": [
        {
          "text": "To reconstruct the original sensitive data from the model's outputs.",
          "misconception": "Targets [attack type confusion]: This describes data reconstruction attacks."
        },
        {
          "text": "To infer global properties or statistics about the training dataset.",
          "misconception": "Targets [attack type confusion]: This describes property inference attacks."
        },
        {
          "text": "To extract the model's architecture and parameters.",
          "misconception": "Targets [attack type confusion]: This describes model extraction attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Membership inference attacks exploit differences in model behavior between training data and unseen data to infer if a specific record was used during training, posing a privacy risk by revealing dataset membership.",
        "distractor_analysis": "Each distractor describes a different type of privacy attack (data reconstruction, property inference, model extraction), misrepresenting the specific goal of membership inference.",
        "analogy": "It's like a detective trying to figure out if a specific person was present at a secret meeting by observing how closely they match the known attendees' profiles, without knowing the full guest list."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AML_PRIVACY_ATTACKS"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes four functions: Govern, Map, Measure, and Manage. Which function is typically initiated after 'Map' and before 'Manage'?",
      "correct_answer": "Measure",
      "distractors": [
        {
          "text": "Govern",
          "misconception": "Targets [functional order confusion]: Govern is a cross-cutting function, often established first or continuously."
        },
        {
          "text": "Analyze",
          "misconception": "Targets [non-existent function]: 'Analyze' is not one of the four core AI RMF functions."
        },
        {
          "text": "Implement",
          "misconception": "Targets [non-existent function]: 'Implement' is too general and not a specific AI RMF core function."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The AI RMF Core functions are generally sequential: 'Govern' sets the framework, 'Map' establishes context, 'Measure' assesses risks within that context, and 'Manage' implements treatments. Therefore, 'Measure' follows 'Map' and precedes 'Manage'.",
        "distractor_analysis": "Distractors either misrepresent the role of 'Govern' or introduce non-existent functions, failing to recognize the logical flow of risk management activities within the AI RMF.",
        "analogy": "In building a house: 'Govern' is the building code, 'Map' is the blueprint, 'Measure' is surveying the land and checking dimensions, and 'Manage' is the construction process itself."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "procedure",
      "bloom_level": "apply",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is a key characteristic of 'Indirect Prompt Injection Attacks' in GenAI systems?",
      "correct_answer": "They are mounted by a third party who manipulates external resources that the AI system interacts with.",
      "distractors": [
        {
          "text": "They are executed by the primary user through direct query access.",
          "misconception": "Targets [attack vector confusion]: This describes direct prompt injection."
        },
        {
          "text": "They require control over the model's training data or parameters.",
          "misconception": "Targets [attack vector confusion]: This describes poisoning or model control attacks."
        },
        {
          "text": "They aim to extract the model's architecture or parameters.",
          "misconception": "Targets [attack goal confusion]: This describes model extraction attacks."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Indirect prompt injection attacks leverage an attacker's control over external resources (like documents or web pages) that a GenAI system accesses at runtime, allowing them to inject malicious prompts without direct user interaction.",
        "distractor_analysis": "Distractors incorrectly attribute the attack vector to direct user interaction (direct prompt injection), training data control (poisoning), or a specific goal (model extraction), mischaracterizing the indirect nature of the attack.",
        "analogy": "It's like a hacker altering a website that a chatbot uses for information; when the chatbot visits the site, it unknowingly receives malicious instructions."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "GENAI_ATTACK_TYPES"
      ]
    },
    {
      "question_text": "The NIST AI RMF Core includes four functions. Which function is responsible for establishing the context and understanding the potential impacts of an AI system?",
      "correct_answer": "Map",
      "distractors": [
        {
          "text": "Govern",
          "misconception": "Targets [functional role confusion]: Govern focuses on culture, policies, and accountability."
        },
        {
          "text": "Measure",
          "misconception": "Targets [functional role confusion]: Measure focuses on quantifying risks and impacts."
        },
        {
          "text": "Manage",
          "misconception": "Targets [functional role confusion]: Manage focuses on risk treatment and response."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Map' function in the AI RMF is dedicated to understanding the AI system's context, including its intended purposes, potential benefits, and harms. This contextual understanding is foundational for all subsequent risk management activities.",
        "distractor_analysis": "Distractors describe the primary roles of the other AI RMF Core functions (Govern, Measure, Manage), misattributing the function of context establishment and impact assessment to them.",
        "analogy": "In planning a journey, 'Map' is like studying the destination, the terrain, and potential routes before deciding where to go and how to get there."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2e2025, what is a key challenge in developing effective mitigations against adversarial machine learning attacks?",
      "correct_answer": "Many mitigations are empirical and lack theoretical guarantees, making them vulnerable to new or adaptive attacks.",
      "distractors": [
        {
          "text": "Adversarial attacks are too complex for current AI models to generate.",
          "misconception": "Targets [threat underestimation]: NIST AI 100-2e2025 details sophisticated and practical attacks."
        },
        {
          "text": "Defenses are only effective if they completely eliminate all possible adversarial inputs.",
          "misconception": "Targets [unrealistic expectation]: The goal is risk reduction, not absolute elimination, due to inherent trade-offs."
        },
        {
          "text": "The cost of implementing defenses is always lower than the cost of attacks.",
          "misconception": "Targets [cost misconception]: Defense implementation can be very costly, and the cost-benefit analysis is complex."
        }
      ],
      "detailed_explanation": {
        "core_logic": "A significant challenge in AML is that many defenses are developed empirically and can be bypassed by stronger, adaptive attacks, as theoretical guarantees for robustness are often lacking. This necessitates continuous evaluation and adaptation of defenses.",
        "distractor_analysis": "Distractors present unrealistic expectations about attack feasibility, defense completeness, or cost-effectiveness, failing to address the core challenge of empirical defenses being vulnerable to adaptive adversaries.",
        "analogy": "It's like building a security system based on known burglar tools; a new, unknown tool might bypass it, highlighting the need for adaptable and theoretically sound security."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AML_MITIGATION_CHALLENGES"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 19,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "ML Library and Framework Security Security And Risk Management best practices",
    "latency_ms": 37192.172
  },
  "timestamp": "2026-01-01T13:15:55.353785"
}