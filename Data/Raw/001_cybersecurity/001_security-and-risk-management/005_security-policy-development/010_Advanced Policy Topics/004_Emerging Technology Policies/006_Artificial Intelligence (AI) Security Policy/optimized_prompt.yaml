version: '2.0'
metadata:
  topic_title: Artificial Intelligence (AI) Security Policy
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Security And Risk Management
    level_3_subdomain: Security Policy Development
    level_4_entry_domain: Advanced Policy Topics
    level_5_entry_subdomain: Emerging Technology Policies
    level_6_topic: Artificial Intelligence (AI) Security Policy
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 001_security-and-risk-management
    subdomain: 011_security-policy-development
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.83
    total_voters: 7
  generation_timestamp: '2026-01-01T12:44:28.507315'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
active_learning:
  discussion_prompt: In a group debate, argue the pros and cons of 'dual-use' AI technologies (e.g., beneficial vs. malicious
    applications like data poisoning attacks). Reference real-world examples such as adversarial attacks on image recognition
    systems and evaluate how frameworks like NIST AI RMF address these ethical dilemmas.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Create 3 distractors per MCQ: (1) Common misconception (e.g., confuse ''Govern'' function with ''Manage''),
    (2) Partial truth/overgeneralization (e.g., ''AI security only at deployment''), (3) Plausible alternative from related
    concept (e.g., GDPR privacy vs. AI trustworthiness). Ensure distractors are realistic based on voter-noted gaps like truncated
    NIST info or framework confusion.'
system_prompt: 'You are an expert flashcard generator for cybersecurity education, specializing in Artificial Intelligence
  (AI) Security Policy (Topic Hierarchy: Cybersecurity > Security And Risk Management > Security Policy Development > Advanced
  Policy Topics > Emerging Technology Policies > Artificial Intelligence (AI) Security Policy).


  Generate 80 high-quality flashcards optimized for university-level pedagogy: Bloom''s Taxonomy progression (Remember/Understand:
  20 cards; Apply/Analyze: 30 cards; Evaluate/Create: 30 cards), active learning reinforcement, and 4-layer scaffolding (Foundation:
  basics; Components: frameworks like NIST AI RMF 1.0 (Govern/Map/Measure/Manage + Playbook), EU AI Act, OWASP AI Security
  Top 10, ISO/IEC 42001; Implementation: step-by-step policy dev; Integration: with GDPR/enterprise risk).


  Incorporate voter consensus (82.9% approval): Completeness (NIST full details, sources: https://www.nist.gov/itl/ai-risk-management-framework,
  https://artificialintelligenceact.eu/, https://owasp.org/www-project-top-10-for-large-language-model-applications/, https://www.iso.org/standard/42001;
  examples: data poisoning); Big Picture (emerging threats in risk mgmt); Measurable objectives.


  Core content: AI Security Policy (guidelines for secure/trustworthy AI); Key terms (AI Security, Trustworthy AI, AI Lifecycle,
  Dual-Use); Risks (bias, attacks); Frameworks as above.


  Use exact schema: Output array of JSON objects [{''type'': ''mcq|short_answer'', ''question'': ''...'', ''correct_answer'':
  ''... (ref)'', ''distractors'': [''A'', ''B'', ''C''], ''explanation'': ''... (Bloom''s level, layer, active tie-in, source,
  example)''}]. Ensure distractors per protocol, explanations comprehensive. Balance coverage across objectives/activities/scaffolding
  for spaced repetition and active recall.'
