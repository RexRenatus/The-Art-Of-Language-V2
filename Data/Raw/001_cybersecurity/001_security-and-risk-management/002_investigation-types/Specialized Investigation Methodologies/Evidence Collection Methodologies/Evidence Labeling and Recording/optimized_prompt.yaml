version: '2.0'
metadata:
  topic_title: Evidence Labeling and Recording
  hierarchy:
    level_1_category: Cybersecurity
    level_2_domain: Security And Risk Management
    level_3_subdomain: Investigation Types
    level_4_entry_domain: Specialized Investigation Methodologies
    level_5_entry_subdomain: Evidence Collection Methodologies
    level_6_topic: Evidence Labeling and Recording
  curriculum_type: cybersecurity
  source_folders:
    category: 001_cybersecurity
    domain: 001_security-and-risk-management
    subdomain: 002_investigation-types
  exa_sources: []
  voting:
    consensus_reached: true
    approval_percentage: 0.84
    total_voters: 7
  generation_timestamp: '2026-01-01T10:50:06.117874'
learning_objectives:
  understand:
  - objective: Explain core concepts
    verbs:
    - explain
    measurable: true
  apply:
  - objective: Apply knowledge to scenarios
    verbs:
    - apply
    measurable: true
  analyze:
  - objective: Analyze relationships
    verbs:
    - analyze
    measurable: true
  remember:
  - objective: Define key terminology
    verbs:
    - define
    measurable: true
active_learning:
  discussion_prompt: Analyze a real-world case study, such as the 2010 Operation Aurora breach where evidence mishandling
    contributed to challenges in prosecution. Discuss how improper labeling or gaps in chain of custody could lead to case
    dismissal. What specific steps in labeling and recording (e.g., hashing, timestamps) could have prevented this? Debate
    the trade-offs between rapid incident response speed and thorough documentation for evidence integrity.
  peer_teaching: Explain the key concepts to a partner without using technical jargon.
  problem_solving: Given a scenario, apply the framework to solve the problem.
  additional_activities: []
scaffolding:
- level: 1
  name: Foundation
  focus: Basic terminology and definitions
  content: ''
- level: 2
  name: Components
  focus: Framework components and structure
  content: ''
- level: 3
  name: Implementation
  focus: Practical implementation steps
  content: ''
- level: 4
  name: Integration
  focus: Advanced integration and optimization
  content: ''
flashcard_generation:
  output_schema:
    question: string
    correct_answer: string
    distractors:
    - text: string
      explanation: string
    explanation: string
    bloom_level: enum
    topic_hierarchy: object
  distractor_protocol: 'Generate 3 plausible distractors per MCQ based on common misconceptions: 1) Semantic confusion (e.g.,
    swap provenance with integrity), 2) Partial truth (e.g., hashing verifies origin but not listed as such), 3) Overgeneralization
    (e.g., ''timestamps alone suffice for chain of custody''). Ensure distractors are realistic from voter suggestions and
    research (e.g., ignoring physical vs. digital differences).'
system_prompt: "You are an expert flashcard generator for cybersecurity education, specializing in 'Evidence Labeling and\
  \ Recording' (Topic Hierarchy: Cybersecurity > Security And Risk Management > Investigation Types > Specialized Investigation\
  \ Methodologies > Evidence Collection Methodologies > Evidence Labeling and Recording). Generate 50 high-quality flashcards\
  \ optimized for spaced repetition (Anki-compatible), covering all Bloom's Taxonomy levels and 4 scaffolding layers provided\
  \ below.\n\n**Core Content Context (from NIST SP 800-61r3 and best practices):** Definitions: Event (observable occurrence),\
  \ Incident (jeopardizes CIA triad), Evidence (facts/actions), Provenance (origin/history), Integrity (no tampering), Chain\
  \ of Custody (handling documentation). Best practices: Unique IDs, hashing (SHA-256), standardized forms, digital/physical\
  \ labeling. Steps: Collect/tag/hash/document/handoff. Case studies: Mishandling leads to inadmissibility.\n\n**Learning\
  \ Objectives:** [Insert the 6 objectives here].\n\n**Scaffolding Layers:** [Insert the 4 layers here]. Distribute flashcards:\
  \ 30% Layer 1, 25% Layer 2, 25% Layer 3, 20% Layer 4.\n\n**Active Learning Inspiration:** Draw from discussion (case analysis),\
  \ peer teaching (analogies), problem-solving (mock evidence) to create scenario-based cards.\n\n**Flashcard Generation Rules:**\n\
  - Vary types per schema: 40% Basic Q&A, 30% MCQ, 20% Cloze, 10% Application.\n- Follow distractor protocol strictly.\n-\
  \ Include explanation requirements on every back.\n- Ensure measurable, active recall (no pure lists).\n- Prioritize voter\
  \ consensus: NIST compliance, real-world examples, misconceptions.\n\n**Output Format:** JSON array of objects:\n[\n  {\n\
  \    \"id\": 1,\n    \"type\": \"MCQ\",\n    \"bloom_level\": \"REMEMBER\",\n    \"layer\": 1,\n    \"front\": \"Question\
  \ or stem here...\",\n    \"back\": \"Correct answer + explanation + references here...\",\n    \"tags\": [\"evidence-labeling\"\
  , \"nist-800-61\"]\n  }\n]\nGenerate progressively: Start with Layer 1 foundation, build to integration. Ensure 84.3% voter-approved\
  \ quality."
