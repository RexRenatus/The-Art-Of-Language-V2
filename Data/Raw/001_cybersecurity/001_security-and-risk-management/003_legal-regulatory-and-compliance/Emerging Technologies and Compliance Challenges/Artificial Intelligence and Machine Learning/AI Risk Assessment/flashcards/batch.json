{
  "topic_title": "AI Risk Assessment",
  "category": "Cybersecurity - Security And Risk Management - Legal, Regulatory, and Compliance",
  "flashcards": [
    {
      "question_text": "According to the NIST AI Risk Management Framework (AI RMF), which of the following is a core function for managing AI risks?",
      "correct_answer": "Govern",
      "distractors": [
        {
          "text": "Analyze",
          "misconception": "Targets [misnomer]: This function is not part of the AI RMF core, though analysis is implied in other functions."
        },
        {
          "text": "Implement",
          "misconception": "Targets [process confusion]: 'Implement' is a general action, not one of the four core AI RMF functions."
        },
        {
          "text": "Validate",
          "misconception": "Targets [related but distinct concept]: Validation is a part of the 'Measure' function, not a core function itself."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI RMF Core comprises four functions: Govern, Map, Measure, and Manage. The 'Govern' function is crucial because it cultivates a risk management culture and aligns AI risk management with organizational principles, providing a foundational structure for the other functions.",
        "distractor_analysis": "Distractors represent plausible but incorrect terms. 'Analyze' is too general, 'Implement' describes an action rather than a function, and 'Validate' is a specific activity within the 'Measure' function, not a core function itself.",
        "analogy": "Think of the AI RMF core functions like the four legs of a table supporting trustworthy AI: Govern (the foundation), Map (understanding the terrain), Measure (checking the quality), and Manage (making adjustments)."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_BASICS"
      ]
    },
    {
      "question_text": "What is the primary purpose of the 'Map' function within the NIST AI Risk Management Framework (AI RMF)?",
      "correct_answer": "To establish the context for framing AI system risks.",
      "distractors": [
        {
          "text": "To quantify the likelihood and impact of identified risks.",
          "misconception": "Targets [function confusion]: This describes the 'Measure' function, not 'Map'."
        },
        {
          "text": "To implement controls and mitigation strategies for AI risks.",
          "misconception": "Targets [function confusion]: This describes the 'Manage' function, not 'Map'."
        },
        {
          "text": "To define the organizational policies for AI risk management.",
          "misconception": "Targets [function confusion]: This describes the 'Govern' function, not 'Map'."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Map' function is essential because it establishes the context for understanding AI risks by identifying purposes, potential impacts, and operational settings. This contextual understanding is foundational for effective risk management, enabling better decisions in subsequent 'Measure' and 'Manage' functions.",
        "distractor_analysis": "Each distractor incorrectly assigns the primary purpose of another AI RMF core function ('Measure', 'Manage', 'Govern') to the 'Map' function, testing the understanding of each function's distinct role.",
        "analogy": "Mapping an AI risk is like surveying a new territory before building: you need to understand the landscape (context) before you can plan (govern), measure its features (measure), and build defenses (manage)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "Which characteristic of trustworthy AI, as defined by NIST, refers to the AI system's ability to perform as required without failure over a given time interval under specified conditions?",
      "correct_answer": "Reliable",
      "distractors": [
        {
          "text": "Valid",
          "misconception": "Targets [related but distinct concept]: Validity confirms requirements are met for intended use, while reliability focuses on consistent performance without failure."
        },
        {
          "text": "Secure",
          "misconception": "Targets [different characteristic]: Security focuses on protection against unauthorized access and attacks, not operational consistency."
        },
        {
          "text": "Resilient",
          "misconception": "Targets [different characteristic]: Resilience is about withstanding adverse events and recovering, not continuous error-free operation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Reliability is a key characteristic of trustworthy AI because it ensures the system consistently performs its intended function without unexpected failures. This is crucial for trust, as unpredictable behavior undermines confidence and can lead to negative impacts, as defined by ISO/IEC TS 5723:2022.",
        "distractor_analysis": "The distractors represent other trustworthiness characteristics (Valid, Secure, Resilient) that are related but distinct from reliability, testing the precise definition of each.",
        "analogy": "A reliable car starts every time and runs smoothly on your commute, even if it's not the most secure or technologically advanced. Reliability is about consistent, predictable performance."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "TRUSTWORTHY_AI_CHARACTERISTICS"
      ]
    },
    {
      "question_text": "The NIST AI RMF emphasizes that AI risks should not be considered in isolation. What is a key recommendation for integrating AI risk management into broader organizational strategies?",
      "correct_answer": "Integrate AI risk management into enterprise risk management (ERM) strategies.",
      "distractors": [
        {
          "text": "Isolate AI risk management to a dedicated cybersecurity team.",
          "misconception": "Targets [siloed approach]: This contradicts the recommendation for integration and broader organizational involvement."
        },
        {
          "text": "Focus solely on technical risks, ignoring societal and ethical implications.",
          "misconception": "Targets [incomplete risk scope]: The AI RMF explicitly includes socio-technical aspects and potential harms beyond technical issues."
        },
        {
          "text": "Develop AI risk management policies independently of other compliance frameworks.",
          "misconception": "Targets [lack of synergy]: The RMF encourages alignment with existing practices and frameworks for efficiency and comprehensiveness."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Integrating AI risk management into broader enterprise risk management (ERM) is recommended because AI risks often overlap with other critical risks like cybersecurity and privacy. This integration fosters organizational efficiencies and a more holistic approach to risk, as stated in the AI RMF.",
        "distractor_analysis": "The distractors propose siloed, technically focused, or independent approaches, which are contrary to the AI RMF's emphasis on holistic, integrated risk management.",
        "analogy": "Instead of having separate emergency plans for fire, flood, and earthquake, an integrated ERM is like having a comprehensive disaster preparedness plan that covers all potential threats to a community."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "procedure",
      "bloom_level": "apply",
      "prerequisites": [
        "ERM_FUNDAMENTALS",
        "AI_RMF_INTEGRATION"
      ]
    },
    {
      "question_text": "When assessing AI risks, what does the NIST AI RMF identify as a significant challenge related to 'Risk Measurement'?",
      "correct_answer": "The lack of consensus on robust and verifiable measurement methods for AI risk and trustworthiness.",
      "distractors": [
        {
          "text": "AI systems are too fast to measure accurately in real-time.",
          "misconception": "Targets [technical limitation misunderstanding]: While speed is a factor, the core challenge is methodological consensus, not just speed."
        },
        {
          "text": "The cost of AI risk measurement tools is prohibitively high for most organizations.",
          "misconception": "Targets [economic focus over methodological]: While cost is a factor, the primary challenge highlighted is the lack of agreed-upon metrics and methods."
        },
        {
          "text": "AI risks are solely dependent on human cognitive biases, making them unquantifiable.",
          "misconception": "Targets [oversimplification of bias]: While human bias is a factor, AI risks are multifaceted and include systemic and computational aspects, and measurement methods are being developed."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI RMF highlights that a major challenge in AI risk measurement is the current lack of consensus on robust and verifiable methods applicable across diverse AI use cases. This makes it difficult to quantitatively or qualitatively assess AI risks and trustworthiness effectively.",
        "distractor_analysis": "The distractors focus on speed, cost, or a single cause (human bias), whereas the NIST document emphasizes the fundamental challenge of establishing agreed-upon, reliable measurement methodologies.",
        "analogy": "Trying to measure the 'risk' of a new AI is like trying to measure the 'quality' of a novel without established literary criticism standards; it's hard to agree on what to measure and how."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_RISK_MEASUREMENT_CHALLENGES"
      ]
    },
    {
      "question_text": "According to NIST, what is a key difference between AI risks and traditional software risks?",
      "correct_answer": "AI systems may be trained on data that changes over time, affecting functionality and trustworthiness in hard-to-understand ways.",
      "distractors": [
        {
          "text": "Traditional software risks are always more severe than AI risks.",
          "misconception": "Targets [false comparison]: The severity depends on the application; AI introduces unique and potentially significant risks."
        },
        {
          "text": "AI systems are inherently more secure than traditional software.",
          "misconception": "Targets [incorrect assumption]: AI systems introduce new security vulnerabilities, such as adversarial attacks and data poisoning."
        },
        {
          "text": "Traditional software is more complex and harder to manage than AI systems.",
          "misconception": "Targets [complexity misunderstanding]: AI systems, with their dynamic data and emergent properties, often present greater complexity and unpredictability."
        }
      ],
      "detailed_explanation": {
        "core_logic": "A significant distinction highlighted by NIST is that AI systems' reliance on data that can change over time (concept drift, data drift) introduces unique risks. This dynamic nature can alter system behavior and trustworthiness in ways that are difficult to predict or diagnose, unlike the more static nature of traditional software code.",
        "distractor_analysis": "The distractors make false claims about severity, security, or complexity, misrepresenting the unique challenges posed by AI systems, particularly concerning data dependency and dynamic behavior.",
        "analogy": "Traditional software is like a well-written book with fixed text. An AI system is more like a living document that constantly updates based on new information, which can be powerful but also introduce unexpected changes."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "comparison",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_VS_TRADITIONAL_SOFTWARE_RISKS"
      ]
    },
    {
      "question_text": "In the context of AI risk management, what does 'explainability' refer to?",
      "correct_answer": "A representation of the mechanisms underlying an AI system's operation.",
      "distractors": [
        {
          "text": "The meaning of an AI system's output in its designed context.",
          "misconception": "Targets [definition confusion]: This describes 'interpretability', not 'explainability'."
        },
        {
          "text": "The extent to which information about an AI system is available to users.",
          "misconception": "Targets [definition confusion]: This describes 'transparency', not 'explainability'."
        },
        {
          "text": "The AI system's ability to perform correctly without errors.",
          "misconception": "Targets [definition confusion]: This relates to 'validity' and 'reliability', not 'explainability'."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Explainability is crucial for AI risk management because it provides insight into 'how' an AI system makes decisions by representing its underlying mechanisms. This understanding helps in debugging, monitoring, and identifying potential biases or failure points, as detailed in NIST's AI RMF.",
        "distractor_analysis": "Each distractor incorrectly defines explainability by substituting the definitions of related but distinct AI trustworthiness characteristics: interpretability, transparency, and reliability.",
        "analogy": "Explainability is like understanding the engine of a car – knowing how the pistons, fuel injection, and ignition work together – whereas interpretability is understanding what the dashboard lights mean."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_TRUSTWORTHINESS_CHARACTERISTICS"
      ]
    },
    {
      "question_text": "A company is developing an AI system to assist in medical diagnoses. According to the NIST AI RMF, which trustworthiness characteristic is paramount to ensure patient safety and prevent harm?",
      "correct_answer": "Safe",
      "distractors": [
        {
          "text": "Explainable",
          "misconception": "Targets [prioritization error]: While explainability is important, safety is the primary concern for direct patient harm."
        },
        {
          "text": "Transparent",
          "misconception": "Targets [prioritization error]: Transparency is valuable but secondary to ensuring the system does not cause direct harm."
        },
        {
          "text": "Fair",
          "misconception": "Targets [prioritization error]: Fairness is critical, but the immediate concern in medical diagnosis is preventing physical or health-related harm."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Ensuring an AI system is 'Safe' is paramount in medical diagnosis because failures can directly endanger human life, health, property, or the environment, as per ISO/IEC TS 5723:2022. While other characteristics like explainability, transparency, and fairness are important, safety directly addresses the potential for severe harm.",
        "distractor_analysis": "The distractors represent other important AI trustworthiness characteristics, but 'Safe' is the most critical when the AI's output can directly lead to patient harm, testing the ability to prioritize risk based on potential impact.",
        "analogy": "When designing an AI for surgery, the absolute top priority is that it doesn't cause physical harm (safety), even if it's also transparent and fair."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "AI_SAFETY_PRINCIPLES",
        "MEDICAL_DIAGNOSIS_AI_RISKS"
      ]
    },
    {
      "question_text": "The NIST AI RMF outlines four core functions for managing AI risks. If an organization is focused on establishing clear roles, responsibilities, and a culture of risk management for its AI systems, which function is it primarily executing?",
      "correct_answer": "Govern",
      "distractors": [
        {
          "text": "Map",
          "misconception": "Targets [function confusion]: Mapping focuses on understanding context and identifying risks, not establishing organizational structures."
        },
        {
          "text": "Measure",
          "misconception": "Targets [function confusion]: Measuring involves assessing risks and trustworthiness using metrics, not defining organizational roles."
        },
        {
          "text": "Manage",
          "misconception": "Targets [function confusion]: Managing involves responding to and treating identified risks, not setting up the organizational framework."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'Govern' function is designed to cultivate and implement a culture of risk management, establish accountability structures, and align AI risk management with organizational principles. This directly addresses the establishment of roles, responsibilities, and a risk-aware culture, as described in the AI RMF.",
        "distractor_analysis": "The distractors represent other core AI RMF functions, testing the understanding of 'Govern' as the function responsible for organizational structure, culture, and policy.",
        "analogy": "The 'Govern' function is like setting up the rules and leadership structure for a new city before people start building houses or businesses; it defines how everything will operate."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_RMF_CORE_FUNCTIONS"
      ]
    },
    {
      "question_text": "When considering 'Fairness' in AI as per the NIST AI RMF, what is a key aspect to manage beyond demographic balance?",
      "correct_answer": "Mitigating harmful bias and addressing systemic, computational, and human-cognitive biases.",
      "distractors": [
        {
          "text": "Ensuring all users have equal access to the AI system, regardless of technical skill.",
          "misconception": "Targets [scope confusion]: While accessibility is important, fairness in AI specifically addresses bias and discrimination in outcomes."
        },
        {
          "text": "Maximizing the AI system's predictive accuracy across all demographic groups.",
          "misconception": "Targets [accuracy vs. fairness confusion]: High accuracy does not guarantee fairness; bias can exist even in accurate models."
        },
        {
          "text": "Making the AI system's decision-making process fully transparent to all users.",
          "misconception": "Targets [transparency vs. fairness confusion]: Transparency is a related but distinct characteristic; fairness focuses on equitable outcomes and bias mitigation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI RMF emphasizes that fairness in AI extends beyond simple demographic balance to actively managing harmful biases. This includes addressing systemic, computational, and human-cognitive biases, which can perpetuate or amplify inequities even in systems that appear balanced on the surface.",
        "distractor_analysis": "The distractors focus on accessibility, accuracy, or transparency, which are important but not the core definition of managing fairness and bias as described by NIST. They test the understanding that fairness is about equitable outcomes and mitigating various forms of bias.",
        "analogy": "Ensuring fairness in a competition isn't just about letting everyone enter (accessibility); it's about making sure the rules aren't rigged against certain participants (bias mitigation)."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_FAIRNESS_AND_BIAS"
      ]
    },
    {
      "question_text": "A cybersecurity firm is developing an AI-powered threat detection system. They are concerned about adversarial attacks where malicious actors might subtly alter input data to fool the AI. Which trustworthiness characteristic is most directly challenged by such attacks?",
      "correct_answer": "Secure and Resilient",
      "distractors": [
        {
          "text": "Valid and Reliable",
          "misconception": "Targets [related but distinct concept]: While adversarial attacks can impact validity and reliability, the primary challenge is the system's susceptibility to malicious manipulation, which falls under security."
        },
        {
          "text": "Accountable and Transparent",
          "misconception": "Targets [unrelated characteristic]: Adversarial attacks are about system integrity and robustness against manipulation, not about accountability or transparency of decision-making."
        },
        {
          "text": "Explainable and Interpretable",
          "misconception": "Targets [unrelated characteristic]: Explainability and interpretability relate to understanding how the AI works, not its defense against malicious input manipulation."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Adversarial attacks directly challenge the 'Secure and Resilient' characteristic of AI systems because they aim to compromise the system's integrity and intended function through malicious manipulation of inputs. This relates to protecting against unauthorized access and use, and maintaining functionality in the face of adverse events, as per the AI RMF.",
        "distractor_analysis": "The distractors represent other trustworthiness characteristics. While adversarial attacks can indirectly affect validity or reliability, the core issue is the system's security and resilience against deliberate manipulation.",
        "analogy": "A secure vault (Secure and Resilient) is designed to withstand break-in attempts (adversarial attacks), whereas a transparent ledger (Transparent) simply shows all transactions, and a reliable clock (Reliable) keeps accurate time."
      },
      "code_snippets": [],
      "difficulty": "expert",
      "question_type": "attack",
      "bloom_level": "apply",
      "prerequisites": [
        "AI_SECURITY_THREATS",
        "ADVERSARIAL_ML"
      ]
    },
    {
      "question_text": "The NIST AI RMF suggests that AI risk management should be integrated into broader enterprise risk management (ERM). What is a primary benefit of this integration?",
      "correct_answer": "It promotes organizational efficiencies and a more holistic approach to managing interconnected risks.",
      "distractors": [
        {
          "text": "It allows organizations to exclusively focus on AI-specific risks, ignoring other threats.",
          "misconception": "Targets [siloed approach]: Integration aims to connect AI risks with other risks, not isolate them."
        },
        {
          "text": "It simplifies AI risk assessment by reducing the number of factors to consider.",
          "misconception": "Targets [misunderstanding of integration]: Integration often involves considering more factors and their interdependencies, leading to a more comprehensive, not simpler, view."
        },
        {
          "text": "It mandates the use of specific AI risk management tools across all departments.",
          "misconception": "Targets [prescriptive vs. flexible approach]: The AI RMF is voluntary and flexible, encouraging alignment rather than mandating specific tools."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Integrating AI risk management into ERM is beneficial because AI risks often intersect with other organizational risks (e.g., cybersecurity, privacy). This holistic approach fosters efficiencies by leveraging existing ERM processes and provides a more comprehensive understanding of the organization's overall risk landscape, as supported by the AI RMF.",
        "distractor_analysis": "The distractors suggest isolation, oversimplification, or rigid mandates, which are contrary to the AI RMF's principles of integration, holistic risk management, and flexibility.",
        "analogy": "Instead of having separate budgets and teams for managing home security, car insurance, and health insurance, an integrated approach is like having a family financial planner who considers all your financial risks together for better overall management."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "ERM_BENEFITS",
        "AI_RMF_INTEGRATION"
      ]
    },
    {
      "question_text": "When discussing 'AI Risks and Trustworthiness' in the NIST AI RMF, what does the characteristic 'Accountable and Transparent' imply?",
      "correct_answer": "Accountability presupposes transparency, meaning information about the AI system and its outputs is available to relevant parties.",
      "distractors": [
        {
          "text": "The AI system's decision-making process must be fully understandable by any user.",
          "misconception": "Targets [overstated requirement]: Transparency means information is available, not necessarily that it's fully understandable by all, which relates more to explainability/interpretability."
        },
        {
          "text": "The AI system must always prioritize user privacy over operational transparency.",
          "misconception": "Targets [false dichotomy]: Transparency and privacy are both important characteristics that may involve tradeoffs, not necessarily a strict hierarchy."
        },
        {
          "text": "The AI system's developers are solely responsible for all its outcomes, regardless of use.",
          "misconception": "Targets [shared responsibility misunderstanding]: Accountability in AI is often shared among various AI actors (developers, deployers, users), not solely on developers."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI RMF states that accountability is built upon transparency, meaning that relevant information about an AI system's design, data, and decision-making processes should be accessible. This availability of information is crucial for enabling accountability and redress when negative impacts occur.",
        "distractor_analysis": "The distractors misrepresent the scope of transparency (making it universally understandable), create a false conflict between transparency and privacy, and incorrectly assign sole responsibility, testing the nuanced understanding of accountability and transparency in AI.",
        "analogy": "A transparent company (Transparent) makes its financial reports public, allowing stakeholders to hold management accountable (Accountable) for financial performance."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_TRUSTWORTHINESS_CHARACTERISTICS"
      ]
    },
    {
      "question_text": "A government agency is using an AI system to process applications for social benefits. They discover the AI disproportionately denies applications from a specific demographic group, even though the training data was diverse. This scenario highlights a risk related to which trustworthiness characteristic?",
      "correct_answer": "Fair – with Harmful Bias Managed",
      "distractors": [
        {
          "text": "Secure and Resilient",
          "misconception": "Targets [unrelated characteristic]: The issue is about equitable outcomes, not the system's security or ability to withstand attacks."
        },
        {
          "text": "Explainable and Interpretable",
          "misconception": "Targets [related but distinct concept]: While lack of explainability might hide the bias, the core issue is the unfair outcome itself."
        },
        {
          "text": "Valid and Reliable",
          "misconception": "Targets [accuracy vs. fairness confusion]: The system might be technically valid and reliable in its predictions, but those predictions lead to unfair outcomes."
        }
      ],
      "detailed_explanation": {
        "core_logic": "This scenario directly relates to the 'Fair – with Harmful Bias Managed' characteristic because the AI system is producing discriminatory outcomes, even if technically valid. The NIST AI RMF emphasizes that fairness requires addressing biases that can lead to inequitable treatment, regardless of the data's apparent diversity.",
        "distractor_analysis": "The distractors represent other trustworthiness characteristics. While explainability might help diagnose the problem, and validity/reliability are technical measures, the fundamental issue described is unfairness due to bias.",
        "analogy": "If a referee in a sports game consistently makes calls that disadvantage one team, even if they are applying the rules (valid/reliable), the game is not fair (Fairness)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "AI_FAIRNESS_AND_BIAS",
        "BIAS_IN_AI_EXAMPLES"
      ]
    },
    {
      "question_text": "The NIST AI RMF identifies 'AI Risks and Trustworthiness' as a key section. Which of the following is NOT listed as a characteristic of trustworthy AI systems?",
      "correct_answer": "Cost-effective",
      "distractors": [
        {
          "text": "Privacy-Enhanced",
          "misconception": "Targets [incorrect exclusion]: Privacy-Enhanced is explicitly listed as a characteristic of trustworthy AI."
        },
        {
          "text": "Accountable and Transparent",
          "misconception": "Targets [incorrect exclusion]: Accountable and Transparent is explicitly listed as a characteristic of trustworthy AI."
        },
        {
          "text": "Safe",
          "misconception": "Targets [incorrect exclusion]: Safe is explicitly listed as a characteristic of trustworthy AI."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI RMF lists seven characteristics of trustworthy AI: Valid and Reliable, Safe, Secure and Resilient, Accountable and Transparent, Explainable and Interpretable, Privacy-Enhanced, and Fair – with Harmful Bias Managed. 'Cost-effective' is a desirable business attribute but not a core trustworthiness characteristic defined by NIST in this framework.",
        "distractor_analysis": "The distractors are all actual trustworthiness characteristics listed in the NIST AI RMF, testing recall and understanding of the framework's defined attributes.",
        "analogy": "When evaluating a car, 'safety' and 'reliability' are core trustworthiness features, while 'fuel efficiency' or 'low price' are important but secondary considerations."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "remember",
      "prerequisites": [
        "AI_RMF_TRUSTWORTHINESS_CHARACTERISTICS"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 15,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "AI Risk Assessment Security And Risk Management best practices",
    "latency_ms": 23407.091999999997
  },
  "timestamp": "2026-01-01T10:53:33.835479"
}