{
  "topic_title": "Cognitive Computing Risks",
  "category": "Cybersecurity - Security And Risk Management",
  "flashcards": [
    {
      "question_text": "What is a primary security risk associated with the 'black box' nature of some cognitive computing systems, particularly in decision-making processes?",
      "correct_answer": "Lack of transparency and explainability, making it difficult to audit or debug decisions.",
      "distractors": [
        {
          "text": "Over-reliance on human input leading to decision bottlenecks",
          "misconception": "Targets [process error]: Confuses cognitive computing with traditional human-driven processes."
        },
        {
          "text": "High computational resource requirements for basic operations",
          "misconception": "Targets [resource issue]: Focuses on operational cost rather than security risk."
        },
        {
          "text": "Vulnerability to simple denial-of-service attacks on data inputs",
          "misconception": "Targets [attack vector]: Overlooks the complexity of cognitive systems for a basic DoS attack."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The 'black box' nature means the internal logic is opaque, making it hard to understand *why* a decision was made. This lack of explainability is a significant risk because it hinders auditing, debugging, and trust, which are crucial for security and risk management.",
        "distractor_analysis": "Distractors focus on common IT challenges (bottlenecks, resource needs, basic DoS) rather than the specific security implications of opaque AI decision-making.",
        "analogy": "It's like having a brilliant advisor whose advice you can't question or understand the reasoning behind – you have to trust them blindly, which is risky."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "COGNITIVE_COMPUTING_BASICS",
        "AI_TRANSPARENCY"
      ]
    },
    {
      "question_text": "Which NIST publication provides a framework for managing risks associated with AI systems, including those related to cognitive computing?",
      "correct_answer": "NIST AI Risk Management Framework (AI RMF)",
      "distractors": [
        {
          "text": "NIST Cybersecurity Framework (CSF)",
          "misconception": "Targets [scope mismatch]: CSF is broader cybersecurity, AI RMF is AI-specific."
        },
        {
          "text": "NIST SP 800-53, Security and Privacy Controls",
          "misconception": "Targets [control focus]: SP 800-53 lists controls, AI RMF provides a risk management process."
        },
        {
          "text": "NIST AI 100-2, Adversarial Machine Learning Taxonomy",
          "misconception": "Targets [specific focus]: This report details AML attacks, AI RMF is a broader risk management framework."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The NIST AI Risk Management Framework (AI RMF) provides a structured approach to managing risks specifically associated with AI systems, including cognitive computing. It helps organizations understand, measure, and manage AI risks throughout the AI lifecycle, unlike broader cybersecurity frameworks.",
        "distractor_analysis": "Distractors represent related but distinct NIST publications: CSF is general cybersecurity, SP 800-53 lists controls, and AI 100-2 focuses on adversarial ML, whereas the AI RMF offers a comprehensive risk management process for AI.",
        "analogy": "The AI RMF is like a specialized toolkit for managing the unique risks of AI, while the NIST CSF is a general toolbox for all IT security."
      },
      "code_snippets": [],
      "difficulty": "foundational",
      "question_type": "definition",
      "bloom_level": "remember",
      "prerequisites": [
        "NIST_AI_RMF"
      ]
    },
    {
      "question_text": "In the context of cognitive computing, what is a significant risk related to the data used for training and operation?",
      "correct_answer": "Data poisoning or manipulation can lead to biased or incorrect outputs and decisions.",
      "distractors": [
        {
          "text": "Insufficient data volume for effective learning",
          "misconception": "Targets [data quantity vs. quality]: Overlooks the impact of data integrity and bias."
        },
        {
          "text": "Data storage costs exceeding operational budgets",
          "misconception": "Targets [operational cost]: Focuses on financial aspects, not security risks of data."
        },
        {
          "text": "Data privacy violations due to improper handling of PII",
          "misconception": "Targets [privacy vs. integrity]: While privacy is a risk, data poisoning directly impacts decision integrity."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Cognitive systems learn from data; therefore, if that data is poisoned or manipulated, the system will learn incorrect patterns or biases. This directly impacts the integrity and reliability of its outputs and decisions, a core risk in security and risk management.",
        "distractor_analysis": "Distractors touch on data-related issues but miss the core security risk: data integrity leading to compromised decision-making, as highlighted in NIST AI 100-2.",
        "analogy": "Feeding a chef bad ingredients will result in a bad meal, no matter how skilled the chef is; similarly, poisoned data leads to flawed cognitive outputs."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "DATA_INTEGRITY",
        "AI_TRAINING_DATA"
      ]
    },
    {
      "question_text": "How can adversarial attacks, as described in NIST AI 100-2, specifically target cognitive computing systems?",
      "correct_answer": "By subtly altering input data to cause misclassification or incorrect reasoning, leading to flawed outputs.",
      "distractors": [
        {
          "text": "By exploiting vulnerabilities in the underlying hardware infrastructure",
          "misconception": "Targets [attack surface]: Focuses on traditional hardware vulnerabilities, not AI-specific attacks."
        },
        {
          "text": "By overwhelming the system with legitimate user requests",
          "misconception": "Targets [attack type]: Describes a denial-of-service attack, not adversarial manipulation of cognitive processes."
        },
        {
          "text": "By encrypting the cognitive model's decision-making algorithms",
          "misconception": "Targets [mitigation vs. attack]: Encryption is a defense, not an attack method against cognitive logic."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Adversarial attacks, as detailed in NIST AI 100-2, aim to manipulate AI systems by crafting inputs that are slightly altered but cause significant misbehavior. For cognitive computing, this means tricking the system's reasoning or classification processes, leading to incorrect outputs or decisions.",
        "distractor_analysis": "Distractors describe general IT security threats (hardware exploits, DoS, encryption) rather than the specific AI-focused adversarial manipulation of inputs that characterizes attacks on cognitive systems.",
        "analogy": "It's like whispering a slightly altered instruction to a very literal assistant, causing them to misunderstand and perform the wrong task."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "attack",
      "bloom_level": "apply",
      "prerequisites": [
        "ADVERSARIAL_ML",
        "COGNITIVE_COMPUTING_BASICS"
      ]
    },
    {
      "question_text": "What is a key challenge in managing the security risks of cognitive computing systems that rely on continuous learning and adaptation?",
      "correct_answer": "Ensuring that the system's adaptations remain aligned with security policies and do not introduce new vulnerabilities.",
      "distractors": [
        {
          "text": "The high cost of continuous data acquisition and labeling",
          "misconception": "Targets [operational cost]: Focuses on resource expenditure, not the security implications of adaptation."
        },
        {
          "text": "The difficulty in integrating cognitive systems with legacy IT infrastructure",
          "misconception": "Targets [integration challenge]: Addresses compatibility issues, not the security risks of adaptive learning."
        },
        {
          "text": "The need for specialized hardware to support continuous learning",
          "misconception": "Targets [hardware requirement]: Focuses on infrastructure needs, not the security risks of adaptive AI behavior."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Cognitive systems that continuously learn can drift from their intended security posture if not carefully managed. The risk is that adaptations, while improving performance, might inadvertently bypass security controls or learn malicious patterns, requiring robust oversight.",
        "distractor_analysis": "Distractors focus on common operational or integration challenges, missing the core security risk: the potential for adaptive learning to introduce new vulnerabilities or deviate from security policies.",
        "analogy": "It's like a self-driving car that learns new routes; you need to ensure it doesn't learn to ignore traffic laws or drive into dangerous areas."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "CONTINUOUS_LEARNING",
        "AI_SECURITY_PRINCIPLES"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2, what is a primary goal of adversarial machine learning (AML) attacks against AI systems, including cognitive computing?",
      "correct_answer": "To compromise the security, privacy, or safety of AI systems by exploiting their data-driven nature.",
      "distractors": [
        {
          "text": "To increase the computational efficiency of AI models",
          "misconception": "Targets [goal confusion]: AML aims to disrupt, not improve, efficiency."
        },
        {
          "text": "To enhance the explainability of AI decision-making processes",
          "misconception": "Targets [goal confusion]: AML seeks to obscure or manipulate decisions, not clarify them."
        },
        {
          "text": "To reduce the data requirements for training AI models",
          "misconception": "Targets [goal confusion]: AML focuses on exploiting existing data or models, not reducing data needs."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Adversarial machine learning, as defined by NIST AI 100-2, focuses on exploiting the inherent vulnerabilities of AI systems, particularly their reliance on data. The primary goals are to undermine the system's security, compromise user privacy, or cause unsafe operations, rather than to improve its performance or efficiency.",
        "distractor_analysis": "Distractors propose positive or neutral outcomes (efficiency, explainability, reduced data needs) that are contrary to the malicious intent of adversarial attacks.",
        "analogy": "AML is like a saboteur trying to break a complex machine by subtly tampering with its fuel or instructions, not by trying to make it run better."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "ADVERSARIAL_ML",
        "AI_RISK_MANAGEMENT"
      ]
    },
    {
      "question_text": "Which of the following best describes the risk of 'model drift' in cognitive computing systems that continuously learn?",
      "correct_answer": "The system's performance or decision-making deviates from its intended behavior or security policies over time.",
      "distractors": [
        {
          "text": "The model becomes too complex to deploy on edge devices",
          "misconception": "Targets [technical constraint]: Focuses on deployment limitations, not behavioral changes."
        },
        {
          "text": "The training data becomes outdated and irrelevant",
          "misconception": "Targets [data issue]: While data relevance is important, model drift is about the model's learned behavior changing."
        },
        {
          "text": "The system requires more frequent software updates",
          "misconception": "Targets [maintenance issue]: This is a consequence, not the definition of model drift itself."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Model drift occurs when a continuously learning cognitive system's performance degrades or its behavior changes over time due to shifts in data distribution or learning processes. This deviation from its original, intended function is a significant risk because it can lead to incorrect decisions or security policy violations.",
        "distractor_analysis": "Distractors describe related but distinct issues: deployment constraints, data staleness, and maintenance needs, rather than the core concept of a model's learned behavior changing undesirably.",
        "analogy": "It's like a student who learns new things but starts forgetting fundamental lessons or developing bad study habits over time."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "CONTINUOUS_LEARNING",
        "MODEL_DRIFT"
      ]
    },
    {
      "question_text": "A cognitive computing system used for fraud detection starts incorrectly flagging legitimate transactions as fraudulent after being exposed to a new set of subtly manipulated transaction data. What type of risk does this primarily represent?",
      "correct_answer": "Data poisoning leading to integrity violation.",
      "distractors": [
        {
          "text": "Availability breakdown due to system overload",
          "misconception": "Targets [attack type]: The system is functioning but making wrong decisions, not failing to respond."
        },
        {
          "text": "Privacy compromise through unauthorized data access",
          "misconception": "Targets [risk category]: The issue is decision integrity, not data leakage."
        },
        {
          "text": "Model extraction leading to intellectual property theft",
          "misconception": "Targets [attack vector]: The risk is about the system's output, not the theft of the model itself."
        }
      ],
      "detailed_explanation": {
        "core_logic": "The scenario describes data poisoning, where manipulated input data causes the cognitive system to make incorrect decisions (flagging legitimate transactions as fraudulent). This directly impacts the integrity of the system's outputs, a key risk in fraud detection.",
        "distractor_analysis": "Distractors describe other types of risks (availability, privacy, model extraction) that are not directly represented by the described scenario of incorrect decision-making due to bad data.",
        "analogy": "It's like a detective who is fed false clues and starts wrongly accusing innocent people, compromising the integrity of their investigation."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "DATA_POISONING",
        "INTEGRITY_VIOLATION"
      ]
    },
    {
      "question_text": "What is the primary security concern when cognitive computing systems are used to automate critical decision-making processes?",
      "correct_answer": "The potential for errors or malicious manipulation to have severe, cascading consequences due to the system's autonomy.",
      "distractors": [
        {
          "text": "The high cost of implementing and maintaining such systems",
          "misconception": "Targets [operational cost]: Focuses on financial aspects, not the impact of flawed decisions."
        },
        {
          "text": "The difficulty in finding skilled personnel to manage the systems",
          "misconception": "Targets [personnel challenge]: Addresses staffing, not the inherent risk of automated decisions."
        },
        {
          "text": "The limited scalability of cognitive computing solutions",
          "misconception": "Targets [scalability issue]: Focuses on capacity, not the impact of incorrect autonomous decisions."
        }
      ],
      "detailed_explanation": {
        "core_logic": "When cognitive systems automate critical decisions, any error or malicious manipulation can have far-reaching and severe consequences because the system acts autonomously. This lack of human oversight in real-time decision-making amplifies the impact of any failure or compromise.",
        "distractor_analysis": "Distractors address common IT challenges like cost, staffing, and scalability, but fail to capture the unique security risk of autonomous, critical decision-making systems.",
        "analogy": "It's like giving an autonomous drone the authority to make life-or-death decisions without human oversight – any malfunction or misguidance could be catastrophic."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_AUTONOMY",
        "CRITICAL_SYSTEMS_SECURITY"
      ]
    },
    {
      "question_text": "Which of the following best describes the risk of 'bias amplification' in cognitive computing systems?",
      "correct_answer": "The system learns and magnifies existing societal biases present in training data, leading to unfair or discriminatory outcomes.",
      "distractors": [
        {
          "text": "The system's learning algorithms become too complex to understand",
          "misconception": "Targets [explainability vs. bias]: Focuses on complexity, not the origin or impact of bias."
        },
        {
          "text": "The system requires excessive computational power for bias detection",
          "misconception": "Targets [resource issue]: Addresses the cost of mitigation, not the root cause of bias."
        },
        {
          "text": "The system fails to adapt to new data, perpetuating old biases",
          "misconception": "Targets [adaptation failure]: This describes a lack of learning, not the amplification of existing biases."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Bias amplification occurs when a cognitive system, trained on data reflecting societal biases, not only learns these biases but also magnifies them through its decision-making processes. This leads to unfair or discriminatory outcomes, a critical risk in AI ethics and security.",
        "distractor_analysis": "Distractors focus on algorithmic complexity, resource costs, or failure to adapt, rather than the core issue of a system actively increasing existing biases from its training data.",
        "analogy": "It's like a mirror that not only reflects an image but also distorts it, making existing flaws appear much worse."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "definition",
      "bloom_level": "understand",
      "prerequisites": [
        "AI_BIAS",
        "DATA_QUALITY"
      ]
    },
    {
      "question_text": "What is a key best practice for mitigating the risk of bias amplification in cognitive computing systems, as suggested by AI risk management principles?",
      "correct_answer": "Implementing diverse and representative training datasets and employing bias detection/mitigation techniques.",
      "distractors": [
        {
          "text": "Using proprietary algorithms that are not publicly scrutinized",
          "misconception": "Targets [transparency issue]: Proprietary systems can hide bias; transparency is key."
        },
        {
          "text": "Limiting the system's learning capabilities to prevent adaptation",
          "misconception": "Targets [functionality reduction]: This hinders performance and doesn't address bias in existing learning."
        },
        {
          "text": "Relying solely on human oversight for all critical decisions",
          "misconception": "Targets [oversight vs. mitigation]: While oversight is important, it doesn't fix the biased system itself."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Mitigating bias amplification requires addressing its root cause: biased data and learning processes. Using diverse datasets ensures broader representation, while specific detection and mitigation techniques actively work to identify and correct biases, aligning with AI risk management best practices.",
        "distractor_analysis": "Distractors suggest approaches that either hide bias (proprietary algorithms), cripple the system (limit learning), or bypass the problem (human oversight) rather than directly addressing bias in the cognitive system.",
        "analogy": "To ensure a fair judge, you'd want a diverse jury pool and clear, unbiased legal precedents, not just a judge who claims to be fair without evidence."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "defense",
      "bloom_level": "apply",
      "prerequisites": [
        "AI_BIAS_MITIGATION",
        "DATA_GOVERNANCE"
      ]
    },
    {
      "question_text": "How does the 'explainability' challenge in cognitive computing relate to security and risk management?",
      "correct_answer": "Lack of explainability makes it difficult to audit decisions, identify root causes of failures or attacks, and build trust.",
      "distractors": [
        {
          "text": "It increases the system's susceptibility to denial-of-service attacks",
          "misconception": "Targets [attack vector]: Explainability is about understanding decisions, not preventing network attacks."
        },
        {
          "text": "It requires more powerful hardware for processing complex models",
          "misconception": "Targets [resource requirement]: Explainability is a functional/security issue, not a hardware one."
        },
        {
          "text": "It leads to higher energy consumption during operation",
          "misconception": "Targets [operational cost]: Explainability is about transparency, not energy efficiency."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Explainability is crucial for security because it allows for auditing, debugging, and understanding system behavior. Without it, identifying the source of errors, malicious manipulations (attacks), or policy violations becomes nearly impossible, eroding trust and hindering risk management efforts.",
        "distractor_analysis": "Distractors focus on unrelated technical or operational aspects like DoS, hardware needs, or energy consumption, missing the direct link between explainability and security/risk management functions like auditing and trust.",
        "analogy": "Trying to secure a system without understanding how it works is like trying to fix a lock without knowing its mechanism – you don't know what to secure or how it might fail."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "AI_EXPLAINABILITY",
        "AI_RISK_MANAGEMENT"
      ]
    },
    {
      "question_text": "What is a potential security risk if a cognitive computing system used for medical diagnosis is trained on data that is not representative of diverse patient populations?",
      "correct_answer": "The system may exhibit biased diagnostic performance, leading to misdiagnosis for underrepresented groups.",
      "distractors": [
        {
          "text": "The system's processing speed will be significantly reduced",
          "misconception": "Targets [performance metric]: Focuses on speed, not diagnostic accuracy or fairness."
        },
        {
          "text": "The system's user interface will become difficult to navigate",
          "misconception": "Targets [usability issue]: Irrelevant to the diagnostic accuracy and fairness of the AI."
        },
        {
          "text": "The system's data storage requirements will increase dramatically",
          "misconception": "Targets [resource requirement]: Data representation doesn't inherently increase storage needs."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Cognitive systems learn from data. If the training data lacks diversity, the system will not learn to accurately diagnose conditions across all patient demographics. This bias amplification leads to unequal and potentially harmful diagnostic performance, a critical security and ethical risk in healthcare.",
        "distractor_analysis": "Distractors focus on unrelated aspects like processing speed, UI usability, or storage, failing to address the core security and patient safety risk of biased diagnostic outcomes.",
        "analogy": "It's like training a doctor only on adult patients and then expecting them to accurately diagnose children – their knowledge would be incomplete and potentially dangerous."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "scenario",
      "bloom_level": "apply",
      "prerequisites": [
        "AI_BIAS",
        "HEALTHCARE_AI_RISKS"
      ]
    },
    {
      "question_text": "Which of the following is a critical security consideration for cognitive computing systems that integrate with external data sources or APIs?",
      "correct_answer": "Ensuring secure API endpoints and validating data integrity from external sources to prevent injection attacks.",
      "distractors": [
        {
          "text": "Maximizing the number of external data sources for richer insights",
          "misconception": "Targets [feature vs. risk]: More sources increase attack surface and complexity, not necessarily insights."
        },
        {
          "text": "Minimizing the system's computational footprint for efficiency",
          "misconception": "Targets [operational goal]: Efficiency is secondary to security when integrating external data."
        },
        {
          "text": "Using proprietary data formats to ensure data uniqueness",
          "misconception": "Targets [data format]: Data format doesn't inherently secure integrations; standards and validation do."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Integrating with external data sources or APIs introduces new attack vectors. Securing these integration points (APIs) and validating the incoming data are paramount to prevent malicious data injection or unauthorized access, which could compromise the cognitive system's integrity and security.",
        "distractor_analysis": "Distractors focus on increasing data sources, optimizing efficiency, or using proprietary formats, none of which directly address the security risks of external integrations like API security and data validation.",
        "analogy": "Connecting your smart home system to external services is like opening doors to your house; you need strong locks (secure APIs) and to check who's coming in (validate data)."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "defense",
      "bloom_level": "apply",
      "prerequisites": [
        "API_SECURITY",
        "DATA_VALIDATION",
        "INTEGRATION_RISKS"
      ]
    },
    {
      "question_text": "What is the primary risk of 'overfitting' in cognitive computing models, from a security and risk management perspective?",
      "correct_answer": "The model may perform poorly on new, unseen data, potentially leading to incorrect or unreliable decisions in real-world applications.",
      "distractors": [
        {
          "text": "The model becomes too large to deploy on standard hardware",
          "misconception": "Targets [deployment constraint]: Overfitting relates to performance on new data, not model size."
        },
        {
          "text": "The model's training process becomes excessively slow",
          "misconception": "Targets [performance metric]: Training speed is separate from the model's generalization ability."
        },
        {
          "text": "The model's internal parameters become unreadable",
          "misconception": "Targets [explainability vs. generalization]: Overfitting affects predictive accuracy, not parameter readability."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Overfitting means a cognitive model has learned the training data too well, including its noise and specific patterns, to the detriment of generalizing to new, unseen data. This leads to unreliable decisions in real-world applications, a significant risk for systems making critical judgments.",
        "distractor_analysis": "Distractors focus on model size, training speed, or parameter readability, which are not direct consequences of overfitting. The core risk is poor performance on new data.",
        "analogy": "It's like memorizing answers for a specific test but being unable to solve slightly different problems on the actual exam because you didn't truly understand the concepts."
      },
      "code_snippets": [],
      "difficulty": "intermediate",
      "question_type": "analysis",
      "bloom_level": "analyze",
      "prerequisites": [
        "OVERFITTING",
        "MODEL_GENERALIZATION"
      ]
    },
    {
      "question_text": "According to NIST AI 100-2, what is a key characteristic of 'evasion attacks' that makes them a threat to cognitive computing systems?",
      "correct_answer": "They involve subtly modifying input data to cause misclassification or incorrect behavior without being easily detectable.",
      "distractors": [
        {
          "text": "They require direct access to the model's training data",
          "misconception": "Targets [attack model]: Evasion attacks often work in black-box scenarios with query access."
        },
        {
          "text": "They aim to degrade the overall performance of the AI system",
          "misconception": "Targets [attack objective]: Evasion is typically targeted at specific inputs, not general degradation."
        },
        {
          "text": "They involve injecting malicious code into the AI's algorithms",
          "misconception": "Targets [attack vector]: This describes code injection, not manipulation of input data for misclassification."
        }
      ],
      "detailed_explanation": {
        "core_logic": "Evasion attacks, as detailed in NIST AI 100-2, are designed to fool AI systems by making small, often imperceptible changes to input data. This causes the system to misclassify or behave incorrectly, posing a significant threat because these attacks can be stealthy and bypass standard input validation.",
        "distractor_analysis": "Distractors misrepresent the attack's requirements (training data access), objective (general degradation), or method (code injection), failing to capture the essence of input manipulation for misclassification.",
        "analogy": "It's like changing a few letters in a word so a spell-checker flags it as incorrect, even though a human would easily read it correctly."
      },
      "code_snippets": [],
      "difficulty": "advanced",
      "question_type": "defense",
      "bloom_level": "analyze",
      "prerequisites": [
        "ADVERSARIAL_ML",
        "EVASION_ATTACKS"
      ]
    }
  ],
  "generation_metadata": {
    "model": "google/gemini-2.5-flash-lite",
    "num_generated": 16,
    "temperature": 0.1,
    "web_citations": [],
    "research_method": "openrouter_web_plugin",
    "search_query": "Cognitive Computing Risks Security And Risk Management best practices",
    "latency_ms": 24392.63
  },
  "timestamp": "2026-01-01T11:49:41.642465"
}