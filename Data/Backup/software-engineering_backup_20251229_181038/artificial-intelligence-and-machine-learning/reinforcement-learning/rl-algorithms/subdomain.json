{
  "subdomain_title": "RL Algorithms",
  "domain_title": "Reinforcement Learning",
  "category_title": "Artificial Intelligence and Machine Learning",
  "curriculum_type": "software-engineering",
  "processed_at": "2025-12-29T15:43:35.026430",
  "result": {
    "subdomain_title": "RL Algorithms",
    "curriculum_type": "software-engineering",
    "topic_root": "RL Algorithms",
    "topic_root_citation": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf",
    "detailed_hierarchy": [
      {
        "domain": "Foundational Concepts",
        "subdomains": [
          {
            "subdomain": "Markov Decision Processes",
            "topics": [
              "Agent-Environment Interface",
              "Goals and Rewards",
              "Returns and Episodes",
              "Markov Property",
              "Value Functions",
              "Optimal Value Functions",
              "Bellman Equations",
              "Policy Definitions"
            ]
          },
          {
            "subdomain": "Multi-Armed Bandits",
            "topics": [
              "Action-Value Methods",
              "Incremental Implementation",
              "Epsilon-Greedy Selection",
              "Optimistic Initial Values",
              "Upper-Confidence-Bound (UCB)",
              "Gradient Bandits",
              "Contextual Bandits"
            ]
          },
          {
            "subdomain": "Exploration-Exploitation",
            "topics": [
              "Epsilon-Greedy Strategies",
              "Softmax Action Selection",
              "Thompson Sampling",
              "Optimism Under Uncertainty",
              "Curiosity-Driven Methods",
              "Count-Based Exploration"
            ]
          }
        ]
      },
      {
        "domain": "Tabular Solution Methods",
        "subdomains": [
          {
            "subdomain": "Dynamic Programming",
            "topics": [
              "Policy Evaluation",
              "Policy Improvement",
              "Policy Iteration",
              "Value Iteration",
              "Asynchronous Dynamic Programming",
              "Generalized Policy Iteration",
              "Principle of Optimality"
            ]
          },
          {
            "subdomain": "Monte Carlo Methods",
            "topics": [
              "First-Visit MC Prediction",
              "Every-Visit MC Prediction",
              "MC Estimation of Action Values",
              "MC Control with Exploring Starts",
              "On-Policy MC Control",
              "Off-Policy MC Prediction",
              "Importance Sampling",
              "Incremental MC Implementation"
            ]
          },
          {
            "subdomain": "Temporal-Difference Learning",
            "topics": [
              "TD(0) Prediction",
              "TD Error",
              "SARSA (On-Policy TD Control)",
              "Q-Learning (Off-Policy TD Control)",
              "Expected SARSA",
              "Double Q-Learning",
              "n-Step TD Methods",
              "TD(λ) with Eligibility Traces"
            ]
          },
          {
            "subdomain": "Planning with Tabular Methods",
            "topics": [
              "Dyna-Q",
              "Dyna-Q+",
              "Prioritized Sweeping",
              "Trajectory Sampling",
              "Real-Time Dynamic Programming",
              "Monte Carlo Tree Search (MCTS)",
              "UCT (Upper Confidence Trees)"
            ]
          }
        ]
      },
      {
        "domain": "Value-Based Deep RL",
        "subdomains": [
          {
            "subdomain": "Deep Q-Networks",
            "topics": [
              "DQN (Deep Q-Network)",
              "Experience Replay",
              "Target Networks",
              "Double DQN (DDQN)",
              "Dueling DQN",
              "Prioritized Experience Replay (PER)",
              "Noisy DQN",
              "Rainbow DQN"
            ]
          },
          {
            "subdomain": "Distributional RL",
            "topics": [
              "C51 (Categorical DQN)",
              "Quantile Regression DQN (QR-DQN)",
              "Implicit Quantile Networks (IQN)",
              "Fully Parameterized Quantile Function (FQF)",
              "Distributional Policy Gradients",
              "Risk-Sensitive RL"
            ]
          },
          {
            "subdomain": "Recurrent Q-Networks",
            "topics": [
              "Deep Recurrent Q-Learning (DRQN)",
              "R2D2 (Recurrent Replay Distributed DQN)",
              "LSTM-based Q-Networks",
              "Recurrent Experience Replay",
              "Stored State vs. Zero State"
            ]
          }
        ]
      },
      {
        "domain": "Policy Gradient Methods",
        "subdomains": [
          {
            "subdomain": "Basic Policy Gradients",
            "topics": [
              "Policy Gradient Theorem",
              "REINFORCE",
              "REINFORCE with Baseline",
              "Advantage Function",
              "Natural Policy Gradient",
              "Vanilla Policy Gradient (VPG)"
            ]
          },
          {
            "subdomain": "Actor-Critic Methods",
            "topics": [
              "Advantage Actor-Critic (A2C)",
              "Asynchronous Advantage Actor-Critic (A3C)",
              "Generalized Advantage Estimation (GAE)",
              "TD(λ) Actor-Critic",
              "Eligibility Traces for Actor-Critic",
              "Soft Actor-Critic (SAC)"
            ]
          },
          {
            "subdomain": "Trust Region Methods",
            "topics": [
              "Trust Region Policy Optimization (TRPO)",
              "Proximal Policy Optimization (PPO)",
              "PPO-Clip",
              "PPO-Penalty",
              "Kronecker-Factored Trust Region (ACKTR)",
              "Actor-Critic with Experience Replay (ACER)"
            ]
          },
          {
            "subdomain": "Deterministic Policy Gradients",
            "topics": [
              "Deterministic Policy Gradient (DPG)",
              "Deep Deterministic Policy Gradient (DDPG)",
              "Twin Delayed DDPG (TD3)",
              "Soft Actor-Critic (SAC)",
              "Stochastic Value Gradients (SVG)"
            ]
          }
        ]
      },
      {
        "domain": "Model-Based RL",
        "subdomains": [
          {
            "subdomain": "Learned Dynamics Models",
            "topics": [
              "Forward Dynamics Models",
              "Inverse Dynamics Models",
              "Gaussian Process Models (PILCO)",
              "Neural Network Dynamics Models",
              "Ensemble Models (PETS)",
              "Probabilistic Dynamics",
              "Uncertainty Quantification"
            ]
          },
          {
            "subdomain": "Planning with Learned Models",
            "topics": [
              "Model Predictive Control (MPC)",
              "Shooting Methods (Random Shooting, CEM)",
              "Dyna Architecture",
              "Model-Based Value Expansion (MBVE)",
              "MBMF (Model-Based Model-Free)",
              "Model-Ensemble Trust-Region (ME-TRPO)",
              "Trajectory Optimization"
            ]
          },
          {
            "subdomain": "Latent Space Models",
            "topics": [
              "World Models",
              "Dreamer (V1, V2, V3)",
              "Recurrent State Space Models (RSSM)",
              "PlaNet",
              "TD-MPC (Temporal Difference MPC)",
              "Latent Imagination",
              "Variational Autoencoders for Dynamics"
            ]
          },
          {
            "subdomain": "Model-Based Policy Optimization",
            "topics": [
              "MBPO (Model-Based Policy Optimization)",
              "STEVE (Stochastic Ensemble Value Expansion)",
              "MB-MPO (Model-Based MPO)",
              "Guided Policy Search (GPS)",
              "Probabilistic Inference for Learning Control (PILCO)",
              "SLBO (Sample-Efficient Learning by Backpropagation)"
            ]
          },
          {
            "subdomain": "Value-Equivalent Models",
            "topics": [
              "MuZero",
              "Stochastic MuZero",
              "Gumbel MuZero",
              "EfficientZero",
              "Sampled MuZero",
              "AlphaZero",
              "Expert Iteration (ExIt)"
            ]
          }
        ]
      },
      {
        "domain": "Exploration Methods",
        "subdomains": [
          {
            "subdomain": "Intrinsic Motivation",
            "topics": [
              "Count-Based Exploration",
              "Pseudocount Methods",
              "Intrinsic Curiosity Module (ICM)",
              "Random Network Distillation (RND)",
              "Prediction Error as Curiosity",
              "Empowerment",
              "Information Gain"
            ]
          },
          {
            "subdomain": "Uncertainty-Based Exploration",
            "topics": [
              "Bayesian Exploration",
              "Thompson Sampling",
              "Posterior Sampling for RL (PSRL)",
              "UCB-Based Methods",
              "Optimism in Face of Uncertainty",
              "Bootstrap DQN"
            ]
          },
          {
            "subdomain": "Unsupervised RL",
            "topics": [
              "DIAYN (Diversity is All You Need)",
              "Variational Intrinsic Control (VIC)",
              "VALOR",
              "Skill Discovery",
              "Goal-Conditioned Exploration",
              "Maximum Entropy Exploration"
            ]
          }
        ]
      },
      {
        "domain": "Hierarchical RL",
        "subdomains": [
          {
            "subdomain": "Temporal Abstraction",
            "topics": [
              "Options Framework",
              "Semi-Markov Decision Processes (SMDPs)",
              "Intra-Option Learning",
              "Option-Critic Architecture",
              "Termination Functions",
              "Option Discovery"
            ]
          },
          {
            "subdomain": "Goal-Conditioned RL",
            "topics": [
              "Universal Value Function Approximators (UVFA)",
              "Hindsight Experience Replay (HER)",
              "Goal-Conditioned Policies",
              "Automatic Goal Generation",
              "Subgoal Discovery",
              "Hierarchical Actor-Critic (HAC)"
            ]
          },
          {
            "subdomain": "Feudal Architectures",
            "topics": [
              "FeUdal Networks (FuN)",
              "Manager-Worker Hierarchy",
              "HIRO (Data Efficient HRL)",
              "Director-Actor-Critic",
              "Strategic Attentive Writer (STRAW)",
              "Hierarchical DQN (h-DQN)"
            ]
          },
          {
            "subdomain": "Task Decomposition",
            "topics": [
              "MAXQ Value Function Decomposition",
              "HAM (Hierarchical Abstract Machines)",
              "Hierarchical Policy Gradients",
              "Recursive Reasoning",
              "Bottleneck Discovery",
              "Graph-Based Abstraction"
            ]
          }
        ]
      },
      {
        "domain": "Multi-Agent RL",
        "subdomains": [
          {
            "subdomain": "Independent Learning",
            "topics": [
              "Independent Q-Learning (IQL)",
              "Independent Actor-Critic (IA2C)",
              "Independent PPO (IPPO)",
              "Independent DDPG (IDDPG)",
              "Independent TRPO (ITRPO)",
              "Hysteretic Q-Learning"
            ]
          },
          {
            "subdomain": "Centralized Training Decentralized Execution",
            "topics": [
              "MADDPG (Multi-Agent DDPG)",
              "MAPPO (Multi-Agent PPO)",
              "MAA2C (Multi-Agent A2C)",
              "MATRPO (Multi-Agent TRPO)",
              "Centralized Critic",
              "Communication Protocols"
            ]
          },
          {
            "subdomain": "Value Decomposition",
            "topics": [
              "VDN (Value Decomposition Network)",
              "QMIX (Monotonic Value Factorization)",
              "QTRAN (Q-Transformation)",
              "QPLEX",
              "Weighted QMIX",
              "VDPPO (Value Decomposition PPO)"
            ]
          },
          {
            "subdomain": "Cooperative MARL",
            "topics": [
              "COMA (Counterfactual Multi-Agent)",
              "FACMAC (Factored Multi-Agent Critic)",
              "HAPPO (Heterogeneous-Agent PPO)",
              "HATRPO (Heterogeneous-Agent TRPO)",
              "Joint Action Learning",
              "Credit Assignment"
            ]
          },
          {
            "subdomain": "Competitive and Mixed MARL",
            "topics": [
              "Nash Q-Learning",
              "Minimax Q-Learning",
              "Self-Play",
              "Population-Based Training",
              "Opponent Modeling",
              "Game-Theoretic RL"
            ]
          }
        ]
      },
      {
        "domain": "Imitation Learning and Inverse RL",
        "subdomains": [
          {
            "subdomain": "Behavioral Cloning",
            "topics": [
              "Supervised Behavioral Cloning",
              "DAgger (Dataset Aggregation)",
              "DART (Disturbance-Based Adaptation)",
              "SMILe (Sequential Model Imitation)",
              "Covariate Shift Problem",
              "Query-Efficient Imitation"
            ]
          },
          {
            "subdomain": "Inverse Reinforcement Learning",
            "topics": [
              "Maximum Entropy IRL",
              "Maximum Causal Entropy IRL",
              "Apprenticeship Learning",
              "Guided Cost Learning (GCL)",
              "Relative Entropy IRL",
              "Bayesian IRL"
            ]
          },
          {
            "subdomain": "Adversarial Imitation Learning",
            "topics": [
              "GAIL (Generative Adversarial Imitation Learning)",
              "AIRL (Adversarial Inverse RL)",
              "VAIL (Variational Adversarial Imitation)",
              "InfoGAIL",
              "Discriminator Design",
              "Reward Recovery"
            ]
          },
          {
            "subdomain": "Learning from Observation",
            "topics": [
              "Third-Person Imitation",
              "Domain Confusion",
              "Time-Contrastive Networks (TCN)",
              "Video Imitation",
              "Cross-Embodiment Imitation",
              "State-Only Imitation"
            ]
          }
        ]
      },
      {
        "domain": "Offline RL and Batch RL",
        "subdomains": [
          {
            "subdomain": "Conservative Value Estimation",
            "topics": [
              "Conservative Q-Learning (CQL)",
              "Batch-Constrained Q-Learning (BCQ)",
              "BEAR (Bootstrapping Error Accumulation Reduction)",
              "BRAC (Behavior Regularized Actor-Critic)",
              "TD3+BC",
              "IQL (Implicit Q-Learning)"
            ]
          },
          {
            "subdomain": "Model-Based Offline RL",
            "topics": [
              "MOPO (Model-Based Offline Policy Optimization)",
              "COMBO (Conservative Offline Model-Based)",
              "RAMBO (Robust Adversarial Model-Based)",
              "MOReL (Model-Based Offline RL)",
              "Pessimism Principle",
              "Uncertainty Penalties"
            ]
          },
          {
            "subdomain": "Offline-to-Online RL",
            "topics": [
              "Offline Pretraining with Online Finetuning",
              "Advantage Weighted Regression (AWR)",
              "Cal-QL (Calibrated Q-Learning)",
              "Hybrid RL",
              "Balanced Replay",
              "Safe Policy Improvement"
            ]
          },
          {
            "subdomain": "Dataset Quality and Coverage",
            "topics": [
              "Data Collection Strategies",
              "Dataset Diagnostics",
              "Behavior Policy Evaluation",
              "Off-Policy Evaluation (OPE)",
              "Importance Sampling Corrections",
              "Marginal Importance Weights"
            ]
          }
        ]
      },
      {
        "domain": "Meta-RL and Transfer Learning",
        "subdomains": [
          {
            "subdomain": "Meta-Learning for RL",
            "topics": [
              "MAML (Model-Agnostic Meta-Learning)",
              "RL² (Fast RL via Slow RL)",
              "SNAIL (Simple Neural Attentive Meta-Learner)",
              "ProMP (Probabilistic Meta-Policy)",
              "Meta-Q-Learning",
              "Gradient-Based Meta-RL"
            ]
          },
          {
            "subdomain": "Contextual and Task-Conditioned RL",
            "topics": [
              "Context Encoders",
              "Task Embeddings",
              "Multi-Task RL",
              "Task Inference Networks",
              "PEARL (Probabilistic Embeddings)",
              "VariBAD (Variational Bayes Adaptive Deep RL)"
            ]
          },
          {
            "subdomain": "Transfer and Lifelong Learning",
            "topics": [
              "Progressive Neural Networks",
              "PathNet",
              "PackNet",
              "Policy Distillation",
              "Successor Features",
              "Generalized Policy Improvement (GPI)"
            ]
          },
          {
            "subdomain": "Few-Shot RL",
            "topics": [
              "Rapid Adaptation",
              "Experience Replay for Meta-RL",
              "Context-Based Adaptation",
              "Model-Based Meta-RL",
              "Zero-Shot Transfer",
              "Domain Randomization"
            ]
          }
        ]
      },
      {
        "domain": "Function Approximation",
        "subdomains": [
          {
            "subdomain": "Linear Function Approximation",
            "topics": [
              "Linear Value Function Approximation",
              "Feature Construction",
              "Tile Coding",
              "Radial Basis Functions (RBF)",
              "Fourier Basis",
              "Coarse Coding"
            ]
          },
          {
            "subdomain": "Neural Network Architectures",
            "topics": [
              "Multilayer Perceptrons (MLP)",
              "Convolutional Neural Networks (CNN)",
              "Recurrent Neural Networks (RNN/LSTM/GRU)",
              "Transformers for RL",
              "Attention Mechanisms",
              "Graph Neural Networks (GNN)"
            ]
          },
          {
            "subdomain": "Stability and Convergence",
            "topics": [
              "Deadly Triad Problem",
              "Gradient Temporal-Difference (GTD)",
              "GTD2 and TDC",
              "Emphatic TD",
              "Residual Gradients",
              "Target Networks"
            ]
          },
          {
            "subdomain": "Representation Learning",
            "topics": [
              "Autoencoders for State Representation",
              "Contrastive Learning",
              "CURL (Contrastive Unsupervised RL)",
              "Self-Supervised Learning",
              "Successor Representations",
              "Auxiliary Tasks"
            ]
          }
        ]
      },
      {
        "domain": "Specialized RL Algorithms",
        "subdomains": [
          {
            "subdomain": "Partially Observable Environments",
            "topics": [
              "POMDP (Partially Observable MDP)",
              "Belief State Planning",
              "Memory-Augmented Networks",
              "DRQN (Deep Recurrent Q-Network)",
              "Neural Episodic Control (NEC)",
              "MERLIN"
            ]
          },
          {
            "subdomain": "Continuous Action Spaces",
            "topics": [
              "Action Space Discretization",
              "Normalized Advantage Functions (NAF)",
              "Parameterized Actions",
              "Hybrid Action Spaces",
              "Stochastic Policy Gradients",
              "Deterministic Policy Gradients"
            ]
          },
          {
            "subdomain": "Sparse and Delayed Rewards",
            "topics": [
              "Reward Shaping",
              "Hindsight Learning",
              "Curriculum Learning",
              "Episodic Memory",
              "Go-Explore",
              "Never Give Up (NGU)"
            ]
          },
          {
            "subdomain": "Average Reward Setting",
            "topics": [
              "R-Learning",
              "Average Reward TD",
              "Differential Q-Learning",
              "Relative Value Iteration",
              "Gain-Bias Functions",
              "Continuing Tasks"
            ]
          }
        ]
      },
      {
        "domain": "Safe and Robust RL",
        "subdomains": [
          {
            "subdomain": "Constrained RL",
            "topics": [
              "Constrained MDPs (CMDPs)",
              "Constrained Policy Optimization (CPO)",
              "Lagrangian Methods",
              "Safe Policy Gradients",
              "Reward-Cost Trade-offs",
              "Barrier Functions"
            ]
          },
          {
            "subdomain": "Risk-Sensitive RL",
            "topics": [
              "Conditional Value at Risk (CVaR)",
              "Mean-Variance Optimization",
              "Worst-Case Optimization",
              "Robust MDPs",
              "Distributional Perspectives",
              "Risk-Averse Policies"
            ]
          },
          {
            "subdomain": "Safe Exploration",
            "topics": [
              "Safety Layers",
              "Shield Functions",
              "Human-in-the-Loop RL",
              "Learning from Demonstrations for Safety",
              "Safe Policy Improvement",
              "Constrained Exploration"
            ]
          },
          {
            "subdomain": "Robustness to Distribution Shift",
            "topics": [
              "Domain Randomization",
              "Adversarial Training",
              "Robust Value Functions",
              "Uncertainty Quantification",
              "Sim-to-Real Transfer",
              "Model Ensembles"
            ]
          }
        ]
      },
      {
        "domain": "Reward Learning and Specification",
        "subdomains": [
          {
            "subdomain": "Preference-Based RL",
            "topics": [
              "Learning from Human Preferences",
              "Dueling Bandits",
              "Preference Elicitation",
              "Ranking-Based Learning",
              "Comparison Queries",
              "Active Preference Learning"
            ]
          },
          {
            "subdomain": "Reward Shaping",
            "topics": [
              "Potential-Based Reward Shaping",
              "Intrinsic Rewards",
              "Auxiliary Rewards",
              "Reward Engineering",
              "Optimal Reward Design",
              "Shaping Convergence Guarantees"
            ]
          },
          {
            "subdomain": "Multi-Objective RL",
            "topics": [
              "Pareto Optimality",
              "Scalarization Methods",
              "Pareto Front Approximation",
              "Weight Adaptation",
              "Multi-Objective Policy Gradients",
              "Trade-off Discovery"
            ]
          }
        ]
      },
      {
        "domain": "Scalability and Distributed RL",
        "subdomains": [
          {
            "subdomain": "Parallel and Distributed Training",
            "topics": [
              "A3C (Asynchronous Advantage Actor-Critic)",
              "IMPALA (Importance Weighted Actor-Learner)",
              "Ape-X (Distributed Prioritized Experience Replay)",
              "R2D2 (Recurrent Replay Distributed DQN)",
              "Parameter Server Architectures",
              "Gradient Aggregation"
            ]
          },
          {
            "subdomain": "Sample Efficiency",
            "topics": [
              "Experience Replay Strategies",
              "Data Augmentation",
              "Model-Based Acceleration",
              "Auxiliary Tasks",
              "Self-Supervised Pretraining",
              "Transfer from Simulators"
            ]
          },
          {
            "subdomain": "Computational Efficiency",
            "topics": [
              "Network Compression",
              "Knowledge Distillation",
              "Pruning",
              "Quantization",
              "Early Stopping Strategies",
              "Efficient Architecture Search"
            ]
          }
        ]
      },
      {
        "domain": "Theoretical Foundations",
        "subdomains": [
          {
            "subdomain": "Convergence and Optimality",
            "topics": [
              "Policy Improvement Theorem",
              "Contraction Mappings",
              "Fixed Point Theorems",
              "Convergence of TD Learning",
              "Convergence of Q-Learning",
              "Optimality Conditions"
            ]
          },
          {
            "subdomain": "Sample Complexity",
            "topics": [
              "PAC Learning Bounds",
              "Regret Bounds",
              "Sample Complexity of Exploration",
              "Information-Theoretic Bounds",
              "Minimax Lower Bounds",
              "Instance-Dependent Bounds"
            ]
          },
          {
            "subdomain": "Function Approximation Theory",
            "topics": [
              "Approximation Error",
              "Generalization Error",
              "Bias-Variance Tradeoff",
              "Universal Approximation",
              "Feature Selection Theory",
              "Representational Capacity"
            ]
          },
          {
            "subdomain": "Policy Gradient Theory",
            "topics": [
              "Compatible Function Approximation",
              "Natural Gradient Justification",
              "Trust Region Analysis",
              "Monotonic Improvement Guarantees",
              "Variance Reduction Theory",
              "Actor-Critic Convergence"
            ]
          }
        ]
      }
    ]
  },
  "metadata": {
    "duration_seconds": 1123.227558,
    "timestamp": "2025-12-29T15:27:22.758341"
  }
}