{
  "domain_title": "reinforcement-learning",
  "category_title": "artificial-intelligence-and-machine-learning",
  "curriculum_type": "software-engineering",
  "subdomain_count": 2,
  "aggregated_at": "2025-12-29T16:15:54.860932",
  "subdomains": [
    {
      "subdomain_title": "RL Algorithms",
      "processed_at": "2025-12-29T15:43:35.026430",
      "result": {
        "subdomain_title": "RL Algorithms",
        "curriculum_type": "software-engineering",
        "topic_root": "RL Algorithms",
        "topic_root_citation": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf",
        "detailed_hierarchy": [
          {
            "domain": "Foundational Concepts",
            "subdomains": [
              {
                "subdomain": "Markov Decision Processes",
                "topics": [
                  "Agent-Environment Interface",
                  "Goals and Rewards",
                  "Returns and Episodes",
                  "Markov Property",
                  "Value Functions",
                  "Optimal Value Functions",
                  "Bellman Equations",
                  "Policy Definitions"
                ]
              },
              {
                "subdomain": "Multi-Armed Bandits",
                "topics": [
                  "Action-Value Methods",
                  "Incremental Implementation",
                  "Epsilon-Greedy Selection",
                  "Optimistic Initial Values",
                  "Upper-Confidence-Bound (UCB)",
                  "Gradient Bandits",
                  "Contextual Bandits"
                ]
              },
              {
                "subdomain": "Exploration-Exploitation",
                "topics": [
                  "Epsilon-Greedy Strategies",
                  "Softmax Action Selection",
                  "Thompson Sampling",
                  "Optimism Under Uncertainty",
                  "Curiosity-Driven Methods",
                  "Count-Based Exploration"
                ]
              }
            ]
          },
          {
            "domain": "Tabular Solution Methods",
            "subdomains": [
              {
                "subdomain": "Dynamic Programming",
                "topics": [
                  "Policy Evaluation",
                  "Policy Improvement",
                  "Policy Iteration",
                  "Value Iteration",
                  "Asynchronous Dynamic Programming",
                  "Generalized Policy Iteration",
                  "Principle of Optimality"
                ]
              },
              {
                "subdomain": "Monte Carlo Methods",
                "topics": [
                  "First-Visit MC Prediction",
                  "Every-Visit MC Prediction",
                  "MC Estimation of Action Values",
                  "MC Control with Exploring Starts",
                  "On-Policy MC Control",
                  "Off-Policy MC Prediction",
                  "Importance Sampling",
                  "Incremental MC Implementation"
                ]
              },
              {
                "subdomain": "Temporal-Difference Learning",
                "topics": [
                  "TD(0) Prediction",
                  "TD Error",
                  "SARSA (On-Policy TD Control)",
                  "Q-Learning (Off-Policy TD Control)",
                  "Expected SARSA",
                  "Double Q-Learning",
                  "n-Step TD Methods",
                  "TD(λ) with Eligibility Traces"
                ]
              },
              {
                "subdomain": "Planning with Tabular Methods",
                "topics": [
                  "Dyna-Q",
                  "Dyna-Q+",
                  "Prioritized Sweeping",
                  "Trajectory Sampling",
                  "Real-Time Dynamic Programming",
                  "Monte Carlo Tree Search (MCTS)",
                  "UCT (Upper Confidence Trees)"
                ]
              }
            ]
          },
          {
            "domain": "Value-Based Deep RL",
            "subdomains": [
              {
                "subdomain": "Deep Q-Networks",
                "topics": [
                  "DQN (Deep Q-Network)",
                  "Experience Replay",
                  "Target Networks",
                  "Double DQN (DDQN)",
                  "Dueling DQN",
                  "Prioritized Experience Replay (PER)",
                  "Noisy DQN",
                  "Rainbow DQN"
                ]
              },
              {
                "subdomain": "Distributional RL",
                "topics": [
                  "C51 (Categorical DQN)",
                  "Quantile Regression DQN (QR-DQN)",
                  "Implicit Quantile Networks (IQN)",
                  "Fully Parameterized Quantile Function (FQF)",
                  "Distributional Policy Gradients",
                  "Risk-Sensitive RL"
                ]
              },
              {
                "subdomain": "Recurrent Q-Networks",
                "topics": [
                  "Deep Recurrent Q-Learning (DRQN)",
                  "R2D2 (Recurrent Replay Distributed DQN)",
                  "LSTM-based Q-Networks",
                  "Recurrent Experience Replay",
                  "Stored State vs. Zero State"
                ]
              }
            ]
          },
          {
            "domain": "Policy Gradient Methods",
            "subdomains": [
              {
                "subdomain": "Basic Policy Gradients",
                "topics": [
                  "Policy Gradient Theorem",
                  "REINFORCE",
                  "REINFORCE with Baseline",
                  "Advantage Function",
                  "Natural Policy Gradient",
                  "Vanilla Policy Gradient (VPG)"
                ]
              },
              {
                "subdomain": "Actor-Critic Methods",
                "topics": [
                  "Advantage Actor-Critic (A2C)",
                  "Asynchronous Advantage Actor-Critic (A3C)",
                  "Generalized Advantage Estimation (GAE)",
                  "TD(λ) Actor-Critic",
                  "Eligibility Traces for Actor-Critic",
                  "Soft Actor-Critic (SAC)"
                ]
              },
              {
                "subdomain": "Trust Region Methods",
                "topics": [
                  "Trust Region Policy Optimization (TRPO)",
                  "Proximal Policy Optimization (PPO)",
                  "PPO-Clip",
                  "PPO-Penalty",
                  "Kronecker-Factored Trust Region (ACKTR)",
                  "Actor-Critic with Experience Replay (ACER)"
                ]
              },
              {
                "subdomain": "Deterministic Policy Gradients",
                "topics": [
                  "Deterministic Policy Gradient (DPG)",
                  "Deep Deterministic Policy Gradient (DDPG)",
                  "Twin Delayed DDPG (TD3)",
                  "Soft Actor-Critic (SAC)",
                  "Stochastic Value Gradients (SVG)"
                ]
              }
            ]
          },
          {
            "domain": "Model-Based RL",
            "subdomains": [
              {
                "subdomain": "Learned Dynamics Models",
                "topics": [
                  "Forward Dynamics Models",
                  "Inverse Dynamics Models",
                  "Gaussian Process Models (PILCO)",
                  "Neural Network Dynamics Models",
                  "Ensemble Models (PETS)",
                  "Probabilistic Dynamics",
                  "Uncertainty Quantification"
                ]
              },
              {
                "subdomain": "Planning with Learned Models",
                "topics": [
                  "Model Predictive Control (MPC)",
                  "Shooting Methods (Random Shooting, CEM)",
                  "Dyna Architecture",
                  "Model-Based Value Expansion (MBVE)",
                  "MBMF (Model-Based Model-Free)",
                  "Model-Ensemble Trust-Region (ME-TRPO)",
                  "Trajectory Optimization"
                ]
              },
              {
                "subdomain": "Latent Space Models",
                "topics": [
                  "World Models",
                  "Dreamer (V1, V2, V3)",
                  "Recurrent State Space Models (RSSM)",
                  "PlaNet",
                  "TD-MPC (Temporal Difference MPC)",
                  "Latent Imagination",
                  "Variational Autoencoders for Dynamics"
                ]
              },
              {
                "subdomain": "Model-Based Policy Optimization",
                "topics": [
                  "MBPO (Model-Based Policy Optimization)",
                  "STEVE (Stochastic Ensemble Value Expansion)",
                  "MB-MPO (Model-Based MPO)",
                  "Guided Policy Search (GPS)",
                  "Probabilistic Inference for Learning Control (PILCO)",
                  "SLBO (Sample-Efficient Learning by Backpropagation)"
                ]
              },
              {
                "subdomain": "Value-Equivalent Models",
                "topics": [
                  "MuZero",
                  "Stochastic MuZero",
                  "Gumbel MuZero",
                  "EfficientZero",
                  "Sampled MuZero",
                  "AlphaZero",
                  "Expert Iteration (ExIt)"
                ]
              }
            ]
          },
          {
            "domain": "Exploration Methods",
            "subdomains": [
              {
                "subdomain": "Intrinsic Motivation",
                "topics": [
                  "Count-Based Exploration",
                  "Pseudocount Methods",
                  "Intrinsic Curiosity Module (ICM)",
                  "Random Network Distillation (RND)",
                  "Prediction Error as Curiosity",
                  "Empowerment",
                  "Information Gain"
                ]
              },
              {
                "subdomain": "Uncertainty-Based Exploration",
                "topics": [
                  "Bayesian Exploration",
                  "Thompson Sampling",
                  "Posterior Sampling for RL (PSRL)",
                  "UCB-Based Methods",
                  "Optimism in Face of Uncertainty",
                  "Bootstrap DQN"
                ]
              },
              {
                "subdomain": "Unsupervised RL",
                "topics": [
                  "DIAYN (Diversity is All You Need)",
                  "Variational Intrinsic Control (VIC)",
                  "VALOR",
                  "Skill Discovery",
                  "Goal-Conditioned Exploration",
                  "Maximum Entropy Exploration"
                ]
              }
            ]
          },
          {
            "domain": "Hierarchical RL",
            "subdomains": [
              {
                "subdomain": "Temporal Abstraction",
                "topics": [
                  "Options Framework",
                  "Semi-Markov Decision Processes (SMDPs)",
                  "Intra-Option Learning",
                  "Option-Critic Architecture",
                  "Termination Functions",
                  "Option Discovery"
                ]
              },
              {
                "subdomain": "Goal-Conditioned RL",
                "topics": [
                  "Universal Value Function Approximators (UVFA)",
                  "Hindsight Experience Replay (HER)",
                  "Goal-Conditioned Policies",
                  "Automatic Goal Generation",
                  "Subgoal Discovery",
                  "Hierarchical Actor-Critic (HAC)"
                ]
              },
              {
                "subdomain": "Feudal Architectures",
                "topics": [
                  "FeUdal Networks (FuN)",
                  "Manager-Worker Hierarchy",
                  "HIRO (Data Efficient HRL)",
                  "Director-Actor-Critic",
                  "Strategic Attentive Writer (STRAW)",
                  "Hierarchical DQN (h-DQN)"
                ]
              },
              {
                "subdomain": "Task Decomposition",
                "topics": [
                  "MAXQ Value Function Decomposition",
                  "HAM (Hierarchical Abstract Machines)",
                  "Hierarchical Policy Gradients",
                  "Recursive Reasoning",
                  "Bottleneck Discovery",
                  "Graph-Based Abstraction"
                ]
              }
            ]
          },
          {
            "domain": "Multi-Agent RL",
            "subdomains": [
              {
                "subdomain": "Independent Learning",
                "topics": [
                  "Independent Q-Learning (IQL)",
                  "Independent Actor-Critic (IA2C)",
                  "Independent PPO (IPPO)",
                  "Independent DDPG (IDDPG)",
                  "Independent TRPO (ITRPO)",
                  "Hysteretic Q-Learning"
                ]
              },
              {
                "subdomain": "Centralized Training Decentralized Execution",
                "topics": [
                  "MADDPG (Multi-Agent DDPG)",
                  "MAPPO (Multi-Agent PPO)",
                  "MAA2C (Multi-Agent A2C)",
                  "MATRPO (Multi-Agent TRPO)",
                  "Centralized Critic",
                  "Communication Protocols"
                ]
              },
              {
                "subdomain": "Value Decomposition",
                "topics": [
                  "VDN (Value Decomposition Network)",
                  "QMIX (Monotonic Value Factorization)",
                  "QTRAN (Q-Transformation)",
                  "QPLEX",
                  "Weighted QMIX",
                  "VDPPO (Value Decomposition PPO)"
                ]
              },
              {
                "subdomain": "Cooperative MARL",
                "topics": [
                  "COMA (Counterfactual Multi-Agent)",
                  "FACMAC (Factored Multi-Agent Critic)",
                  "HAPPO (Heterogeneous-Agent PPO)",
                  "HATRPO (Heterogeneous-Agent TRPO)",
                  "Joint Action Learning",
                  "Credit Assignment"
                ]
              },
              {
                "subdomain": "Competitive and Mixed MARL",
                "topics": [
                  "Nash Q-Learning",
                  "Minimax Q-Learning",
                  "Self-Play",
                  "Population-Based Training",
                  "Opponent Modeling",
                  "Game-Theoretic RL"
                ]
              }
            ]
          },
          {
            "domain": "Imitation Learning and Inverse RL",
            "subdomains": [
              {
                "subdomain": "Behavioral Cloning",
                "topics": [
                  "Supervised Behavioral Cloning",
                  "DAgger (Dataset Aggregation)",
                  "DART (Disturbance-Based Adaptation)",
                  "SMILe (Sequential Model Imitation)",
                  "Covariate Shift Problem",
                  "Query-Efficient Imitation"
                ]
              },
              {
                "subdomain": "Inverse Reinforcement Learning",
                "topics": [
                  "Maximum Entropy IRL",
                  "Maximum Causal Entropy IRL",
                  "Apprenticeship Learning",
                  "Guided Cost Learning (GCL)",
                  "Relative Entropy IRL",
                  "Bayesian IRL"
                ]
              },
              {
                "subdomain": "Adversarial Imitation Learning",
                "topics": [
                  "GAIL (Generative Adversarial Imitation Learning)",
                  "AIRL (Adversarial Inverse RL)",
                  "VAIL (Variational Adversarial Imitation)",
                  "InfoGAIL",
                  "Discriminator Design",
                  "Reward Recovery"
                ]
              },
              {
                "subdomain": "Learning from Observation",
                "topics": [
                  "Third-Person Imitation",
                  "Domain Confusion",
                  "Time-Contrastive Networks (TCN)",
                  "Video Imitation",
                  "Cross-Embodiment Imitation",
                  "State-Only Imitation"
                ]
              }
            ]
          },
          {
            "domain": "Offline RL and Batch RL",
            "subdomains": [
              {
                "subdomain": "Conservative Value Estimation",
                "topics": [
                  "Conservative Q-Learning (CQL)",
                  "Batch-Constrained Q-Learning (BCQ)",
                  "BEAR (Bootstrapping Error Accumulation Reduction)",
                  "BRAC (Behavior Regularized Actor-Critic)",
                  "TD3+BC",
                  "IQL (Implicit Q-Learning)"
                ]
              },
              {
                "subdomain": "Model-Based Offline RL",
                "topics": [
                  "MOPO (Model-Based Offline Policy Optimization)",
                  "COMBO (Conservative Offline Model-Based)",
                  "RAMBO (Robust Adversarial Model-Based)",
                  "MOReL (Model-Based Offline RL)",
                  "Pessimism Principle",
                  "Uncertainty Penalties"
                ]
              },
              {
                "subdomain": "Offline-to-Online RL",
                "topics": [
                  "Offline Pretraining with Online Finetuning",
                  "Advantage Weighted Regression (AWR)",
                  "Cal-QL (Calibrated Q-Learning)",
                  "Hybrid RL",
                  "Balanced Replay",
                  "Safe Policy Improvement"
                ]
              },
              {
                "subdomain": "Dataset Quality and Coverage",
                "topics": [
                  "Data Collection Strategies",
                  "Dataset Diagnostics",
                  "Behavior Policy Evaluation",
                  "Off-Policy Evaluation (OPE)",
                  "Importance Sampling Corrections",
                  "Marginal Importance Weights"
                ]
              }
            ]
          },
          {
            "domain": "Meta-RL and Transfer Learning",
            "subdomains": [
              {
                "subdomain": "Meta-Learning for RL",
                "topics": [
                  "MAML (Model-Agnostic Meta-Learning)",
                  "RL² (Fast RL via Slow RL)",
                  "SNAIL (Simple Neural Attentive Meta-Learner)",
                  "ProMP (Probabilistic Meta-Policy)",
                  "Meta-Q-Learning",
                  "Gradient-Based Meta-RL"
                ]
              },
              {
                "subdomain": "Contextual and Task-Conditioned RL",
                "topics": [
                  "Context Encoders",
                  "Task Embeddings",
                  "Multi-Task RL",
                  "Task Inference Networks",
                  "PEARL (Probabilistic Embeddings)",
                  "VariBAD (Variational Bayes Adaptive Deep RL)"
                ]
              },
              {
                "subdomain": "Transfer and Lifelong Learning",
                "topics": [
                  "Progressive Neural Networks",
                  "PathNet",
                  "PackNet",
                  "Policy Distillation",
                  "Successor Features",
                  "Generalized Policy Improvement (GPI)"
                ]
              },
              {
                "subdomain": "Few-Shot RL",
                "topics": [
                  "Rapid Adaptation",
                  "Experience Replay for Meta-RL",
                  "Context-Based Adaptation",
                  "Model-Based Meta-RL",
                  "Zero-Shot Transfer",
                  "Domain Randomization"
                ]
              }
            ]
          },
          {
            "domain": "Function Approximation",
            "subdomains": [
              {
                "subdomain": "Linear Function Approximation",
                "topics": [
                  "Linear Value Function Approximation",
                  "Feature Construction",
                  "Tile Coding",
                  "Radial Basis Functions (RBF)",
                  "Fourier Basis",
                  "Coarse Coding"
                ]
              },
              {
                "subdomain": "Neural Network Architectures",
                "topics": [
                  "Multilayer Perceptrons (MLP)",
                  "Convolutional Neural Networks (CNN)",
                  "Recurrent Neural Networks (RNN/LSTM/GRU)",
                  "Transformers for RL",
                  "Attention Mechanisms",
                  "Graph Neural Networks (GNN)"
                ]
              },
              {
                "subdomain": "Stability and Convergence",
                "topics": [
                  "Deadly Triad Problem",
                  "Gradient Temporal-Difference (GTD)",
                  "GTD2 and TDC",
                  "Emphatic TD",
                  "Residual Gradients",
                  "Target Networks"
                ]
              },
              {
                "subdomain": "Representation Learning",
                "topics": [
                  "Autoencoders for State Representation",
                  "Contrastive Learning",
                  "CURL (Contrastive Unsupervised RL)",
                  "Self-Supervised Learning",
                  "Successor Representations",
                  "Auxiliary Tasks"
                ]
              }
            ]
          },
          {
            "domain": "Specialized RL Algorithms",
            "subdomains": [
              {
                "subdomain": "Partially Observable Environments",
                "topics": [
                  "POMDP (Partially Observable MDP)",
                  "Belief State Planning",
                  "Memory-Augmented Networks",
                  "DRQN (Deep Recurrent Q-Network)",
                  "Neural Episodic Control (NEC)",
                  "MERLIN"
                ]
              },
              {
                "subdomain": "Continuous Action Spaces",
                "topics": [
                  "Action Space Discretization",
                  "Normalized Advantage Functions (NAF)",
                  "Parameterized Actions",
                  "Hybrid Action Spaces",
                  "Stochastic Policy Gradients",
                  "Deterministic Policy Gradients"
                ]
              },
              {
                "subdomain": "Sparse and Delayed Rewards",
                "topics": [
                  "Reward Shaping",
                  "Hindsight Learning",
                  "Curriculum Learning",
                  "Episodic Memory",
                  "Go-Explore",
                  "Never Give Up (NGU)"
                ]
              },
              {
                "subdomain": "Average Reward Setting",
                "topics": [
                  "R-Learning",
                  "Average Reward TD",
                  "Differential Q-Learning",
                  "Relative Value Iteration",
                  "Gain-Bias Functions",
                  "Continuing Tasks"
                ]
              }
            ]
          },
          {
            "domain": "Safe and Robust RL",
            "subdomains": [
              {
                "subdomain": "Constrained RL",
                "topics": [
                  "Constrained MDPs (CMDPs)",
                  "Constrained Policy Optimization (CPO)",
                  "Lagrangian Methods",
                  "Safe Policy Gradients",
                  "Reward-Cost Trade-offs",
                  "Barrier Functions"
                ]
              },
              {
                "subdomain": "Risk-Sensitive RL",
                "topics": [
                  "Conditional Value at Risk (CVaR)",
                  "Mean-Variance Optimization",
                  "Worst-Case Optimization",
                  "Robust MDPs",
                  "Distributional Perspectives",
                  "Risk-Averse Policies"
                ]
              },
              {
                "subdomain": "Safe Exploration",
                "topics": [
                  "Safety Layers",
                  "Shield Functions",
                  "Human-in-the-Loop RL",
                  "Learning from Demonstrations for Safety",
                  "Safe Policy Improvement",
                  "Constrained Exploration"
                ]
              },
              {
                "subdomain": "Robustness to Distribution Shift",
                "topics": [
                  "Domain Randomization",
                  "Adversarial Training",
                  "Robust Value Functions",
                  "Uncertainty Quantification",
                  "Sim-to-Real Transfer",
                  "Model Ensembles"
                ]
              }
            ]
          },
          {
            "domain": "Reward Learning and Specification",
            "subdomains": [
              {
                "subdomain": "Preference-Based RL",
                "topics": [
                  "Learning from Human Preferences",
                  "Dueling Bandits",
                  "Preference Elicitation",
                  "Ranking-Based Learning",
                  "Comparison Queries",
                  "Active Preference Learning"
                ]
              },
              {
                "subdomain": "Reward Shaping",
                "topics": [
                  "Potential-Based Reward Shaping",
                  "Intrinsic Rewards",
                  "Auxiliary Rewards",
                  "Reward Engineering",
                  "Optimal Reward Design",
                  "Shaping Convergence Guarantees"
                ]
              },
              {
                "subdomain": "Multi-Objective RL",
                "topics": [
                  "Pareto Optimality",
                  "Scalarization Methods",
                  "Pareto Front Approximation",
                  "Weight Adaptation",
                  "Multi-Objective Policy Gradients",
                  "Trade-off Discovery"
                ]
              }
            ]
          },
          {
            "domain": "Scalability and Distributed RL",
            "subdomains": [
              {
                "subdomain": "Parallel and Distributed Training",
                "topics": [
                  "A3C (Asynchronous Advantage Actor-Critic)",
                  "IMPALA (Importance Weighted Actor-Learner)",
                  "Ape-X (Distributed Prioritized Experience Replay)",
                  "R2D2 (Recurrent Replay Distributed DQN)",
                  "Parameter Server Architectures",
                  "Gradient Aggregation"
                ]
              },
              {
                "subdomain": "Sample Efficiency",
                "topics": [
                  "Experience Replay Strategies",
                  "Data Augmentation",
                  "Model-Based Acceleration",
                  "Auxiliary Tasks",
                  "Self-Supervised Pretraining",
                  "Transfer from Simulators"
                ]
              },
              {
                "subdomain": "Computational Efficiency",
                "topics": [
                  "Network Compression",
                  "Knowledge Distillation",
                  "Pruning",
                  "Quantization",
                  "Early Stopping Strategies",
                  "Efficient Architecture Search"
                ]
              }
            ]
          },
          {
            "domain": "Theoretical Foundations",
            "subdomains": [
              {
                "subdomain": "Convergence and Optimality",
                "topics": [
                  "Policy Improvement Theorem",
                  "Contraction Mappings",
                  "Fixed Point Theorems",
                  "Convergence of TD Learning",
                  "Convergence of Q-Learning",
                  "Optimality Conditions"
                ]
              },
              {
                "subdomain": "Sample Complexity",
                "topics": [
                  "PAC Learning Bounds",
                  "Regret Bounds",
                  "Sample Complexity of Exploration",
                  "Information-Theoretic Bounds",
                  "Minimax Lower Bounds",
                  "Instance-Dependent Bounds"
                ]
              },
              {
                "subdomain": "Function Approximation Theory",
                "topics": [
                  "Approximation Error",
                  "Generalization Error",
                  "Bias-Variance Tradeoff",
                  "Universal Approximation",
                  "Feature Selection Theory",
                  "Representational Capacity"
                ]
              },
              {
                "subdomain": "Policy Gradient Theory",
                "topics": [
                  "Compatible Function Approximation",
                  "Natural Gradient Justification",
                  "Trust Region Analysis",
                  "Monotonic Improvement Guarantees",
                  "Variance Reduction Theory",
                  "Actor-Critic Convergence"
                ]
              }
            ]
          }
        ]
      }
    },
    {
      "subdomain_title": "RL Fundamentals",
      "processed_at": "2025-12-29T15:43:29.652928",
      "result": {
        "subdomain_title": "RL Fundamentals",
        "curriculum_type": "software-engineering",
        "topic_root": "RL Fundamentals",
        "topic_root_citation": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf",
        "detailed_hierarchy": [
          {
            "domain": "Sequential Decision-Making Foundations",
            "subdomains": [
              {
                "subdomain": "Multi-Armed Bandits",
                "atomic_topics": [
                  "n-Armed Bandit Problem Formulation",
                  "Action-Value Methods",
                  "Incremental Implementation",
                  "Nonstationary Problem Tracking",
                  "Optimistic Initial Values",
                  "Upper-Confidence-Bound (UCB) Action Selection",
                  "Gradient Bandit Algorithms",
                  "Associative Search and Contextual Bandits"
                ]
              },
              {
                "subdomain": "Exploration-Exploitation Trade-off",
                "atomic_topics": [
                  "Epsilon-Greedy Methods",
                  "Greedy Action Selection",
                  "Sample-Average Methods",
                  "Exploration Strategies",
                  "Exploitation Strategies",
                  "Balancing Exploration and Exploitation",
                  "Regret Minimization",
                  "Performance Metrics for Bandit Algorithms"
                ]
              },
              {
                "subdomain": "Agent-Environment Interface",
                "atomic_topics": [
                  "State Representation",
                  "Action Space Definition",
                  "Reward Signal Structure",
                  "Transition Dynamics",
                  "Observation vs. State",
                  "Feedback Mechanisms",
                  "Sequential Interaction Protocol",
                  "Time Step Formalization"
                ]
              }
            ]
          },
          {
            "domain": "Markov Decision Processes (MDPs)",
            "subdomains": [
              {
                "subdomain": "MDP Formalism",
                "atomic_topics": [
                  "State Space Definition",
                  "Action Space Definition",
                  "Transition Probability Functions",
                  "Reward Function Specification",
                  "Discount Factor and Its Role",
                  "Markov Property",
                  "MDP Tuple Notation",
                  "Episodic vs. Continuing Tasks"
                ]
              },
              {
                "subdomain": "Returns and Goals",
                "atomic_topics": [
                  "Expected Return Definition",
                  "Cumulative Return Calculation",
                  "Discounted Return",
                  "Undiscounted Return",
                  "Reward Hypothesis",
                  "Finite Horizon Returns",
                  "Infinite Horizon Returns",
                  "Unified Notation for Episodic and Continuing Tasks"
                ]
              },
              {
                "subdomain": "Policies",
                "atomic_topics": [
                  "Deterministic Policies",
                  "Stochastic Policies",
                  "Policy Representation",
                  "Policy Notation and Formalism",
                  "Stationary Policies",
                  "Non-Stationary Policies",
                  "Policy Evaluation Criteria",
                  "Optimal Policy Definition"
                ]
              }
            ]
          },
          {
            "domain": "Value Functions",
            "subdomains": [
              {
                "subdomain": "State-Value Functions",
                "atomic_topics": [
                  "State-Value Function Definition (V-function)",
                  "Expected Return from State",
                  "State-Value under Policy π",
                  "Recursive Decomposition",
                  "Value Function Properties",
                  "State-Value Estimation Methods",
                  "Optimal State-Value Function (V*)",
                  "Value Function Representation"
                ]
              },
              {
                "subdomain": "Action-Value Functions",
                "atomic_topics": [
                  "Action-Value Function Definition (Q-function)",
                  "Expected Return from State-Action Pair",
                  "Action-Value under Policy π",
                  "Q-function Properties",
                  "Relationship between V and Q",
                  "Action Selection from Q-values",
                  "Optimal Action-Value Function (Q*)",
                  "Q-function Estimation Methods"
                ]
              },
              {
                "subdomain": "Bellman Equations",
                "atomic_topics": [
                  "Bellman Expectation Equation for V",
                  "Bellman Expectation Equation for Q",
                  "Bellman Optimality Equation for V*",
                  "Bellman Optimality Equation for Q*",
                  "Bellman Backup Operations",
                  "Bellman Equation Derivation",
                  "Recursive Value Function Relations",
                  "Solving Bellman Equations"
                ]
              }
            ]
          },
          {
            "domain": "Dynamic Programming Methods",
            "subdomains": [
              {
                "subdomain": "Policy Evaluation",
                "atomic_topics": [
                  "Iterative Policy Evaluation Algorithm",
                  "State-Value Prediction",
                  "Synchronous Backups",
                  "Convergence Guarantees",
                  "Truncation and Stopping Criteria",
                  "In-Place Updates",
                  "Computational Complexity",
                  "Full Model Requirement"
                ]
              },
              {
                "subdomain": "Policy Improvement",
                "atomic_topics": [
                  "Policy Improvement Theorem",
                  "Greedy Policy Improvement",
                  "Action Selection from Value Functions",
                  "Policy Improvement Guarantees",
                  "Deterministic Policy Improvement",
                  "Policy Comparison Methods",
                  "Improvement Step Formalization",
                  "Convergence to Optimal Policy"
                ]
              },
              {
                "subdomain": "Control Algorithms",
                "atomic_topics": [
                  "Policy Iteration Algorithm",
                  "Value Iteration Algorithm",
                  "Generalized Policy Iteration (GPI)",
                  "Asynchronous Dynamic Programming",
                  "Modified Policy Iteration",
                  "Prioritized Sweeping",
                  "Real-Time Dynamic Programming",
                  "Computational Efficiency Considerations"
                ]
              }
            ]
          },
          {
            "domain": "Monte Carlo Methods",
            "subdomains": [
              {
                "subdomain": "Monte Carlo Prediction",
                "atomic_topics": [
                  "First-Visit MC Prediction",
                  "Every-Visit MC Prediction",
                  "Sample Episode Generation",
                  "Return Averaging",
                  "Incremental Mean Computation",
                  "Model-Free Prediction",
                  "Convergence Properties",
                  "Bias-Variance Trade-offs"
                ]
              },
              {
                "subdomain": "Monte Carlo Control",
                "atomic_topics": [
                  "Monte Carlo ES (Exploring Starts)",
                  "On-Policy MC Control",
                  "Epsilon-Soft Policies",
                  "Action-Value Estimation for Control",
                  "GPI with Monte Carlo Methods",
                  "Maintaining Exploration",
                  "Convergence to Optimal Policy",
                  "Episode-Based Updates"
                ]
              },
              {
                "subdomain": "Off-Policy Monte Carlo",
                "atomic_topics": [
                  "Importance Sampling Basics",
                  "Ordinary Importance Sampling",
                  "Weighted Importance Sampling",
                  "Off-Policy Prediction",
                  "Off-Policy Control",
                  "Target Policy vs. Behavior Policy",
                  "Importance Sampling Ratio",
                  "Variance Reduction Techniques"
                ]
              }
            ]
          },
          {
            "domain": "Temporal-Difference Learning",
            "subdomains": [
              {
                "subdomain": "TD Prediction",
                "atomic_topics": [
                  "TD(0) Algorithm",
                  "Bootstrapping Concept",
                  "TD Update Rule",
                  "TD Error Calculation",
                  "Learning Rate (Step Size) Selection",
                  "Advantages of TD Methods",
                  "TD vs. MC Comparison",
                  "Convergence Properties of TD(0)"
                ]
              },
              {
                "subdomain": "On-Policy TD Control",
                "atomic_topics": [
                  "Sarsa Algorithm",
                  "State-Action-Reward-State-Action Transitions",
                  "On-Policy Learning",
                  "Epsilon-Greedy Action Selection",
                  "Sarsa Convergence",
                  "Expected Sarsa",
                  "Sarsa for Episodic Tasks",
                  "Sarsa for Continuing Tasks"
                ]
              },
              {
                "subdomain": "Off-Policy TD Control",
                "atomic_topics": [
                  "Q-Learning Algorithm",
                  "Off-Policy Learning Mechanism",
                  "Max Q-Value Operator",
                  "Q-Learning Update Rule",
                  "Target Policy vs. Behavior Policy in Q-Learning",
                  "Q-Learning Convergence Guarantees",
                  "Double Q-Learning",
                  "Optimality of Q-Learning"
                ]
              }
            ]
          },
          {
            "domain": "n-Step and Eligibility Trace Methods",
            "subdomains": [
              {
                "subdomain": "n-Step Methods",
                "atomic_topics": [
                  "n-Step TD Prediction",
                  "n-Step Return Definition",
                  "n-Step Sarsa",
                  "n-Step Q-Learning",
                  "Unified View of n-Step Methods",
                  "Choosing n Value",
                  "n-Step Off-Policy Learning",
                  "Truncated Returns"
                ]
              },
              {
                "subdomain": "Eligibility Traces",
                "atomic_topics": [
                  "Forward View of TD(λ)",
                  "Backward View of TD(λ)",
                  "λ-Return Definition",
                  "Trace Decay Parameter",
                  "Accumulating Traces",
                  "Replacing Traces",
                  "Eligibility Trace Updates",
                  "Equivalence of Forward and Backward Views"
                ]
              },
              {
                "subdomain": "Advanced Trace Methods",
                "atomic_topics": [
                  "Sarsa(λ) Algorithm",
                  "Watkins's Q(λ)",
                  "Peng's Q(λ)",
                  "Off-Policy Eligibility Traces",
                  "Importance Sampling with Traces",
                  "Variable λ Methods",
                  "Implementation Efficiency",
                  "True Online TD(λ)"
                ]
              }
            ]
          },
          {
            "domain": "Function Approximation",
            "subdomains": [
              {
                "subdomain": "Value Function Approximation Foundations",
                "atomic_topics": [
                  "Need for Function Approximation",
                  "Generalization in RL",
                  "Feature Engineering",
                  "Linear Function Approximation",
                  "Nonlinear Function Approximation",
                  "Approximation Error",
                  "State Aggregation",
                  "Parametric Value Functions"
                ]
              },
              {
                "subdomain": "Gradient-Based Methods",
                "atomic_topics": [
                  "Gradient Descent for Value Functions",
                  "Stochastic Gradient Descent (SGD)",
                  "Mean Squared Value Error",
                  "TD Learning with Function Approximation",
                  "Semi-Gradient Methods",
                  "Deadly Triad of RL",
                  "Convergence with Function Approximation",
                  "Update Rule Derivation"
                ]
              },
              {
                "subdomain": "Deep Q-Networks (DQN)",
                "atomic_topics": [
                  "Neural Network Function Approximators",
                  "Deep Q-Network Architecture",
                  "Experience Replay Buffer",
                  "Target Network Stabilization",
                  "DQN Training Algorithm",
                  "Atari Game Benchmarks",
                  "Overestimation Bias in DQN",
                  "DQN Variants and Extensions"
                ]
              }
            ]
          },
          {
            "domain": "Policy Gradient Methods",
            "subdomains": [
              {
                "subdomain": "Policy Parameterization",
                "atomic_topics": [
                  "Parametric Policy Representation",
                  "Softmax Policy Parameterization",
                  "Gaussian Policy Parameterization",
                  "Neural Network Policies",
                  "Policy Gradient Theorem",
                  "Score Function Estimator",
                  "Log Derivative Trick",
                  "Direct Policy Optimization"
                ]
              },
              {
                "subdomain": "REINFORCE and Variants",
                "atomic_topics": [
                  "REINFORCE Algorithm",
                  "Monte Carlo Policy Gradient",
                  "REINFORCE with Baseline",
                  "Variance Reduction Techniques",
                  "Baseline Function Selection",
                  "Policy Gradient Update Rule",
                  "Unbiased Gradient Estimates",
                  "REINFORCE Convergence"
                ]
              },
              {
                "subdomain": "Actor-Critic Methods",
                "atomic_topics": [
                  "Actor-Critic Architecture",
                  "Value Function as Critic",
                  "Policy as Actor",
                  "Advantage Function",
                  "A3C (Asynchronous Advantage Actor-Critic)",
                  "A2C (Advantage Actor-Critic)",
                  "TD Error for Policy Gradients",
                  "Bootstrapped Policy Gradients"
                ]
              }
            ]
          },
          {
            "domain": "Advanced Policy Optimization",
            "subdomains": [
              {
                "subdomain": "Trust Region Methods",
                "atomic_topics": [
                  "Trust Region Policy Optimization (TRPO)",
                  "KL Divergence Constraint",
                  "Natural Policy Gradient",
                  "Conjugate Gradient Method",
                  "Fisher Information Matrix",
                  "Line Search Procedures",
                  "Conservative Policy Updates",
                  "TRPO Performance Guarantees"
                ]
              },
              {
                "subdomain": "Proximal Policy Optimization",
                "atomic_topics": [
                  "PPO Algorithm",
                  "Clipped Surrogate Objective",
                  "Importance Sampling Ratio Clipping",
                  "Adaptive KL Penalty",
                  "PPO vs. TRPO Comparison",
                  "Multiple Epochs of Minibatch Updates",
                  "PPO Implementation Simplicity",
                  "PPO Empirical Performance"
                ]
              },
              {
                "subdomain": "Deterministic Policy Gradients",
                "atomic_topics": [
                  "Deterministic Policy Gradient Theorem",
                  "DDPG (Deep Deterministic Policy Gradient)",
                  "Continuous Action Spaces",
                  "Ornstein-Uhlenbeck Noise",
                  "TD3 (Twin Delayed DDPG)",
                  "Clipped Double-Q Learning",
                  "Target Policy Smoothing",
                  "Delayed Policy Updates"
                ]
              }
            ]
          },
          {
            "domain": "Model-Based Reinforcement Learning",
            "subdomains": [
              {
                "subdomain": "Model Learning",
                "atomic_topics": [
                  "Transition Model Learning",
                  "Reward Model Learning",
                  "Supervised Learning for Models",
                  "Model Accuracy and Errors",
                  "Sample Efficiency of Model-Based RL",
                  "Table-Based Models",
                  "Parametric Model Approximation",
                  "Neural Network Dynamics Models"
                ]
              },
              {
                "subdomain": "Planning with Models",
                "atomic_topics": [
                  "Dyna Architecture",
                  "Dyna-Q Algorithm",
                  "Integrated Planning and Learning",
                  "Simulated Experience Generation",
                  "Real vs. Simulated Experience",
                  "Model Errors and Their Impact",
                  "Trajectory Sampling",
                  "Background Planning"
                ]
              },
              {
                "subdomain": "Search and Planning",
                "atomic_topics": [
                  "Heuristic Search Methods",
                  "Monte Carlo Tree Search (MCTS)",
                  "Upper Confidence Bounds for Trees (UCT)",
                  "Rollout Policies",
                  "AlphaGo and MCTS",
                  "Simulation-Based Planning",
                  "Tree Policy vs. Default Policy",
                  "Planning Depth Considerations"
                ]
              }
            ]
          },
          {
            "domain": "Exploration Strategies",
            "subdomains": [
              {
                "subdomain": "Undirected Exploration",
                "atomic_topics": [
                  "Epsilon-Greedy Exploration",
                  "Boltzmann Exploration",
                  "Random Noise Addition",
                  "Action Space Dithering",
                  "Decaying Exploration Rates",
                  "Entropy Regularization",
                  "Uniform Random Exploration",
                  "Parameter Space Noise"
                ]
              },
              {
                "subdomain": "Directed Exploration",
                "atomic_topics": [
                  "Upper Confidence Bounds (UCB)",
                  "Thompson Sampling",
                  "Optimism in Face of Uncertainty",
                  "Count-Based Exploration Bonuses",
                  "Intrinsic Motivation",
                  "Curiosity-Driven Exploration",
                  "Information Gain Methods",
                  "Posterior Sampling"
                ]
              },
              {
                "subdomain": "Advanced Exploration Methods",
                "atomic_topics": [
                  "Exploration via Disagreement",
                  "Prediction Error as Exploration Signal",
                  "Random Network Distillation (RND)",
                  "Novelty Detection",
                  "State Visitation Counts",
                  "Hash-Based State Counting",
                  "Empowerment and Information Theory",
                  "Goal-Conditioned Exploration"
                ]
              }
            ]
          },
          {
            "domain": "Off-Policy and Offline Reinforcement Learning",
            "subdomains": [
              {
                "subdomain": "Off-Policy Evaluation",
                "atomic_topics": [
                  "Importance Sampling for Off-Policy",
                  "Per-Decision Importance Sampling",
                  "Doubly Robust Estimation",
                  "Off-Policy Policy Evaluation",
                  "Behavior Policy vs. Target Policy",
                  "High Variance in Off-Policy Methods",
                  "Off-Policy Correction Techniques",
                  "Evaluation Metrics"
                ]
              },
              {
                "subdomain": "Batch and Offline RL",
                "atomic_topics": [
                  "Offline RL Problem Formulation",
                  "Batch-Constrained RL",
                  "Conservative Q-Learning (CQL)",
                  "Distribution Shift Challenges",
                  "Extrapolation Error",
                  "Policy Constraints for Offline RL",
                  "Behavior Cloning from Fixed Dataset",
                  "Offline Policy Optimization"
                ]
              },
              {
                "subdomain": "Imitation Learning",
                "atomic_topics": [
                  "Behavioral Cloning",
                  "Supervised Learning from Demonstrations",
                  "Dataset Aggregation (DAgger)",
                  "Inverse Reinforcement Learning (IRL)",
                  "Apprenticeship Learning",
                  "Generative Adversarial Imitation Learning (GAIL)",
                  "Learning from Human Preferences",
                  "Expert Demonstration Requirements"
                ]
              }
            ]
          },
          {
            "domain": "Special Topics and Extensions",
            "subdomains": [
              {
                "subdomain": "Continuous Control",
                "atomic_topics": [
                  "Continuous State Spaces",
                  "Continuous Action Spaces",
                  "Action Space Discretization",
                  "Policy for Continuous Actions",
                  "Soft Actor-Critic (SAC)",
                  "Maximum Entropy RL",
                  "Stochastic Policies for Continuous Actions",
                  "Torque and Force Control"
                ]
              },
              {
                "subdomain": "Partial Observability",
                "atomic_topics": [
                  "Partially Observable MDPs (POMDPs)",
                  "Observation vs. State Distinction",
                  "Belief States",
                  "Recurrent Neural Networks for POMDPs",
                  "LSTM and GRU for Memory",
                  "History-Based Policies",
                  "State Estimation Methods",
                  "Observation Models"
                ]
              },
              {
                "subdomain": "Multi-Agent and Hierarchical RL",
                "atomic_topics": [
                  "Multi-Agent RL Fundamentals",
                  "Cooperative vs. Competitive Settings",
                  "Nash Equilibrium in Multi-Agent RL",
                  "Communication in Multi-Agent Systems",
                  "Hierarchical Reinforcement Learning",
                  "Options Framework",
                  "Temporal Abstraction",
                  "Skill Discovery and Reuse"
                ]
              }
            ]
          }
        ]
      }
    }
  ]
}