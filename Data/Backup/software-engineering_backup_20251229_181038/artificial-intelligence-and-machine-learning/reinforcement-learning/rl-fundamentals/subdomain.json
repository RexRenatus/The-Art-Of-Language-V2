{
  "subdomain_title": "RL Fundamentals",
  "domain_title": "Reinforcement Learning",
  "category_title": "Artificial Intelligence and Machine Learning",
  "curriculum_type": "software-engineering",
  "processed_at": "2025-12-29T15:43:29.652928",
  "result": {
    "subdomain_title": "RL Fundamentals",
    "curriculum_type": "software-engineering",
    "topic_root": "RL Fundamentals",
    "topic_root_citation": "https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf",
    "detailed_hierarchy": [
      {
        "domain": "Sequential Decision-Making Foundations",
        "subdomains": [
          {
            "subdomain": "Multi-Armed Bandits",
            "atomic_topics": [
              "n-Armed Bandit Problem Formulation",
              "Action-Value Methods",
              "Incremental Implementation",
              "Nonstationary Problem Tracking",
              "Optimistic Initial Values",
              "Upper-Confidence-Bound (UCB) Action Selection",
              "Gradient Bandit Algorithms",
              "Associative Search and Contextual Bandits"
            ]
          },
          {
            "subdomain": "Exploration-Exploitation Trade-off",
            "atomic_topics": [
              "Epsilon-Greedy Methods",
              "Greedy Action Selection",
              "Sample-Average Methods",
              "Exploration Strategies",
              "Exploitation Strategies",
              "Balancing Exploration and Exploitation",
              "Regret Minimization",
              "Performance Metrics for Bandit Algorithms"
            ]
          },
          {
            "subdomain": "Agent-Environment Interface",
            "atomic_topics": [
              "State Representation",
              "Action Space Definition",
              "Reward Signal Structure",
              "Transition Dynamics",
              "Observation vs. State",
              "Feedback Mechanisms",
              "Sequential Interaction Protocol",
              "Time Step Formalization"
            ]
          }
        ]
      },
      {
        "domain": "Markov Decision Processes (MDPs)",
        "subdomains": [
          {
            "subdomain": "MDP Formalism",
            "atomic_topics": [
              "State Space Definition",
              "Action Space Definition",
              "Transition Probability Functions",
              "Reward Function Specification",
              "Discount Factor and Its Role",
              "Markov Property",
              "MDP Tuple Notation",
              "Episodic vs. Continuing Tasks"
            ]
          },
          {
            "subdomain": "Returns and Goals",
            "atomic_topics": [
              "Expected Return Definition",
              "Cumulative Return Calculation",
              "Discounted Return",
              "Undiscounted Return",
              "Reward Hypothesis",
              "Finite Horizon Returns",
              "Infinite Horizon Returns",
              "Unified Notation for Episodic and Continuing Tasks"
            ]
          },
          {
            "subdomain": "Policies",
            "atomic_topics": [
              "Deterministic Policies",
              "Stochastic Policies",
              "Policy Representation",
              "Policy Notation and Formalism",
              "Stationary Policies",
              "Non-Stationary Policies",
              "Policy Evaluation Criteria",
              "Optimal Policy Definition"
            ]
          }
        ]
      },
      {
        "domain": "Value Functions",
        "subdomains": [
          {
            "subdomain": "State-Value Functions",
            "atomic_topics": [
              "State-Value Function Definition (V-function)",
              "Expected Return from State",
              "State-Value under Policy π",
              "Recursive Decomposition",
              "Value Function Properties",
              "State-Value Estimation Methods",
              "Optimal State-Value Function (V*)",
              "Value Function Representation"
            ]
          },
          {
            "subdomain": "Action-Value Functions",
            "atomic_topics": [
              "Action-Value Function Definition (Q-function)",
              "Expected Return from State-Action Pair",
              "Action-Value under Policy π",
              "Q-function Properties",
              "Relationship between V and Q",
              "Action Selection from Q-values",
              "Optimal Action-Value Function (Q*)",
              "Q-function Estimation Methods"
            ]
          },
          {
            "subdomain": "Bellman Equations",
            "atomic_topics": [
              "Bellman Expectation Equation for V",
              "Bellman Expectation Equation for Q",
              "Bellman Optimality Equation for V*",
              "Bellman Optimality Equation for Q*",
              "Bellman Backup Operations",
              "Bellman Equation Derivation",
              "Recursive Value Function Relations",
              "Solving Bellman Equations"
            ]
          }
        ]
      },
      {
        "domain": "Dynamic Programming Methods",
        "subdomains": [
          {
            "subdomain": "Policy Evaluation",
            "atomic_topics": [
              "Iterative Policy Evaluation Algorithm",
              "State-Value Prediction",
              "Synchronous Backups",
              "Convergence Guarantees",
              "Truncation and Stopping Criteria",
              "In-Place Updates",
              "Computational Complexity",
              "Full Model Requirement"
            ]
          },
          {
            "subdomain": "Policy Improvement",
            "atomic_topics": [
              "Policy Improvement Theorem",
              "Greedy Policy Improvement",
              "Action Selection from Value Functions",
              "Policy Improvement Guarantees",
              "Deterministic Policy Improvement",
              "Policy Comparison Methods",
              "Improvement Step Formalization",
              "Convergence to Optimal Policy"
            ]
          },
          {
            "subdomain": "Control Algorithms",
            "atomic_topics": [
              "Policy Iteration Algorithm",
              "Value Iteration Algorithm",
              "Generalized Policy Iteration (GPI)",
              "Asynchronous Dynamic Programming",
              "Modified Policy Iteration",
              "Prioritized Sweeping",
              "Real-Time Dynamic Programming",
              "Computational Efficiency Considerations"
            ]
          }
        ]
      },
      {
        "domain": "Monte Carlo Methods",
        "subdomains": [
          {
            "subdomain": "Monte Carlo Prediction",
            "atomic_topics": [
              "First-Visit MC Prediction",
              "Every-Visit MC Prediction",
              "Sample Episode Generation",
              "Return Averaging",
              "Incremental Mean Computation",
              "Model-Free Prediction",
              "Convergence Properties",
              "Bias-Variance Trade-offs"
            ]
          },
          {
            "subdomain": "Monte Carlo Control",
            "atomic_topics": [
              "Monte Carlo ES (Exploring Starts)",
              "On-Policy MC Control",
              "Epsilon-Soft Policies",
              "Action-Value Estimation for Control",
              "GPI with Monte Carlo Methods",
              "Maintaining Exploration",
              "Convergence to Optimal Policy",
              "Episode-Based Updates"
            ]
          },
          {
            "subdomain": "Off-Policy Monte Carlo",
            "atomic_topics": [
              "Importance Sampling Basics",
              "Ordinary Importance Sampling",
              "Weighted Importance Sampling",
              "Off-Policy Prediction",
              "Off-Policy Control",
              "Target Policy vs. Behavior Policy",
              "Importance Sampling Ratio",
              "Variance Reduction Techniques"
            ]
          }
        ]
      },
      {
        "domain": "Temporal-Difference Learning",
        "subdomains": [
          {
            "subdomain": "TD Prediction",
            "atomic_topics": [
              "TD(0) Algorithm",
              "Bootstrapping Concept",
              "TD Update Rule",
              "TD Error Calculation",
              "Learning Rate (Step Size) Selection",
              "Advantages of TD Methods",
              "TD vs. MC Comparison",
              "Convergence Properties of TD(0)"
            ]
          },
          {
            "subdomain": "On-Policy TD Control",
            "atomic_topics": [
              "Sarsa Algorithm",
              "State-Action-Reward-State-Action Transitions",
              "On-Policy Learning",
              "Epsilon-Greedy Action Selection",
              "Sarsa Convergence",
              "Expected Sarsa",
              "Sarsa for Episodic Tasks",
              "Sarsa for Continuing Tasks"
            ]
          },
          {
            "subdomain": "Off-Policy TD Control",
            "atomic_topics": [
              "Q-Learning Algorithm",
              "Off-Policy Learning Mechanism",
              "Max Q-Value Operator",
              "Q-Learning Update Rule",
              "Target Policy vs. Behavior Policy in Q-Learning",
              "Q-Learning Convergence Guarantees",
              "Double Q-Learning",
              "Optimality of Q-Learning"
            ]
          }
        ]
      },
      {
        "domain": "n-Step and Eligibility Trace Methods",
        "subdomains": [
          {
            "subdomain": "n-Step Methods",
            "atomic_topics": [
              "n-Step TD Prediction",
              "n-Step Return Definition",
              "n-Step Sarsa",
              "n-Step Q-Learning",
              "Unified View of n-Step Methods",
              "Choosing n Value",
              "n-Step Off-Policy Learning",
              "Truncated Returns"
            ]
          },
          {
            "subdomain": "Eligibility Traces",
            "atomic_topics": [
              "Forward View of TD(λ)",
              "Backward View of TD(λ)",
              "λ-Return Definition",
              "Trace Decay Parameter",
              "Accumulating Traces",
              "Replacing Traces",
              "Eligibility Trace Updates",
              "Equivalence of Forward and Backward Views"
            ]
          },
          {
            "subdomain": "Advanced Trace Methods",
            "atomic_topics": [
              "Sarsa(λ) Algorithm",
              "Watkins's Q(λ)",
              "Peng's Q(λ)",
              "Off-Policy Eligibility Traces",
              "Importance Sampling with Traces",
              "Variable λ Methods",
              "Implementation Efficiency",
              "True Online TD(λ)"
            ]
          }
        ]
      },
      {
        "domain": "Function Approximation",
        "subdomains": [
          {
            "subdomain": "Value Function Approximation Foundations",
            "atomic_topics": [
              "Need for Function Approximation",
              "Generalization in RL",
              "Feature Engineering",
              "Linear Function Approximation",
              "Nonlinear Function Approximation",
              "Approximation Error",
              "State Aggregation",
              "Parametric Value Functions"
            ]
          },
          {
            "subdomain": "Gradient-Based Methods",
            "atomic_topics": [
              "Gradient Descent for Value Functions",
              "Stochastic Gradient Descent (SGD)",
              "Mean Squared Value Error",
              "TD Learning with Function Approximation",
              "Semi-Gradient Methods",
              "Deadly Triad of RL",
              "Convergence with Function Approximation",
              "Update Rule Derivation"
            ]
          },
          {
            "subdomain": "Deep Q-Networks (DQN)",
            "atomic_topics": [
              "Neural Network Function Approximators",
              "Deep Q-Network Architecture",
              "Experience Replay Buffer",
              "Target Network Stabilization",
              "DQN Training Algorithm",
              "Atari Game Benchmarks",
              "Overestimation Bias in DQN",
              "DQN Variants and Extensions"
            ]
          }
        ]
      },
      {
        "domain": "Policy Gradient Methods",
        "subdomains": [
          {
            "subdomain": "Policy Parameterization",
            "atomic_topics": [
              "Parametric Policy Representation",
              "Softmax Policy Parameterization",
              "Gaussian Policy Parameterization",
              "Neural Network Policies",
              "Policy Gradient Theorem",
              "Score Function Estimator",
              "Log Derivative Trick",
              "Direct Policy Optimization"
            ]
          },
          {
            "subdomain": "REINFORCE and Variants",
            "atomic_topics": [
              "REINFORCE Algorithm",
              "Monte Carlo Policy Gradient",
              "REINFORCE with Baseline",
              "Variance Reduction Techniques",
              "Baseline Function Selection",
              "Policy Gradient Update Rule",
              "Unbiased Gradient Estimates",
              "REINFORCE Convergence"
            ]
          },
          {
            "subdomain": "Actor-Critic Methods",
            "atomic_topics": [
              "Actor-Critic Architecture",
              "Value Function as Critic",
              "Policy as Actor",
              "Advantage Function",
              "A3C (Asynchronous Advantage Actor-Critic)",
              "A2C (Advantage Actor-Critic)",
              "TD Error for Policy Gradients",
              "Bootstrapped Policy Gradients"
            ]
          }
        ]
      },
      {
        "domain": "Advanced Policy Optimization",
        "subdomains": [
          {
            "subdomain": "Trust Region Methods",
            "atomic_topics": [
              "Trust Region Policy Optimization (TRPO)",
              "KL Divergence Constraint",
              "Natural Policy Gradient",
              "Conjugate Gradient Method",
              "Fisher Information Matrix",
              "Line Search Procedures",
              "Conservative Policy Updates",
              "TRPO Performance Guarantees"
            ]
          },
          {
            "subdomain": "Proximal Policy Optimization",
            "atomic_topics": [
              "PPO Algorithm",
              "Clipped Surrogate Objective",
              "Importance Sampling Ratio Clipping",
              "Adaptive KL Penalty",
              "PPO vs. TRPO Comparison",
              "Multiple Epochs of Minibatch Updates",
              "PPO Implementation Simplicity",
              "PPO Empirical Performance"
            ]
          },
          {
            "subdomain": "Deterministic Policy Gradients",
            "atomic_topics": [
              "Deterministic Policy Gradient Theorem",
              "DDPG (Deep Deterministic Policy Gradient)",
              "Continuous Action Spaces",
              "Ornstein-Uhlenbeck Noise",
              "TD3 (Twin Delayed DDPG)",
              "Clipped Double-Q Learning",
              "Target Policy Smoothing",
              "Delayed Policy Updates"
            ]
          }
        ]
      },
      {
        "domain": "Model-Based Reinforcement Learning",
        "subdomains": [
          {
            "subdomain": "Model Learning",
            "atomic_topics": [
              "Transition Model Learning",
              "Reward Model Learning",
              "Supervised Learning for Models",
              "Model Accuracy and Errors",
              "Sample Efficiency of Model-Based RL",
              "Table-Based Models",
              "Parametric Model Approximation",
              "Neural Network Dynamics Models"
            ]
          },
          {
            "subdomain": "Planning with Models",
            "atomic_topics": [
              "Dyna Architecture",
              "Dyna-Q Algorithm",
              "Integrated Planning and Learning",
              "Simulated Experience Generation",
              "Real vs. Simulated Experience",
              "Model Errors and Their Impact",
              "Trajectory Sampling",
              "Background Planning"
            ]
          },
          {
            "subdomain": "Search and Planning",
            "atomic_topics": [
              "Heuristic Search Methods",
              "Monte Carlo Tree Search (MCTS)",
              "Upper Confidence Bounds for Trees (UCT)",
              "Rollout Policies",
              "AlphaGo and MCTS",
              "Simulation-Based Planning",
              "Tree Policy vs. Default Policy",
              "Planning Depth Considerations"
            ]
          }
        ]
      },
      {
        "domain": "Exploration Strategies",
        "subdomains": [
          {
            "subdomain": "Undirected Exploration",
            "atomic_topics": [
              "Epsilon-Greedy Exploration",
              "Boltzmann Exploration",
              "Random Noise Addition",
              "Action Space Dithering",
              "Decaying Exploration Rates",
              "Entropy Regularization",
              "Uniform Random Exploration",
              "Parameter Space Noise"
            ]
          },
          {
            "subdomain": "Directed Exploration",
            "atomic_topics": [
              "Upper Confidence Bounds (UCB)",
              "Thompson Sampling",
              "Optimism in Face of Uncertainty",
              "Count-Based Exploration Bonuses",
              "Intrinsic Motivation",
              "Curiosity-Driven Exploration",
              "Information Gain Methods",
              "Posterior Sampling"
            ]
          },
          {
            "subdomain": "Advanced Exploration Methods",
            "atomic_topics": [
              "Exploration via Disagreement",
              "Prediction Error as Exploration Signal",
              "Random Network Distillation (RND)",
              "Novelty Detection",
              "State Visitation Counts",
              "Hash-Based State Counting",
              "Empowerment and Information Theory",
              "Goal-Conditioned Exploration"
            ]
          }
        ]
      },
      {
        "domain": "Off-Policy and Offline Reinforcement Learning",
        "subdomains": [
          {
            "subdomain": "Off-Policy Evaluation",
            "atomic_topics": [
              "Importance Sampling for Off-Policy",
              "Per-Decision Importance Sampling",
              "Doubly Robust Estimation",
              "Off-Policy Policy Evaluation",
              "Behavior Policy vs. Target Policy",
              "High Variance in Off-Policy Methods",
              "Off-Policy Correction Techniques",
              "Evaluation Metrics"
            ]
          },
          {
            "subdomain": "Batch and Offline RL",
            "atomic_topics": [
              "Offline RL Problem Formulation",
              "Batch-Constrained RL",
              "Conservative Q-Learning (CQL)",
              "Distribution Shift Challenges",
              "Extrapolation Error",
              "Policy Constraints for Offline RL",
              "Behavior Cloning from Fixed Dataset",
              "Offline Policy Optimization"
            ]
          },
          {
            "subdomain": "Imitation Learning",
            "atomic_topics": [
              "Behavioral Cloning",
              "Supervised Learning from Demonstrations",
              "Dataset Aggregation (DAgger)",
              "Inverse Reinforcement Learning (IRL)",
              "Apprenticeship Learning",
              "Generative Adversarial Imitation Learning (GAIL)",
              "Learning from Human Preferences",
              "Expert Demonstration Requirements"
            ]
          }
        ]
      },
      {
        "domain": "Special Topics and Extensions",
        "subdomains": [
          {
            "subdomain": "Continuous Control",
            "atomic_topics": [
              "Continuous State Spaces",
              "Continuous Action Spaces",
              "Action Space Discretization",
              "Policy for Continuous Actions",
              "Soft Actor-Critic (SAC)",
              "Maximum Entropy RL",
              "Stochastic Policies for Continuous Actions",
              "Torque and Force Control"
            ]
          },
          {
            "subdomain": "Partial Observability",
            "atomic_topics": [
              "Partially Observable MDPs (POMDPs)",
              "Observation vs. State Distinction",
              "Belief States",
              "Recurrent Neural Networks for POMDPs",
              "LSTM and GRU for Memory",
              "History-Based Policies",
              "State Estimation Methods",
              "Observation Models"
            ]
          },
          {
            "subdomain": "Multi-Agent and Hierarchical RL",
            "atomic_topics": [
              "Multi-Agent RL Fundamentals",
              "Cooperative vs. Competitive Settings",
              "Nash Equilibrium in Multi-Agent RL",
              "Communication in Multi-Agent Systems",
              "Hierarchical Reinforcement Learning",
              "Options Framework",
              "Temporal Abstraction",
              "Skill Discovery and Reuse"
            ]
          }
        ]
      }
    ]
  },
  "metadata": {
    "duration_seconds": 1026.58772,
    "timestamp": "2025-12-29T15:25:46.118482"
  }
}